{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec9d199",
   "metadata": {},
   "source": [
    "# Model Testing on Test Set\n",
    "\n",
    "Runs inference on the test audio files and displays the final metrics of the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e92077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import our custom modules\n",
    "from utils.inference import perform_audio_inference\n",
    "from utils.models import BirdCNN\n",
    "from utils.metrics import plot_confusion_matrix\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf089cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "NUM_CLASSES = 27\n",
    "MODEL_WEIGHTS_PATH = os.path.join('..', 'models', 'bird_cnn.pth')\n",
    "TEST_AUDIO_DIR = os.path.join('..', 'database', 'audio', 'test')\n",
    "TEST_METADATA_PATH = os.path.join('..', 'database', 'meta', 'test_data.csv')\n",
    "\n",
    "# Load test metadata\n",
    "print(\"Loading test metadata...\")\n",
    "test_df = pd.read_csv(TEST_METADATA_PATH)\n",
    "print(f\"✓ Loaded {len(test_df)} test samples\")\n",
    "print(f\"✓ Columns: {list(test_df.columns)}\")\n",
    "print(f\"✓ Classes represented: {test_df['class_id'].nunique()}\")\n",
    "print(f\"✓ Class distribution:\")\n",
    "print(test_df['class_id'].value_counts().sort_index())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify audio files exist and create file mapping\n",
    "print(\"Verifying audio files...\")\n",
    "audio_files = os.listdir(TEST_AUDIO_DIR)\n",
    "print(f\"✓ Found {len(audio_files)} audio files in test directory\")\n",
    "\n",
    "# Create mapping of filename to full path and check if all metadata files exist\n",
    "test_files = []\n",
    "missing_files = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    filename = row['filename']\n",
    "    audio_path = os.path.join(TEST_AUDIO_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(audio_path):\n",
    "        test_files.append({\n",
    "            'filename': filename,\n",
    "            'path': audio_path,\n",
    "            'class_id': row['class_id']\n",
    "        })\n",
    "    else:\n",
    "        missing_files.append(filename)\n",
    "\n",
    "print(f\"✓ Valid files: {len(test_files)}\")\n",
    "if missing_files:\n",
    "    print(f\"⚠ Missing files: {len(missing_files)}\")\n",
    "    print(f\"  First few missing: {missing_files[:5]}\")\n",
    "else:\n",
    "    print(\"✓ All metadata files found in audio directory\")\n",
    "\n",
    "print(f\"\\nReady to test on {len(test_files)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ebee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics functions\n",
    "def calculate_metrics(y_true, y_pred_probs, y_pred_classes):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics\n",
    "    \n",
    "    Args:\n",
    "        y_true: True class labels (numpy array)\n",
    "        y_pred_probs: Predicted probabilities (numpy array, shape: [n_samples, n_classes])\n",
    "        y_pred_classes: Predicted class labels (numpy array)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Accuracy\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred_classes)\n",
    "    \n",
    "    # F1 Score (macro average)\n",
    "    metrics['f1_macro'] = f1_score(y_true, y_pred_classes, average='macro', zero_division=0)\n",
    "    \n",
    "    # Cross Entropy Loss\n",
    "    # Convert to one-hot for cross entropy calculation\n",
    "    y_true_onehot = np.zeros((len(y_true), NUM_CLASSES))\n",
    "    y_true_onehot[np.arange(len(y_true)), y_true] = 1\n",
    "    \n",
    "    # Clip probabilities to avoid log(0)\n",
    "    y_pred_probs_clipped = np.clip(y_pred_probs, 1e-15, 1 - 1e-15)\n",
    "    metrics['cross_entropy'] = log_loss(y_true, y_pred_probs_clipped)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred_classes, labels=range(NUM_CLASSES))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"Print evaluation metrics in a nice format\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "    print(f\"F1 Score (Macro): {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Cross Entropy Loss: {metrics['cross_entropy']:.4f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on all test files\n",
    "print(\"Starting inference on test set...\")\n",
    "print(f\"Total files to process: {len(test_files)}\")\n",
    "\n",
    "# Storage for results\n",
    "all_predictions = []\n",
    "all_probabilities = []\n",
    "all_true_labels = []\n",
    "failed_files = []\n",
    "\n",
    "# Process each test file\n",
    "for i, file_info in enumerate(tqdm(test_files, desc=\"Processing audio files\")):\n",
    "    try:\n",
    "        # Perform inference\n",
    "        probabilities = perform_audio_inference(\n",
    "            audio_path=file_info['path'],\n",
    "            model_class=BirdCNN,\n",
    "            model_weights_path=MODEL_WEIGHTS_PATH,\n",
    "            reduce_noise=True\n",
    "        )\n",
    "        \n",
    "        # Get predicted class (argmax of probabilities)\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        \n",
    "        # Store results\n",
    "        all_probabilities.append(probabilities)\n",
    "        all_predictions.append(predicted_class)\n",
    "        all_true_labels.append(file_info['class_id'])\n",
    "        \n",
    "        # Progress update every 50 files\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(test_files)} files\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {file_info['filename']}: {str(e)}\")\n",
    "        failed_files.append(file_info['filename'])\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Inference completed!\")\n",
    "print(f\"✓ Successfully processed: {len(all_predictions)} files\")\n",
    "print(f\"✓ Failed files: {len(failed_files)}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"Failed files: {failed_files[:5]}{'...' if len(failed_files) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "if len(all_predictions) > 0:\n",
    "    print(\"Calculating evaluation metrics...\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(all_true_labels)\n",
    "    y_pred_classes = np.array(all_predictions)\n",
    "    y_pred_probs = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(y_true, y_pred_probs, y_pred_classes)\n",
    "    \n",
    "    # Print results\n",
    "    print_metrics(metrics)\n",
    "    \n",
    "    # Additional detailed statistics\n",
    "    print(f\"\\nDetailed Statistics:\")\n",
    "    print(f\"Total test samples: {len(y_true)}\")\n",
    "    print(f\"Correctly classified: {np.sum(y_true == y_pred_classes)}\")\n",
    "    print(f\"Misclassified: {np.sum(y_true != y_pred_classes)}\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print(f\"\\nPer-class Statistics:\")\n",
    "    unique_classes = np.unique(y_true)\n",
    "    print(f\"Classes present in test set: {len(unique_classes)}\")\n",
    "    \n",
    "    for class_id in sorted(unique_classes):\n",
    "        class_mask = (y_true == class_id)\n",
    "        class_correct = np.sum((y_true == y_pred_classes) & class_mask)\n",
    "        class_total = np.sum(class_mask)\n",
    "        class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "        print(f\"Class {class_id:2d}: {class_correct:2d}/{class_total:2d} = {class_acc:.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No successful predictions to evaluate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acebae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "if len(all_predictions) > 0:\n",
    "    print(\"Generating confusion matrix visualization...\")\n",
    "    \n",
    "    # Create confusion matrix plot\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Use the plot_confusion_matrix function from metrics\n",
    "    plot_confusion_matrix(\n",
    "        metrics['confusion_matrix'], \n",
    "        title=\"Test Set Confusion Matrix\",\n",
    "        figsize=(15, 12),\n",
    "        show_counts=True\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print confusion matrix statistics\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"\\nConfusion Matrix Shape: {cm.shape}\")\n",
    "    print(f\"Total predictions: {np.sum(cm)}\")\n",
    "    print(f\"Diagonal sum (correct predictions): {np.trace(cm)}\")\n",
    "    \n",
    "    # Find most confused classes\n",
    "    print(f\"\\nMost Confused Class Pairs (excluding diagonal):\")\n",
    "    cm_off_diag = cm.copy()\n",
    "    np.fill_diagonal(cm_off_diag, 0)\n",
    "    \n",
    "    # Get indices of highest confusion values\n",
    "    max_indices = np.unravel_index(np.argsort(cm_off_diag.ravel())[-10:], cm_off_diag.shape)\n",
    "    \n",
    "    for i in range(len(max_indices[0])-1, -1, -1):  # Reverse to show highest first\n",
    "        true_class = max_indices[0][i]\n",
    "        pred_class = max_indices[1][i]\n",
    "        count = cm_off_diag[true_class, pred_class]\n",
    "        if count > 0:\n",
    "            print(f\"True: {true_class:2d} → Predicted: {pred_class:2d} ({count:2d} times)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No predictions available for confusion matrix!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed results to CSV\n",
    "if len(all_predictions) > 0:\n",
    "    print(\"Saving detailed results...\")\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame({\n",
    "        'filename': [test_files[i]['filename'] for i in range(len(all_predictions))],\n",
    "        'true_class': y_true,\n",
    "        'predicted_class': y_pred_classes,\n",
    "        'correct': y_true == y_pred_classes,\n",
    "        'max_probability': [max(probs) for probs in y_pred_probs],\n",
    "        'prediction_confidence': [y_pred_probs[i][y_pred_classes[i]] for i in range(len(y_pred_classes))]\n",
    "    })\n",
    "    \n",
    "    # Add individual class probabilities\n",
    "    for class_id in range(NUM_CLASSES):\n",
    "        results_df[f'prob_class_{class_id}'] = [probs[class_id] for probs in y_pred_probs]\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_csv_path = 'test_results_detailed.csv'\n",
    "    results_df.to_csv(results_csv_path, index=False)\n",
    "    print(f\"✓ Detailed results saved to: {results_csv_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nFinal Summary:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model Performance on Test Set\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples tested: {len(y_true)}\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "    print(f\"F1 Score (Macro): {metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Cross Entropy Loss: {metrics['cross_entropy']:.4f}\")\n",
    "    print(f\"Correctly classified: {np.sum(y_pred_classes == y_true)}/{len(y_true)}\")\n",
    "    print(f\"Average prediction confidence: {np.mean([results_df.loc[i, 'prediction_confidence'] for i in range(len(results_df))]):.4f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Show some examples of correct and incorrect predictions\n",
    "    print(f\"\\nExample Correct Predictions:\")\n",
    "    correct_mask = results_df['correct'] == True\n",
    "    if correct_mask.sum() > 0:\n",
    "        correct_examples = results_df[correct_mask].nlargest(5, 'prediction_confidence')\n",
    "        for _, row in correct_examples.iterrows():\n",
    "            print(f\"  {row['filename']}: True={row['true_class']}, Pred={row['predicted_class']}, Conf={row['prediction_confidence']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nExample Incorrect Predictions:\")\n",
    "    incorrect_mask = results_df['correct'] == False\n",
    "    if incorrect_mask.sum() > 0:\n",
    "        incorrect_examples = results_df[incorrect_mask].nlargest(5, 'prediction_confidence')\n",
    "        for _, row in incorrect_examples.iterrows():\n",
    "            print(f\"  {row['filename']}: True={row['true_class']}, Pred={row['predicted_class']}, Conf={row['prediction_confidence']:.3f}\")\n",
    "\n",
    "print(\"\\n🎉 Model testing completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
