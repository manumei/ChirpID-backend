{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7495a4",
   "metadata": {},
   "source": [
    "In this one, test the different architectures with the top candidate configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e764f",
   "metadata": {},
   "source": [
    "## Pre-Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f76cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available - running on CPU (will be slow)\")\n",
    "\n",
    "# Performance optimization settings\n",
    "ENABLE_OPTIMIZATIONS = True  # Set to False to disable all optimizations\n",
    "ENABLE_PARALLEL_FOLDS = False  # Set to True for cross-validation mode\n",
    "MAX_PARALLEL_FOLDS = -1  # Adjust based on GPU memory\n",
    "\n",
    "def load_npy_data(specs_dir: str, specs_csv_path: str) -> Tuple[np.ndarray, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Load spectrograms from .npy files and metadata from CSV.\n",
    "    \n",
    "    Args:\n",
    "        specs_dir (str): Directory containing .npy spectrogram files\n",
    "        specs_csv_path (str): Path to CSV file containing metadata (filename, class_id, author)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.array, np.array]: Returns features, labels, and authors.\n",
    "        Features are already normalized to [0,1] and shaped as (N, 1, 224, 313)\n",
    "    \"\"\"\n",
    "    # Load metadata CSV\n",
    "    df = pd.read_csv(specs_csv_path)\n",
    "    \n",
    "    print(f\"Metadata shape: {df.shape}\")\n",
    "    print(f\"Number of classes: {df['class_id'].nunique()}\")\n",
    "    print(f\"Number of authors: {df['author'].nunique()}\")\n",
    "    \n",
    "    # Extract labels and authors\n",
    "    labels = df['class_id'].values.astype(np.int64)\n",
    "    authors = df['author'].values\n",
    "    filenames = df['filename'].values\n",
    "    \n",
    "    # Load spectrograms from .npy files\n",
    "    features_list = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, filename in enumerate(filenames):\n",
    "        spec_path = os.path.join(specs_dir, filename)\n",
    "        \n",
    "        if os.path.exists(spec_path):\n",
    "            try:\n",
    "                # Load .npy file - already normalized to [0,1] as float32\n",
    "                spec_array = np.load(spec_path)\n",
    "                \n",
    "                # Add channel dimension: (1, height, width)\n",
    "                spec_array = spec_array[np.newaxis, ...]\n",
    "                \n",
    "                features_list.append(spec_array)\n",
    "                valid_indices.append(i)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {spec_path}\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features = np.array(features_list, dtype=np.float32)\n",
    "    \n",
    "    # Filter labels and authors to match loaded features\n",
    "    labels = labels[valid_indices]\n",
    "    authors = authors[valid_indices]\n",
    "    \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Authors shape: {authors.shape}\")\n",
    "    print(f\"Unique classes: {len(np.unique(labels))}\")\n",
    "    print(f\"Unique authors: {len(np.unique(authors))}\")\n",
    "    print(f\"Successfully loaded {len(features)} out of {len(filenames)} spectrograms\")\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return features, labels, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ba291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New pipeline using .npy spectrograms from specs/ directory\n",
    "specs_dir = os.path.join('..', 'database', 'specs')\n",
    "specs_csv_path = os.path.join('..', 'database', 'meta', 'final_specs.csv')\n",
    "features, labels, authors = load_npy_data(specs_dir, specs_csv_path)\n",
    "\n",
    "# Display class distribution\n",
    "plt.figure(figsize=(9, 5))\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "plt.bar(unique_labels, counts, alpha=0.7)\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xticks(unique_labels)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average samples per class: {len(labels) / len(unique_labels):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a99d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with a set seed\n",
    "from utils.split import get_set_seed_indices, get_set_seed_kfold_indices, display_split_statistics\n",
    "seed_single = 245323 # Quality: 0.2671\n",
    "seed_kfold = 11052 # Quality: 0.3332\n",
    "\n",
    "single_fold_split = get_set_seed_indices(\n",
    "    features=features,\n",
    "    labels=labels, \n",
    "    authors=authors,\n",
    "    test_size=0.2,\n",
    "    seed=seed_single)\n",
    "\n",
    "kfold_splits = get_set_seed_kfold_indices(\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    authors=authors,\n",
    "    n_splits=4,\n",
    "    seed=seed_kfold)\n",
    "\n",
    "display_split_statistics(single_fold_split, \"single\")\n",
    "display_split_statistics(kfold_splits, \"kfold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914fe1d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models and training utilities\n",
    "from utils.models import (\n",
    "    BirdCNN_v1, BirdCNN_v2, BirdCNN_v3, BirdCNN_v4, BirdCNN_v5, BirdCNN_v6, BirdCNN_v7, BirdCNN_v8, BirdCNN_v9, BirdCNN_v10, \n",
    "    BirdCNN_v11, BirdCNN_v12, BirdCNN_v13, BirdCNN_v14, BirdCNN_v15, BirdCNN_v16, BirdCNN_v17, BirdCNN_v18, BirdCNN_v19, BirdCNN_v20\n",
    ")\n",
    "from utils.training_core import single_fold_training\n",
    "from utils.metrics import plot_metrics\n",
    "\n",
    "# Define all 16 model architectures for testing\n",
    "model_architectures = {\n",
    "    'BirdCNN_v1': BirdCNN_v1,   # ResNet-style with residual blocks\n",
    "    'BirdCNN_v2': BirdCNN_v2,   # VGG-style deep CNN\n",
    "    'BirdCNN_v3': BirdCNN_v3,   # PANN-inspired with attention pooling\n",
    "    'BirdCNN_v4': BirdCNN_v4,   # EfficientNet-inspired with MBConv blocks\n",
    "    'BirdCNN_v5': BirdCNN_v5,   # Inception-style with multi-scale convolutions\n",
    "    'BirdCNN_v6': BirdCNN_v6,   # DenseNet-inspired with dense connections\n",
    "    'BirdCNN_v7': BirdCNN_v7,   # ShuffleNet-inspired with channel shuffle\n",
    "    'BirdCNN_v8': BirdCNN_v8,   # RegNet-inspired with group convolutions\n",
    "    'BirdCNN_v9': BirdCNN_v9,   # Frequency-aware CNN with split processing\n",
    "    'BirdCNN_v10': BirdCNN_v10, # Hybrid CNN-RNN with LSTM\n",
    "    'BirdCNN_v11': BirdCNN_v11, # Lightweight MobileNet-style\n",
    "    'BirdCNN_v12': BirdCNN_v12, # Multi-scale feature extraction\n",
    "    'BirdCNN_v13': BirdCNN_v13, # Frequency and temporal attention\n",
    "    'BirdCNN_v14': BirdCNN_v14, # Hierarchical feature learning\n",
    "    'BirdCNN_v15': BirdCNN_v15, # Enhanced ResNet with SE blocks\n",
    "    'BirdCNN_v16': BirdCNN_v16, # Ensemble-like multi-path CNN\n",
    "    'BirdCNN_v17': BirdCNN_v17, # mi bombo\n",
    "    'BirdCNN_v18': BirdCNN_v18, # mi bombo\n",
    "    'BirdCNN_v19': BirdCNN_v19, # mi bombo\n",
    "    'BirdCNN_v20': BirdCNN_v20  # mi bombo\n",
    "}\n",
    "\n",
    "# Defining Configurations\n",
    "configurations = {\n",
    "    'configA' : {\n",
    "        'name': 'Parameters Frankenstein',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 36,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 0.0007,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.97},\n",
    "        'initial_lr': 0.0015,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'mixed_precision': True,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'parallel_folds': False,\n",
    "        'max_parallel_folds': 2,\n",
    "        'optimize_dataloaders': True,\n",
    "    },\n",
    "\n",
    "    'configB': {\n",
    "        'name': 'Balanced Classes Focus',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 24,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 0.0007,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 75},\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'configC': {\n",
    "        'name': 'Chaos Theory',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 48,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 0.00025,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 73},\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': False,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 247,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.73,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'configD': {  # AdamW variant of config9\n",
    "        'name': 'Config del Chef',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 16,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 0.008,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.96},\n",
    "        'initial_lr': 0.002,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize results database for all model-config combinations\n",
    "building_results = {}\n",
    "class_num = len(np.unique(labels))\n",
    "\n",
    "arqs = len(model_architectures)\n",
    "confs = len(configurations)\n",
    "eta = arqs * confs * 5 / 60\n",
    "print(f\"Prepared to test {arqs} architectures across {confs} configurations\")\n",
    "print(f\"Total training runs: {arqs * confs}\")\n",
    "print(f\"Estimated time: {eta:.2f} hours (assuming 5 min per run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG A TRAINING: Parameters Frankenstein ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG A: Parameters Frankenstein\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_a = configurations['configA']\n",
    "config_a_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config A\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            authors=authors,\n",
    "            model_class=model_class,\n",
    "            num_classes=class_num,\n",
    "            config=config_a,\n",
    "            spec_augment=config_a['spec_augment'],\n",
    "            gaussian_noise=config_a['noise_augment'],\n",
    "            precomputed_split=single_fold_split,  # Use pre-computed single fold split\n",
    "            config_id=\"Config A\"  # Pass config_id for progress bar\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_a_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configA'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configA\"] = config_a_results[model_name]\n",
    "        \n",
    "        print(f\"✅ {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_a_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} failed: {str(e)}\")\n",
    "        config_a_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configA'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configA\"] = config_a_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG A TRAINING COMPLETED\")\n",
    "successful_a = sum(1 for result in config_a_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_a}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG B TRAINING: Balanced Classes Focus ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG B: Balanced Classes Focus\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_b = configurations['configB']\n",
    "config_b_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config B\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=class_num)\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            authors=authors,\n",
    "            model_class=model_class,\n",
    "            num_classes=class_num,\n",
    "            config=config_b,\n",
    "            spec_augment=config_b['spec_augment'],\n",
    "            gaussian_noise=config_b['noise_augment'],\n",
    "            precomputed_split=single_fold_split,  # Use pre-computed single fold split\n",
    "            config_id=\"Config B\"  # Pass config_id for progress bar\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_b_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configB'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configB\"] = config_b_results[model_name]\n",
    "        \n",
    "        print(f\"✅ {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_b_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} failed: {str(e)}\")\n",
    "        config_b_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configB'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configB\"] = config_b_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG B TRAINING COMPLETED\")\n",
    "successful_b = sum(1 for result in config_b_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_b}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG C TRAINING: Chaos Theory ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG C: Chaos Theory\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_c = configurations['configC']\n",
    "config_c_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config C\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=class_num)\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            authors=authors,\n",
    "            model_class=model_class,\n",
    "            num_classes=class_num,\n",
    "            config=config_c,\n",
    "            spec_augment=config_c['spec_augment'],\n",
    "            gaussian_noise=config_c['noise_augment'],\n",
    "            precomputed_split=single_fold_split,  # Use pre-computed single fold split\n",
    "            config_id=\"Config C\"  # Pass config_id for progress bar\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_c_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configC'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configC\"] = config_c_results[model_name]\n",
    "        \n",
    "        print(f\"✅ {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_c_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} failed: {str(e)}\")\n",
    "        config_c_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configC'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configC\"] = config_c_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG C TRAINING COMPLETED\")\n",
    "successful_c = sum(1 for result in config_c_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_c}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG D TRAINING: Balanced Classes AdamW ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG D: Balanced Classes AdamW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_d = configurations['configD']\n",
    "config_d_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config D\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=class_num)\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            authors=authors,\n",
    "            model_class=model_class,\n",
    "            num_classes=class_num,\n",
    "            config=config_d,\n",
    "            spec_augment=config_d['spec_augment'],\n",
    "            gaussian_noise=config_d['noise_augment'],\n",
    "            precomputed_split=single_fold_split,  # Use pre-computed single fold split\n",
    "            config_id=\"Config D\"  # Pass config_id for progress bar\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_d_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configD'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configD\"] = config_d_results[model_name]\n",
    "        \n",
    "        print(f\"✅ {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_d_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} failed: {str(e)}\")\n",
    "        config_d_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configD'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configD\"] = config_d_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG D TRAINING COMPLETED\")\n",
    "successful_d = sum(1 for result in config_d_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_d}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08722d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL TRAINING COMPLETED!\")\n",
    "print(f\"{'='*60}\")\n",
    "total_successful = sum(1 for result in building_results.values() if result['status'] == 'success')\n",
    "total_runs = len(building_results)\n",
    "print(f\"Total successful runs: {total_successful}/{total_runs}\")\n",
    "print(f\"Success rate: {total_successful/total_runs*100:.1f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e6bfa",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results plotting - Training metrics for all successful model-config combinations\n",
    "print(\"=\"*80)\n",
    "print(\"PLOTTING TRAINING METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot training metrics for all successful configurations\n",
    "successful_results = []\n",
    "for config_id, data in building_results.items():\n",
    "    if data['status'] == 'success' and 'result' in data:\n",
    "        result = data['result']\n",
    "        try:\n",
    "            plot_metrics(config_id, result)\n",
    "            successful_results.append((config_id, result))\n",
    "            print(f\"✅ Plotted metrics for {config_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to plot {config_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully plotted metrics for {len(successful_results)} model-config combinations\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table display - Comprehensive results for all model-config combinations\n",
    "print(\"=\"*80)\n",
    "print(\"BUILDING RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive results table\n",
    "table_data = []\n",
    "for config_id, data in building_results.items():\n",
    "    if data['status'] == 'success' and 'result' in data:\n",
    "        result = data['result']\n",
    "        \n",
    "        # Parse model name and config\n",
    "        parts = config_id.split('_config')\n",
    "        model_name = parts[0]\n",
    "        config_name = 'config' + parts[1] if len(parts) > 1 else 'unknown'\n",
    "        \n",
    "        table_data.append({\n",
    "            'model': model_name,\n",
    "            'config': config_name,\n",
    "            'config_id': config_id,\n",
    "            'best_val_acc': result.get('best_val_acc', 0),\n",
    "            'best_val_f1': result.get('best_val_f1', 0),\n",
    "            'training_time': data.get('training_time', 0)\n",
    "        })\n",
    "\n",
    "# Create DataFrame and sort by best_val_f1 (descending)\n",
    "if table_data:\n",
    "    results_table = pd.DataFrame(table_data)\n",
    "    results_table = results_table.sort_values('best_val_f1', ascending=False)\n",
    "\n",
    "    print(\"MODEL BUILDING RESULTS TABLE (sorted by Best Val F1)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(results_table.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total successful runs: {len(results_table)}\")\n",
    "    print(f\"Best F1 score: {results_table['best_val_f1'].max():.4f} ({results_table.loc[results_table['best_val_f1'].idxmax(), 'config_id']})\")\n",
    "    print(f\"Best accuracy: {results_table['best_val_acc'].max():.4f} ({results_table.loc[results_table['best_val_acc'].idxmax(), 'config_id']})\")\n",
    "    print(f\"Average F1 score: {results_table['best_val_f1'].mean():.4f}\")\n",
    "    print(f\"Average accuracy: {results_table['best_val_acc'].mean():.4f}\")\n",
    "    print(f\"Total training time: {results_table['training_time'].sum()/3600:.2f} hours\")\n",
    "    \n",
    "    # Top 5 models by F1\n",
    "    print(f\"\\nTOP 5 MODELS BY F1 SCORE:\")\n",
    "    top_5 = results_table.head(5)[['model', 'config', 'best_val_f1', 'best_val_acc']]\n",
    "    print(top_5.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "else:\n",
    "    print(\"No successful results found to display.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5x4 plot grid: F1 scores for each architecture across all configs + highest F1\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING ARCHITECTURE COMPARISON PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for plotting\n",
    "models_f1_data = {}\n",
    "configs = ['configA', 'configB', 'configC', 'configD']\n",
    "\n",
    "for model_name in model_architectures.keys():\n",
    "    models_f1_data[model_name] = {\n",
    "        'configA': 0,\n",
    "        'configB': 0, \n",
    "        'configC': 0,\n",
    "        'configD': 0,\n",
    "        'max_f1': 0\n",
    "    }\n",
    "    \n",
    "    # Extract F1 scores for each config\n",
    "    for config in configs:\n",
    "        key = f\"{model_name}_{config}\"\n",
    "        if key in building_results and building_results[key]['status'] == 'success':\n",
    "            f1_score = building_results[key]['result'].get('best_val_f1', 0)\n",
    "            models_f1_data[model_name][config] = f1_score\n",
    "    \n",
    "    # Calculate max F1 across all configs\n",
    "    config_f1s = [models_f1_data[model_name][config] for config in configs]\n",
    "    models_f1_data[model_name]['max_f1'] = max(config_f1s)\n",
    "\n",
    "# Create 5x4 subplot grid for 20 architectures\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20, 20))\n",
    "fig.suptitle('Model Architecture Performance Comparison\\n(F1 Scores Across Configurations)', \n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "model_names = list(model_architectures.keys())\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Data for this model\n",
    "    data = models_f1_data[model_name]\n",
    "    \n",
    "    # Create bar plot with 4 bars: 4 configs\n",
    "    categories = ['A', 'B', 'C', 'D']\n",
    "    values = [data['configA'], data['configB'], data['configC'], data['configD']]\n",
    "    colors = ['skyblue', 'lightgreen', 'orange', 'lightcoral']\n",
    "    \n",
    "    bars = ax.bar(categories, values, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_title(f'{model_name}', fontweight='bold', fontsize=10)\n",
    "    ax.set_ylabel('F1 Score', fontsize=8)\n",
    "    ax.set_ylim(0, max(1.0, max(values) * 1.1))\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', labelsize=8)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        if value > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "    \n",
    "    # Highlight the best performing config\n",
    "    if data['max_f1'] > 0:\n",
    "        best_config_idx = values.index(data['max_f1'])\n",
    "        bars[best_config_idx].set_edgecolor('red')\n",
    "        bars[best_config_idx].set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
