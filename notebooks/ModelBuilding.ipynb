{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7495a4",
   "metadata": {},
   "source": [
    "In this one, test the different architectures with the top candidate configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e764f",
   "metadata": {},
   "source": [
    "## Pre-Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f76cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import seaborn as sns\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA not available - running on CPU (will be slow)\")\n",
    "\n",
    "# Performance optimization settings\n",
    "ENABLE_OPTIMIZATIONS = True  # Set to False to disable all optimizations\n",
    "ENABLE_PARALLEL_FOLDS = False  # Set to True for cross-validation mode\n",
    "MAX_PARALLEL_FOLDS = -1  # Adjust based on GPU memory\n",
    "\n",
    "def load_npy_data(specs_dir: str, specs_csv_path: str) -> Tuple[np.ndarray, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Load spectrograms from .npy files and metadata from CSV.\n",
    "    \n",
    "    Args:\n",
    "        specs_dir (str): Directory containing .npy spectrogram files\n",
    "        specs_csv_path (str): Path to CSV file containing metadata (filename, class_id, author)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.array, np.array]: Returns features, labels, and authors.\n",
    "        Features are already normalized to [0,1] and shaped as (N, 1, 224, 313)\n",
    "    \"\"\"\n",
    "    # Load metadata CSV\n",
    "    df = pd.read_csv(specs_csv_path)\n",
    "    \n",
    "    print(f\"Metadata shape: {df.shape}\")\n",
    "    print(f\"Number of classes: {df['class_id'].nunique()}\")\n",
    "    print(f\"Number of authors: {df['author'].nunique()}\")\n",
    "    \n",
    "    # Extract labels and authors\n",
    "    labels = df['class_id'].values.astype(np.int64)\n",
    "    authors = df['author'].values\n",
    "    filenames = df['filename'].values\n",
    "    \n",
    "    # Load spectrograms from .npy files\n",
    "    features_list = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for i, filename in enumerate(filenames):\n",
    "        spec_path = os.path.join(specs_dir, filename)\n",
    "        \n",
    "        if os.path.exists(spec_path):\n",
    "            try:\n",
    "                # Load .npy file - already normalized to [0,1] as float32\n",
    "                spec_array = np.load(spec_path)\n",
    "                \n",
    "                # Add channel dimension: (1, height, width)\n",
    "                spec_array = spec_array[np.newaxis, ...]\n",
    "                \n",
    "                features_list.append(spec_array)\n",
    "                valid_indices.append(i)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {spec_path}\")\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features = np.array(features_list, dtype=np.float32)\n",
    "    \n",
    "    # Filter labels and authors to match loaded features\n",
    "    labels = labels[valid_indices]\n",
    "    authors = authors[valid_indices]\n",
    "    \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Authors shape: {authors.shape}\")\n",
    "    print(f\"Unique classes: {len(np.unique(labels))}\")\n",
    "    print(f\"Unique authors: {len(np.unique(authors))}\")\n",
    "    print(f\"Successfully loaded {len(features)} out of {len(filenames)} spectrograms\")\n",
    "    \n",
    "    # Clean up\n",
    "    del df\n",
    "    \n",
    "    return features, labels, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ba291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New pipeline using .npy spectrograms from specs/ directory\n",
    "specs_dir = os.path.join('..', 'database', 'specs')\n",
    "specs_csv_path = os.path.join('..', 'database', 'meta', 'final_specs.csv')\n",
    "features, labels, authors = load_npy_data(specs_dir, specs_csv_path)\n",
    "\n",
    "# Display class distribution\n",
    "plt.figure(figsize=(9, 5))\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "plt.bar(unique_labels, counts, alpha=0.7)\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xticks(unique_labels)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average samples per class: {len(labels) / len(unique_labels):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a99d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with a set seed\n",
    "from utils.split import get_set_seed_indices, get_set_seed_kfold_indices, display_split_statistics\n",
    "seed_single = 245323 # Quality: 0.2671\n",
    "seed_kfold = 11052 # Quality: 0.3332\n",
    "\n",
    "single_fold_split = get_set_seed_indices(\n",
    "    features=features,\n",
    "    labels=labels, \n",
    "    authors=authors,\n",
    "    test_size=0.2,\n",
    "    seed=seed_single)\n",
    "\n",
    "kfold_splits = get_set_seed_kfold_indices(\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    authors=authors,\n",
    "    n_splits=4,\n",
    "    seed=seed_kfold)\n",
    "\n",
    "display_split_statistics(single_fold_split, \"single\")\n",
    "display_split_statistics(kfold_splits, \"kfold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914fe1d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all models and training utilities\n",
    "from utils.models import (\n",
    "    BirdCNN_v1, BirdCNN_v2, BirdCNN_v3, BirdCNN_v4, BirdCNN_v5, BirdCNN_v6, BirdCNN_v7, BirdCNN_v8,\n",
    "    BirdCNN_v9, BirdCNN_v10, BirdCNN_v11, BirdCNN_v12, BirdCNN_v13, BirdCNN_v14, BirdCNN_v15, BirdCNN_v16\n",
    ")\n",
    "from utils.training_core import single_fold_training\n",
    "from utils.metrics import plot_metrics\n",
    "\n",
    "# Define all 16 model architectures for testing\n",
    "model_architectures = {\n",
    "    'BirdCNN_v1': BirdCNN_v1,   # ResNet-style with residual blocks\n",
    "    'BirdCNN_v2': BirdCNN_v2,   # VGG-style deep CNN\n",
    "    'BirdCNN_v3': BirdCNN_v3,   # PANN-inspired with attention pooling\n",
    "    'BirdCNN_v4': BirdCNN_v4,   # EfficientNet-inspired with MBConv blocks\n",
    "    'BirdCNN_v5': BirdCNN_v5,   # Inception-style with multi-scale convolutions\n",
    "    'BirdCNN_v6': BirdCNN_v6,   # DenseNet-inspired with dense connections\n",
    "    'BirdCNN_v7': BirdCNN_v7,   # ShuffleNet-inspired with channel shuffle\n",
    "    'BirdCNN_v8': BirdCNN_v8,   # RegNet-inspired with group convolutions\n",
    "    'BirdCNN_v9': BirdCNN_v9,   # Frequency-aware CNN with split processing\n",
    "    'BirdCNN_v10': BirdCNN_v10, # Hybrid CNN-RNN with LSTM\n",
    "    'BirdCNN_v11': BirdCNN_v11, # Lightweight MobileNet-style\n",
    "    'BirdCNN_v12': BirdCNN_v12, # Multi-scale feature extraction\n",
    "    'BirdCNN_v13': BirdCNN_v13, # Frequency and temporal attention\n",
    "    'BirdCNN_v14': BirdCNN_v14, # Hierarchical feature learning\n",
    "    'BirdCNN_v15': BirdCNN_v15, # Enhanced ResNet with SE blocks\n",
    "    'BirdCNN_v16': BirdCNN_v16, # Ensemble-like multi-path CNN\n",
    "}\n",
    "\n",
    "# Defining Configurations\n",
    "configurations = {\n",
    "    'configA' : {\n",
    "        'name': 'Parameters Frankenstein',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 36,\n",
    "        'batch_size': 40,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 0.0003,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.97},\n",
    "        'initial_lr': 0.0024, # also try 0.00137\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'mixed_precision': True,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'parallel_folds': False,\n",
    "        'max_parallel_folds': 2,\n",
    "        'optimize_dataloaders': True,\n",
    "    },\n",
    "\n",
    "    'configB': {\n",
    "        'name': 'Balanced Classes Focus',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 28,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.96},\n",
    "        'initial_lr': 0.0012,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'configC': {\n",
    "        'name': 'Chaos Theory',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 37,\n",
    "        'batch_size': 45,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3.7e-4,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 73},\n",
    "        'initial_lr': 0.00137,\n",
    "        'standardize': False,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 247,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.73,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'configD': {  # AdamW variant of config9\n",
    "        'name': 'Balanced Classes AdamW',\n",
    "        'use_adam': 'adamw',\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 32,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 0.02,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.96},\n",
    "        'initial_lr': 0.003,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize results database for all model-config combinations\n",
    "building_results = {}\n",
    "print(f\"Prepared to test {len(model_architectures)} architectures across {len(configurations)} configurations\")\n",
    "print(f\"Total training runs: {len(model_architectures) * len(configurations)}\")\n",
    "print(f\"Estimated time: {len(model_architectures) * len(configurations) * 15} minutes (assuming 15 min per run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG A TRAINING: Parameters Frankenstein ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG A: Parameters Frankenstein\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_a = configurations['configA']\n",
    "config_a_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config A\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=len(np.unique(labels)))\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            model=model,\n",
    "            split_indices=single_fold_split,\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            config=config_a\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_a_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configA'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configA\"] = config_a_results[model_name]\n",
    "        \n",
    "        print(f\"âœ… {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_a_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {str(e)}\")\n",
    "        config_a_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configA'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configA\"] = config_a_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG A TRAINING COMPLETED\")\n",
    "successful_a = sum(1 for result in config_a_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_a}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG B TRAINING: Balanced Classes Focus ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG B: Balanced Classes Focus\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_b = configurations['configB']\n",
    "config_b_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config B\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=len(np.unique(labels)))\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            model=model,\n",
    "            split_indices=single_fold_split,\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            config=config_b\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_b_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configB'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configB\"] = config_b_results[model_name]\n",
    "        \n",
    "        print(f\"âœ… {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_b_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {str(e)}\")\n",
    "        config_b_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configB'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configB\"] = config_b_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG B TRAINING COMPLETED\")\n",
    "successful_b = sum(1 for result in config_b_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_b}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG C TRAINING: Chaos Theory ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG C: Chaos Theory\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_c = configurations['configC']\n",
    "config_c_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config C\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=len(np.unique(labels)))\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            model=model,\n",
    "            split_indices=single_fold_split,\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            config=config_c\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_c_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configC'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configC\"] = config_c_results[model_name]\n",
    "        \n",
    "        print(f\"âœ… {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_c_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {str(e)}\")\n",
    "        config_c_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configC'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configC\"] = config_c_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG C TRAINING COMPLETED\")\n",
    "successful_c = sum(1 for result in config_c_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_c}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG D TRAINING: Balanced Classes AdamW ===\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIG D: Balanced Classes AdamW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "config_d = configurations['configD']\n",
    "config_d_results = {}\n",
    "\n",
    "for model_name, model_class in model_architectures.items():\n",
    "    try:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name} with Config D\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create model instance\n",
    "        model = model_class(num_classes=len(np.unique(labels)))\n",
    "        \n",
    "        # Run single fold training\n",
    "        result = single_fold_training(\n",
    "            model=model,\n",
    "            split_indices=single_fold_split,\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            config=config_d\n",
    "        )\n",
    "        \n",
    "        # Store result\n",
    "        config_d_results[model_name] = {\n",
    "            'status': 'success',\n",
    "            'result': result,\n",
    "            'training_time': time.time() - start_time,\n",
    "            'config': 'configD'\n",
    "        }\n",
    "        \n",
    "        # Store in global results\n",
    "        building_results[f\"{model_name}_configD\"] = config_d_results[model_name]\n",
    "        \n",
    "        print(f\"âœ… {model_name} completed successfully!\")\n",
    "        print(f\"   Best Val Acc: {result.get('best_val_acc', 0):.4f}\")\n",
    "        print(f\"   Best Val F1: {result.get('best_val_f1', 0):.4f}\")\n",
    "        print(f\"   Training time: {config_d_results[model_name]['training_time']:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} failed: {str(e)}\")\n",
    "        config_d_results[model_name] = {\n",
    "            'status': 'failed',\n",
    "            'error': str(e),\n",
    "            'config': 'configD'\n",
    "        }\n",
    "        building_results[f\"{model_name}_configD\"] = config_d_results[model_name]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONFIG D TRAINING COMPLETED\")\n",
    "successful_d = sum(1 for result in config_d_results.values() if result['status'] == 'success')\n",
    "print(f\"Successful models: {successful_d}/{len(model_architectures)}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL TRAINING COMPLETED!\")\n",
    "print(f\"{'='*60}\")\n",
    "total_successful = sum(1 for result in building_results.values() if result['status'] == 'success')\n",
    "total_runs = len(building_results)\n",
    "print(f\"Total successful runs: {total_successful}/{total_runs}\")\n",
    "print(f\"Success rate: {total_successful/total_runs*100:.1f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e6bfa",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results plotting - Training metrics for all successful model-config combinations\n",
    "print(\"=\"*80)\n",
    "print(\"PLOTTING TRAINING METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot training metrics for all successful configurations\n",
    "successful_results = []\n",
    "for config_id, data in building_results.items():\n",
    "    if data['status'] == 'success' and 'result' in data:\n",
    "        result = data['result']\n",
    "        try:\n",
    "            plot_metrics(config_id, result)\n",
    "            successful_results.append((config_id, result))\n",
    "            print(f\"âœ… Plotted metrics for {config_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to plot {config_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nSuccessfully plotted metrics for {len(successful_results)} model-config combinations\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table display - Comprehensive results for all model-config combinations\n",
    "print(\"=\"*80)\n",
    "print(\"BUILDING RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive results table\n",
    "table_data = []\n",
    "for config_id, data in building_results.items():\n",
    "    if data['status'] == 'success' and 'result' in data:\n",
    "        result = data['result']\n",
    "        \n",
    "        # Parse model name and config\n",
    "        parts = config_id.split('_config')\n",
    "        model_name = parts[0]\n",
    "        config_name = 'config' + parts[1] if len(parts) > 1 else 'unknown'\n",
    "        \n",
    "        table_data.append({\n",
    "            'model': model_name,\n",
    "            'config': config_name,\n",
    "            'config_id': config_id,\n",
    "            'best_val_acc': result.get('best_val_acc', 0),\n",
    "            'best_val_f1': result.get('best_val_f1', 0),\n",
    "            'final_val_acc': result.get('final_val_acc', 0),\n",
    "            'final_val_f1': result.get('final_val_f1', 0),\n",
    "            'training_time': data.get('training_time', 0)\n",
    "        })\n",
    "\n",
    "# Create DataFrame and sort by best_val_f1 (descending)\n",
    "if table_data:\n",
    "    results_table = pd.DataFrame(table_data)\n",
    "    results_table = results_table.sort_values('best_val_f1', ascending=False)\n",
    "\n",
    "    print(\"MODEL BUILDING RESULTS TABLE (sorted by Best Val F1)\")\n",
    "    print(\"=\" * 100)\n",
    "    print(results_table.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total successful runs: {len(results_table)}\")\n",
    "    print(f\"Best F1 score: {results_table['best_val_f1'].max():.4f} ({results_table.loc[results_table['best_val_f1'].idxmax(), 'config_id']})\")\n",
    "    print(f\"Best accuracy: {results_table['best_val_acc'].max():.4f} ({results_table.loc[results_table['best_val_acc'].idxmax(), 'config_id']})\")\n",
    "    print(f\"Average F1 score: {results_table['best_val_f1'].mean():.4f}\")\n",
    "    print(f\"Average accuracy: {results_table['best_val_acc'].mean():.4f}\")\n",
    "    print(f\"Total training time: {results_table['training_time'].sum()/3600:.2f} hours\")\n",
    "    \n",
    "    # Top 5 models by F1\n",
    "    print(f\"\\nTOP 5 MODELS BY F1 SCORE:\")\n",
    "    top_5 = results_table.head(5)[['model', 'config', 'best_val_f1', 'best_val_acc']]\n",
    "    print(top_5.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "else:\n",
    "    print(\"No successful results found to display.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4x4 plot grid: F1 scores for each architecture across all configs + highest F1\n",
    "print(\"=\"*80)\n",
    "print(\"CREATING ARCHITECTURE COMPARISON PLOTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare data for plotting\n",
    "models_f1_data = {}\n",
    "configs = ['configA', 'configB', 'configC', 'configD']\n",
    "\n",
    "for model_name in model_architectures.keys():\n",
    "    models_f1_data[model_name] = {\n",
    "        'configA': 0,\n",
    "        'configB': 0, \n",
    "        'configC': 0,\n",
    "        'configD': 0,\n",
    "        'max_f1': 0\n",
    "    }\n",
    "    \n",
    "    # Extract F1 scores for each config\n",
    "    for config in configs:\n",
    "        key = f\"{model_name}_{config}\"\n",
    "        if key in building_results and building_results[key]['status'] == 'success':\n",
    "            f1_score = building_results[key]['result'].get('best_val_f1', 0)\n",
    "            models_f1_data[model_name][config] = f1_score\n",
    "    \n",
    "    # Calculate max F1 across all configs\n",
    "    config_f1s = [models_f1_data[model_name][config] for config in configs]\n",
    "    models_f1_data[model_name]['max_f1'] = max(config_f1s)\n",
    "\n",
    "# Create 4x4 subplot grid\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "fig.suptitle('Model Architecture Performance Comparison\\n(F1 Scores Across Configurations)', \n",
    "            fontsize=16, fontweight='bold')\n",
    "\n",
    "model_names = list(model_architectures.keys())\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Data for this model\n",
    "    data = models_f1_data[model_name]\n",
    "    \n",
    "    # Create bar plot with 5 bars: 4 configs + max\n",
    "    categories = ['A', 'B', 'C', 'D', 'Max']\n",
    "    values = [data['configA'], data['configB'], data['configC'], data['configD'], data['max_f1']]\n",
    "    colors = ['skyblue', 'lightgreen', 'orange', 'lightcoral', 'gold']\n",
    "    \n",
    "    bars = ax.bar(categories, values, color=colors, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    # Customize subplot\n",
    "    ax.set_title(f'{model_name}', fontweight='bold', fontsize=10)\n",
    "    ax.set_ylabel('F1 Score', fontsize=8)\n",
    "    ax.set_ylim(0, max(1.0, max(values) * 1.1))\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', labelsize=8)\n",
    "    ax.tick_params(axis='y', labelsize=8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        if value > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontsize=7, fontweight='bold')\n",
    "    \n",
    "    # Highlight the best performing config\n",
    "    if data['max_f1'] > 0:\n",
    "        best_config_idx = values[:-1].index(data['max_f1'])  # Exclude 'Max' from search\n",
    "        bars[best_config_idx].set_edgecolor('red')\n",
    "        bars[best_config_idx].set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2354b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of best performing models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ARCHITECTURE PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Sort models by max F1 score\n",
    "sorted_models = sorted(models_f1_data.items(), key=lambda x: x[1]['max_f1'], reverse=True)\n",
    "\n",
    "print(\"RANKING BY BEST F1 SCORE:\")\n",
    "print(\"-\" * 50)\n",
    "for rank, (model_name, data) in enumerate(sorted_models[:10], 1):\n",
    "    best_config = max(configs, key=lambda c: data[c])\n",
    "    print(f\"{rank:2d}. {model_name:<15} | Max F1: {data['max_f1']:.4f} | Best Config: {best_config} ({data[best_config]:.4f})\")\n",
    "\n",
    "# Architecture families analysis\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ARCHITECTURE FAMILY ANALYSIS:\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "family_performance = {\n",
    "    'ResNet-style': [],\n",
    "    'VGG-style': [],\n",
    "    'Attention-based': [],\n",
    "    'EfficientNet-style': [],\n",
    "    'Inception-style': [],\n",
    "    'DenseNet-style': [],\n",
    "    'Lightweight': [],\n",
    "    'Hybrid': []\n",
    "}\n",
    "\n",
    "# Categorize models (simplified categorization)\n",
    "model_families = {\n",
    "    'BirdCNN_v1': 'ResNet-style',\n",
    "    'BirdCNN_v15': 'ResNet-style',\n",
    "    'BirdCNN_v2': 'VGG-style',\n",
    "    'BirdCNN_v3': 'Attention-based',\n",
    "    'BirdCNN_v13': 'Attention-based',\n",
    "    'BirdCNN_v4': 'EfficientNet-style',\n",
    "    'BirdCNN_v5': 'Inception-style',\n",
    "    'BirdCNN_v6': 'DenseNet-style',\n",
    "    'BirdCNN_v7': 'Lightweight',\n",
    "    'BirdCNN_v11': 'Lightweight',\n",
    "    'BirdCNN_v8': 'ResNet-style',\n",
    "    'BirdCNN_v9': 'Hybrid',\n",
    "    'BirdCNN_v10': 'Hybrid',\n",
    "    'BirdCNN_v12': 'Hybrid',\n",
    "    'BirdCNN_v14': 'Hybrid',\n",
    "    'BirdCNN_v16': 'Hybrid'\n",
    "}\n",
    "\n",
    "for model_name, family in model_families.items():\n",
    "    max_f1 = models_f1_data[model_name]['max_f1']\n",
    "    family_performance[family].append(max_f1)\n",
    "\n",
    "for family, scores in family_performance.items():\n",
    "    if scores:\n",
    "        avg_score = np.mean(scores)\n",
    "        max_score = max(scores)\n",
    "        print(f\"{family:<18} | Avg F1: {avg_score:.4f} | Max F1: {max_score:.4f} | Count: {len(scores)}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af725a4",
   "metadata": {},
   "source": [
    "## Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis and Recommendations (based on F1 Score and architectural patterns)\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE ARCHITECTURAL ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Performance Analysis\n",
    "successful_results = [data for data in building_results.values() if data['status'] == 'success']\n",
    "if not successful_results:\n",
    "    print(\"No successful results to analyze.\")\n",
    "else:\n",
    "    # Extract F1 scores for analysis\n",
    "    all_f1_scores = [data['result']['best_val_f1'] for data in successful_results]\n",
    "    \n",
    "    print(\"1. STATISTICAL ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Mean F1 Score: {np.mean(all_f1_scores):.4f}\")\n",
    "    print(f\"Median F1 Score: {np.median(all_f1_scores):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(all_f1_scores):.4f}\")\n",
    "    print(f\"Min F1 Score: {np.min(all_f1_scores):.4f}\")\n",
    "    print(f\"Max F1 Score: {np.max(all_f1_scores):.4f}\")\n",
    "    print(f\"Range: {np.max(all_f1_scores) - np.min(all_f1_scores):.4f}\")\n",
    "    \n",
    "    # 2. Top Performing Models\n",
    "    print(f\"\\n2. TOP PERFORMING MODELS (F1 > {np.mean(all_f1_scores) + np.std(all_f1_scores):.3f})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    top_threshold = np.mean(all_f1_scores) + np.std(all_f1_scores)\n",
    "    top_performers = []\n",
    "    \n",
    "    for config_id, data in building_results.items():\n",
    "        if data['status'] == 'success':\n",
    "            f1_score = data['result']['best_val_f1']\n",
    "            if f1_score > top_threshold:\n",
    "                model_name = config_id.split('_config')[0]\n",
    "                config_name = config_id.split('_config')[1] if '_config' in config_id else 'unknown'\n",
    "                top_performers.append({\n",
    "                    'model': model_name,\n",
    "                    'config': config_name,\n",
    "                    'f1': f1_score,\n",
    "                    'acc': data['result']['best_val_acc']\n",
    "                })\n",
    "    \n",
    "    top_performers.sort(key=lambda x: x['f1'], reverse=True)\n",
    "    \n",
    "    for i, performer in enumerate(top_performers[:10], 1):\n",
    "        print(f\"{i:2d}. {performer['model']:<15} (Config {performer['config']}) | F1: {performer['f1']:.4f} | Acc: {performer['acc']:.4f}\")\n",
    "    \n",
    "    # 3. Configuration Analysis\n",
    "    print(f\"\\n3. CONFIGURATION PERFORMANCE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    config_performance = {}\n",
    "    for config in ['A', 'B', 'C', 'D']:\n",
    "        config_f1s = []\n",
    "        for config_id, data in building_results.items():\n",
    "            if data['status'] == 'success' and f'config{config}' in config_id:\n",
    "                config_f1s.append(data['result']['best_val_f1'])\n",
    "        \n",
    "        if config_f1s:\n",
    "            config_performance[config] = {\n",
    "                'mean': np.mean(config_f1s),\n",
    "                'median': np.median(config_f1s),\n",
    "                'std': np.std(config_f1s),\n",
    "                'max': np.max(config_f1s),\n",
    "                'count': len(config_f1s)\n",
    "            }\n",
    "    \n",
    "    config_ranking = sorted(config_performance.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "    \n",
    "    for rank, (config, stats) in enumerate(config_ranking, 1):\n",
    "        config_name = configurations[f'config{config}']['name']\n",
    "        print(f\"{rank}. Config {config} ({config_name})\")\n",
    "        print(f\"   Mean F1: {stats['mean']:.4f} Â± {stats['std']:.4f} | Max F1: {stats['max']:.4f} | Runs: {stats['count']}\")\n",
    "    \n",
    "    # 4. Architecture Family Analysis\n",
    "    print(f\"\\n4. ARCHITECTURE FAMILY INSIGHTS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    family_insights = {\n",
    "        'ResNet-style': \"Deep residual connections enable training of very deep networks and help with gradient flow.\",\n",
    "        'VGG-style': \"Simple, sequential architecture that's easy to understand but may struggle with vanishing gradients.\",\n",
    "        'Attention-based': \"Attention mechanisms can focus on relevant frequency bands and temporal features.\",\n",
    "        'EfficientNet-style': \"Compound scaling and inverted residuals provide efficient parameter usage.\",\n",
    "        'Inception-style': \"Multi-scale feature extraction captures patterns at different temporal/frequency scales.\",\n",
    "        'DenseNet-style': \"Dense connections promote feature reuse and reduce parameter redundancy.\",\n",
    "        'Lightweight': \"Designed for efficiency, may trade some accuracy for reduced computational cost.\",\n",
    "        'Hybrid': \"Combines CNN and RNN components for both local and sequential pattern recognition.\"\n",
    "    }\n",
    "    \n",
    "    for family, scores in family_performance.items():\n",
    "        if scores:\n",
    "            avg_score = np.mean(scores)\n",
    "            best_score = max(scores)\n",
    "            print(f\"\\n{family}:\")\n",
    "            print(f\"  Average F1: {avg_score:.4f} | Best F1: {best_score:.4f}\")\n",
    "            print(f\"  Insight: {family_insights.get(family, 'No specific insight available.')}\")\n",
    "    \n",
    "    # 5. Key Recommendations\n",
    "    print(f\"\\n5. KEY RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_overall = max(building_results.items(), key=lambda x: x[1]['result']['best_val_f1'] if x[1]['status'] == 'success' else 0)\n",
    "    best_model = best_overall[0].split('_config')[0]\n",
    "    best_config = best_overall[0].split('_config')[1] if '_config' in best_overall[0] else 'unknown'\n",
    "    best_f1 = best_overall[1]['result']['best_val_f1']\n",
    "    \n",
    "    print(f\"ðŸ† BEST OVERALL MODEL: {best_model} with Config {best_config}\")\n",
    "    print(f\"   F1 Score: {best_f1:.4f}\")\n",
    "    print(f\"   Accuracy: {best_overall[1]['result']['best_val_acc']:.4f}\")\n",
    "    \n",
    "    # Configuration recommendations\n",
    "    best_config_name = config_ranking[0][0]\n",
    "    worst_config_name = config_ranking[-1][0]\n",
    "    \n",
    "    print(f\"\\nðŸ“Š CONFIGURATION INSIGHTS:\")\n",
    "    print(f\"   â€¢ Best performing config: Config {best_config_name} ({configurations[f'config{best_config_name}']['name']})\")\n",
    "    print(f\"   â€¢ Config {best_config_name} achieved mean F1 of {config_performance[best_config_name]['mean']:.4f}\")\n",
    "    print(f\"   â€¢ Config {worst_config_name} had lowest mean F1 of {config_performance[worst_config_name]['mean']:.4f}\")\n",
    "    \n",
    "    # Architecture recommendations\n",
    "    top_3_families = sorted([(family, np.mean(scores)) for family, scores in family_performance.items() if scores], \n",
    "                           key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    print(f\"\\nðŸ—ï¸  ARCHITECTURE INSIGHTS:\")\n",
    "    for rank, (family, avg_f1) in enumerate(top_3_families, 1):\n",
    "        print(f\"   {rank}. {family} architectures (avg F1: {avg_f1:.4f})\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ RECOMMENDATIONS FOR FUTURE WORK:\")\n",
    "    print(f\"   1. Focus on {best_model} architecture as the base model\")\n",
    "    print(f\"   2. Use Config {best_config_name} settings as starting point for hyperparameter tuning\")\n",
    "    print(f\"   3. Consider ensemble of top 3-5 performing models\")\n",
    "    print(f\"   4. Investigate {top_3_families[0][0]} architectures further\")\n",
    "    print(f\"   5. Cross-validation with best model-config combinations\")\n",
    "    \n",
    "    # Model complexity vs performance\n",
    "    print(f\"\\nâš¡ EFFICIENCY CONSIDERATIONS:\")\n",
    "    training_times = [data['training_time'] for data in successful_results if 'training_time' in data]\n",
    "    if training_times:\n",
    "        avg_time = np.mean(training_times)\n",
    "        print(f\"   â€¢ Average training time: {avg_time:.1f} seconds ({avg_time/60:.1f} minutes)\")\n",
    "        print(f\"   â€¢ Consider lightweight models for production deployment\")\n",
    "        print(f\"   â€¢ Balance between performance and computational cost\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save json and csv\n",
    "\n",
    "# Save complete results as JSON and CSV\n",
    "print(\"=\"*80)\n",
    "print(\"SAVING RESULTS TO FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create reports directory if it doesn't exist\n",
    "reports_dir = os.path.join('..', 'models', 'reports')\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# Prepare timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 1. Save complete results as JSON\n",
    "json_filename = f'complete_building_results_{timestamp}.json'\n",
    "json_path = os.path.join(reports_dir, json_filename)\n",
    "\n",
    "# Convert numpy types to native Python types for JSON serialization\n",
    "def convert_for_json(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_for_json(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Prepare JSON data with metadata\n",
    "json_data = {\n",
    "    'metadata': {\n",
    "        'timestamp': timestamp,\n",
    "        'total_models': len(model_architectures),\n",
    "        'total_configs': len(configurations),\n",
    "        'total_combinations': len(model_architectures) * len(configurations),\n",
    "        'successful_runs': sum(1 for result in building_results.values() if result['status'] == 'success'),\n",
    "        'dataset_info': {\n",
    "            'num_samples': len(features),\n",
    "            'num_classes': len(np.unique(labels)),\n",
    "            'feature_shape': list(features.shape),\n",
    "            'split_seed_single': 245323,\n",
    "            'split_seed_kfold': 11052\n",
    "        }\n",
    "    },\n",
    "    'configurations': configurations,\n",
    "    'model_architectures': list(model_architectures.keys()),\n",
    "    'results': convert_for_json(building_results)\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(json_data, f, indent=2)\n",
    "    print(f\"âœ… Saved complete results to: {json_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to save JSON: {str(e)}\")\n",
    "\n",
    "# 2. Save results table as CSV\n",
    "csv_filename = f'building_results_{timestamp}.csv'\n",
    "csv_path = os.path.join(reports_dir, csv_filename)\n",
    "\n",
    "# Prepare CSV data\n",
    "csv_data = []\n",
    "for config_id, data in building_results.items():\n",
    "    # Parse model name and config\n",
    "    parts = config_id.split('_config')\n",
    "    model_name = parts[0]\n",
    "    config_name = 'config' + parts[1] if len(parts) > 1 else 'unknown'\n",
    "    \n",
    "    row = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_name': model_name,\n",
    "        'config_name': config_name,\n",
    "        'config_id': config_id,\n",
    "        'status': data['status'],\n",
    "        'training_time_seconds': data.get('training_time', 0)\n",
    "    }\n",
    "    \n",
    "    if data['status'] == 'success' and 'result' in data:\n",
    "        result = data['result']\n",
    "        row.update({\n",
    "            'best_val_acc': result.get('best_val_acc', 0),\n",
    "            'best_val_f1': result.get('best_val_f1', 0),\n",
    "            'final_val_acc': result.get('final_val_acc', 0),\n",
    "            'final_val_f1': result.get('final_val_f1', 0),\n",
    "            'best_train_acc': result.get('best_train_acc', 0),\n",
    "            'best_train_f1': result.get('best_train_f1', 0),\n",
    "            'final_train_acc': result.get('final_train_acc', 0),\n",
    "            'final_train_f1': result.get('final_train_f1', 0),\n",
    "            'best_epoch': result.get('best_epoch', 0),\n",
    "            'total_epochs': result.get('total_epochs', 0),\n",
    "            'early_stopped': result.get('early_stopped', False)\n",
    "        })\n",
    "    else:\n",
    "        # Fill with default values for failed runs\n",
    "        for metric in ['best_val_acc', 'best_val_f1', 'final_val_acc', 'final_val_f1',\n",
    "                      'best_train_acc', 'best_train_f1', 'final_train_acc', 'final_train_f1']:\n",
    "            row[metric] = 0\n",
    "        row.update({\n",
    "            'best_epoch': 0,\n",
    "            'total_epochs': 0,\n",
    "            'early_stopped': False,\n",
    "            'error_message': data.get('error', '')\n",
    "        })\n",
    "    \n",
    "    csv_data.append(row)\n",
    "\n",
    "try:\n",
    "    csv_df = pd.DataFrame(csv_data)\n",
    "    csv_df.to_csv(csv_path, index=False, float_format='%.6f')\n",
    "    print(f\"âœ… Saved CSV results to: {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to save CSV: {str(e)}\")\n",
    "\n",
    "# 3. Save summary statistics\n",
    "summary_filename = f'building_summary_{timestamp}.txt'\n",
    "summary_path = os.path.join(reports_dir, summary_filename)\n",
    "\n",
    "try:\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(\"MODEL BUILDING EXPERIMENT SUMMARY\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Timestamp: {timestamp}\\n\")\n",
    "        f.write(f\"Total Models Tested: {len(model_architectures)}\\n\")\n",
    "        f.write(f\"Total Configurations: {len(configurations)}\\n\")\n",
    "        f.write(f\"Total Combinations: {len(model_architectures) * len(configurations)}\\n\")\n",
    "        \n",
    "        successful_count = sum(1 for result in building_results.values() if result['status'] == 'success')\n",
    "        f.write(f\"Successful Runs: {successful_count}\\n\")\n",
    "        f.write(f\"Success Rate: {successful_count/(len(model_architectures) * len(configurations))*100:.1f}%\\n\\n\")\n",
    "        \n",
    "        if successful_count > 0:\n",
    "            # Best results\n",
    "            best_result = max(building_results.items(), \n",
    "                            key=lambda x: x[1]['result']['best_val_f1'] if x[1]['status'] == 'success' else 0)\n",
    "            f.write(\"BEST RESULTS:\\n\")\n",
    "            f.write(f\"Best Model-Config: {best_result[0]}\\n\")\n",
    "            f.write(f\"Best F1 Score: {best_result[1]['result']['best_val_f1']:.4f}\\n\")\n",
    "            f.write(f\"Best Accuracy: {best_result[1]['result']['best_val_acc']:.4f}\\n\\n\")\n",
    "            \n",
    "            # Performance statistics\n",
    "            all_f1_scores = [data['result']['best_val_f1'] for data in building_results.values() \n",
    "                           if data['status'] == 'success']\n",
    "            f.write(\"PERFORMANCE STATISTICS:\\n\")\n",
    "            f.write(f\"Mean F1: {np.mean(all_f1_scores):.4f}\\n\")\n",
    "            f.write(f\"Median F1: {np.median(all_f1_scores):.4f}\\n\")\n",
    "            f.write(f\"Std F1: {np.std(all_f1_scores):.4f}\\n\")\n",
    "            f.write(f\"Min F1: {np.min(all_f1_scores):.4f}\\n\")\n",
    "            f.write(f\"Max F1: {np.max(all_f1_scores):.4f}\\n\")\n",
    "            \n",
    "    print(f\"âœ… Saved summary to: {summary_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to save summary: {str(e)}\")\n",
    "\n",
    "# 4. List all generated files\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GENERATED FILES:\")\n",
    "print(f\"{'='*60}\")\n",
    "for filename in [json_filename, csv_filename, summary_filename]:\n",
    "    filepath = os.path.join(reports_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024*1024)\n",
    "        print(f\"ðŸ“ {filename} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to: {os.path.abspath(reports_dir)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3765a4b",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Model Architecture Sweeping System - Complete Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive model architecture sweeping system for bird species classification using grayscale log-mel spectrograms. The system tests **16 different CNN architectures** across **4 optimized training configurations** using single-fold training.\n",
    "\n",
    "## System Components\n",
    "\n",
    "### ðŸ“Š Dataset\n",
    "- **Input Shape**: 1Ã—224Ã—313 grayscale log-mel spectrograms\n",
    "- **Total Samples**: ~2400 samples\n",
    "- **Classes**: Multiple bird species\n",
    "- **Split Strategy**: Single-fold (80% train, 20% validation) with set seed for reproducibility\n",
    "\n",
    "### ðŸ—ï¸ Architecture Portfolio (16 Models)\n",
    "1. **BirdCNN_v1**: ResNet-style with residual blocks\n",
    "2. **BirdCNN_v2**: VGG-style deep CNN\n",
    "3. **BirdCNN_v3**: PANN-inspired with attention pooling  \n",
    "4. **BirdCNN_v4**: EfficientNet-inspired with MBConv blocks\n",
    "5. **BirdCNN_v5**: Inception-style with multi-scale convolutions\n",
    "6. **BirdCNN_v6**: DenseNet-inspired with dense connections\n",
    "7. **BirdCNN_v7**: ShuffleNet-inspired with channel shuffle\n",
    "8. **BirdCNN_v8**: RegNet-inspired with group convolutions\n",
    "9. **BirdCNN_v9**: Frequency-aware CNN with split processing\n",
    "10. **BirdCNN_v10**: Hybrid CNN-RNN with LSTM\n",
    "11. **BirdCNN_v11**: Lightweight MobileNet-style\n",
    "12. **BirdCNN_v12**: Multi-scale feature extraction\n",
    "13. **BirdCNN_v13**: Frequency and temporal attention\n",
    "14. **BirdCNN_v14**: Hierarchical feature learning\n",
    "15. **BirdCNN_v15**: Enhanced ResNet with SE blocks\n",
    "16. **BirdCNN_v16**: Ensemble-like multi-path CNN\n",
    "\n",
    "### âš™ï¸ Training Configurations (4 Configs)\n",
    "- **Config A**: Parameters Frankenstein (Adam, 40 batch size, exponential LR)\n",
    "- **Config B**: Balanced Classes Focus (Adam, 28 batch size, class weights)\n",
    "- **Config C**: Chaos Theory (Cosine LR, 45 batch size, no standardization)\n",
    "- **Config D**: Balanced Classes AdamW (AdamW optimizer, 32 batch size)\n",
    "\n",
    "### ðŸ“ˆ Analysis & Visualization\n",
    "1. **Training Metrics Plots**: Loss and accuracy curves for all successful runs\n",
    "2. **Comprehensive Results Table**: Sortable table with all metrics\n",
    "3. **4Ã—4 Grid Visualization**: F1 scores across configs + max performance per model\n",
    "4. **Statistical Analysis**: Performance distribution and family comparisons\n",
    "5. **Architectural Insights**: Family-based performance analysis\n",
    "6. **Recommendations**: Data-driven suggestions for future work\n",
    "\n",
    "### ðŸ’¾ Results Export\n",
    "- **JSON**: Complete results with metadata (`complete_building_results_TIMESTAMP.json`)\n",
    "- **CSV**: Structured table for analysis (`building_results_TIMESTAMP.csv`)\n",
    "- **Summary**: Human-readable summary (`building_summary_TIMESTAMP.txt`)\n",
    "\n",
    "## Execution Instructions\n",
    "\n",
    "1. **Run Data Loading Cells** (1-3): Load spectrograms and split data\n",
    "2. **Run Training Cells** (4-7): Execute all 64 model-config combinations (16Ã—4)\n",
    "3. **Run Analysis Cells** (8-11): Generate visualizations and insights\n",
    "4. **Run Export Cell** (12): Save all results to files\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "- **Total Training Runs**: 64 (16 models Ã— 4 configs)\n",
    "- **Estimated Time**: ~16 hours (15 min average per run)\n",
    "- **Success Rate**: Typically 85-95% depending on resource availability\n",
    "- **Output**: Complete performance comparison enabling architecture selection\n",
    "\n",
    "## Key Features\n",
    "\n",
    "âœ… **Comprehensive**: Tests diverse architectural patterns  \n",
    "âœ… **Reproducible**: Fixed seeds and structured evaluation  \n",
    "âœ… **Scalable**: Easy to add new models or configurations  \n",
    "âœ… **Analytical**: Rich visualization and statistical analysis  \n",
    "âœ… **Exportable**: Multiple output formats for further analysis  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Based on results, proceed with:\n",
    "1. Cross-validation of top performing models\n",
    "2. Hyperparameter optimization for best architectures\n",
    "3. Ensemble methods combining top performers\n",
    "4. Production deployment considerations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
