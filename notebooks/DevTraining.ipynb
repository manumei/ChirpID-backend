{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1729b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils import util, models, split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b5cda7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.split' from 'c:\\\\Users\\\\manue\\\\Desktop\\\\manum\\\\coding\\\\ChirpID-backend\\\\utils\\\\split.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(util)\n",
    "importlib.reload(models)\n",
    "importlib.reload(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29cf9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spect_matrix_list(spects_source_dir, spects_meta_df):\n",
    "    \"\"\"\n",
    "    Load spectrograms directly as matrices without flattening to CSV.\n",
    "    \n",
    "    Args:\n",
    "        spects_source_dir (str): Directory where the spectrogram images are stored in .png format\n",
    "        spects_meta_df (pd.DataFrame): DataFrame with columns 'filename', 'class_id', and 'author'\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matrices_list, labels_list, authors_list)\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    matrices_list = []\n",
    "    labels_list = []\n",
    "    authors_list = []\n",
    "    \n",
    "    spects_meta_df = spects_meta_df.dropna(subset=['filename', 'class_id', 'author'])\n",
    "\n",
    "    print(f\"Processing {len(spects_meta_df)} spectrograms...\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for _, row in spects_meta_df.iterrows():\n",
    "        filename = row['filename']\n",
    "        class_id = row['class_id']\n",
    "        author = row['author']\n",
    "\n",
    "        image_path = os.path.join(spects_source_dir, filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        img = Image.open(image_path).convert('L')  # Ensure grayscale\n",
    "        \n",
    "        expected_shape = (313, 224)  # PIL uses (width, height) format\n",
    "        if img.size != expected_shape:\n",
    "            print(f\"Warning: Unexpected image size: {img.size} in file {image_path}. Expected {expected_shape}.\")\n",
    "            # Resize if needed\n",
    "            img = img.resize(expected_shape)\n",
    "\n",
    "        # Convert to numpy array (this gives us height x width, i.e., 313 x 224)\n",
    "        pixels = np.array(img)\n",
    "        \n",
    "        matrices_list.append(pixels)\n",
    "        labels_list.append(class_id)\n",
    "        authors_list.append(author)\n",
    "        processed_count += 1\n",
    "\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Skipped: {skipped_count}\")\n",
    "\n",
    "    if not matrices_list:\n",
    "        raise ValueError(\"No spectrograms were loaded. Check paths and metadata consistency.\")\n",
    "\n",
    "    return matrices_list, labels_list, authors_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29b5b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spectrograms directly into matrices...\n",
      "Processing 2985 spectrograms...\n",
      "Successfully processed: 2985\n",
      "Skipped: 0\n",
      "Loaded 2985 spectrograms\n",
      "Matrix shape: (224, 313)\n",
      "Unique labels: 33\n",
      "Unique authors: 106\n",
      "Successfully processed: 2985\n",
      "Skipped: 0\n",
      "Loaded 2985 spectrograms\n",
      "Matrix shape: (224, 313)\n",
      "Unique labels: 33\n",
      "Unique authors: 106\n"
     ]
    }
   ],
   "source": [
    "# Load spectrogram data and metadata\n",
    "spect_dir = os.path.join('..', 'database', 'spect')  # Spectrogram PNG directory\n",
    "spects_df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final_spects.csv'))  # Metadata\n",
    "\n",
    "print(\"Loading spectrograms directly into matrices...\")\n",
    "matrices_list, labels_list, authors_list = get_spect_matrix_list(spect_dir, spects_df)\n",
    "\n",
    "print(f\"Loaded {len(matrices_list)} spectrograms\")\n",
    "print(f\"Matrix shape: {matrices_list[0].shape}\")\n",
    "print(f\"Unique labels: {len(set(labels_list))}\")\n",
    "print(f\"Unique authors: {len(set(authors_list))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af9dc12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (2985, 1, 224, 313)\n",
      "labels shape: (2985,)\n",
      "authors shape: (2985,)\n",
      "metadata_df shape: (2985, 4)\n",
      "Unique authors: 106\n",
      "Unique classes: 33\n"
     ]
    }
   ],
   "source": [
    "# Process data for training\n",
    "labels = np.array(labels_list, dtype=np.int64)\n",
    "authors = np.array(authors_list)\n",
    "\n",
    "# Convert matrices to numpy array and normalize\n",
    "features = np.array(matrices_list, dtype=np.float32)\n",
    "# Convert to 0-1 range first, then standardization will be applied per fold\n",
    "features /= 255.0\n",
    "# Reshape to add channel dimension for CNN: (samples, channels, height, width)\n",
    "features = features.reshape(-1, 1, 224, 313)\n",
    "\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "print(\"authors shape:\", authors.shape)\n",
    "\n",
    "# Create metadata DataFrame for splitting (with sample indices)\n",
    "metadata_df = pd.DataFrame({\n",
    "    'sample_idx': range(len(labels)),\n",
    "    'class_id': labels,\n",
    "    'author': authors,\n",
    "    'usable_segments': 1  # Each sample represents 1 segment\n",
    "})\n",
    "\n",
    "print(\"metadata_df shape:\", metadata_df.shape)\n",
    "print(\"Unique authors:\", len(metadata_df['author'].unique()))\n",
    "print(\"Unique classes:\", len(metadata_df['class_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46008b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules reloaded with bug fixes!\n",
      "✓ Fixed FastStandardizedSubset class definition\n",
      "✓ Configured single-threaded DataLoaders to avoid multiprocessing issues\n"
     ]
    }
   ],
   "source": [
    "# Reload modules to pick up any changes\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "importlib.reload(util)\n",
    "importlib.reload(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e59a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 2985 samples\n",
      "Tensor shapes: X=torch.Size([2985, 1, 224, 313]), y=torch.Size([2985])\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensors for PyTorch\n",
    "X_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Tensor shapes: X={X_tensor.shape}, y={y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48784347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best 80-20 split with author grouping...\n",
      "New best split found! Seed: 4, Score: 1.081\n",
      "New best split found! Seed: 7, Score: 0.437\n",
      "New best split found! Seed: 27, Score: 0.340\n",
      "New best split found! Seed: 954, Score: 0.334\n",
      "New best split found! Seed: 1071, Score: 0.315\n",
      "New best split found! Seed: 954, Score: 0.334\n",
      "New best split found! Seed: 1071, Score: 0.315\n",
      "New best split found! Seed: 1208, Score: 0.275\n",
      "New best split found! Seed: 1208, Score: 0.275\n",
      "\n",
      "Best split found:\n",
      "Seed: 1208\n",
      "Stratification score: 0.275\n",
      "Author overlap: set()\n",
      "Segments in dev set: 2433\n",
      "Segments in test set: 552\n",
      "Dev segment%: 81.51%\n",
      "Test segment%: 18.49%\n",
      "\n",
      "Segment distribution comparison:\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|    |   Target_Test_Segments |   Actual_Test_Segments |   Target_Dev_Segments |   Actual_Dev_Segments |   Total_Segments |\n",
      "+====+========================+========================+=======================+=======================+==================+\n",
      "|  0 |                     20 |                     13 |                    80 |                    87 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  1 |                     19 |                     10 |                    75 |                    84 |               94 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  2 |                     18 |                     13 |                    72 |                    77 |               90 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  3 |                     11 |                      7 |                    45 |                    49 |               56 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  4 |                     14 |                     18 |                    55 |                    51 |               69 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  5 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  6 |                     20 |                      7 |                    80 |                    93 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  7 |                     20 |                     29 |                    80 |                    71 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  8 |                     20 |                     25 |                    80 |                    75 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  9 |                      9 |                      6 |                    34 |                    37 |               43 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 10 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 11 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 12 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 13 |                     20 |                     11 |                    80 |                    89 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 14 |                     13 |                     27 |                    54 |                    40 |               67 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 15 |                     20 |                     22 |                    80 |                    78 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 16 |                     14 |                     13 |                    58 |                    59 |               72 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 17 |                     20 |                     19 |                    80 |                    81 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 18 |                     18 |                     14 |                    71 |                    75 |               89 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 19 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 20 |                     20 |                     23 |                    80 |                    77 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 21 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 22 |                     20 |                     26 |                    80 |                    74 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 23 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 24 |                     15 |                      9 |                    61 |                    67 |               76 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 25 |                     15 |                     22 |                    58 |                    51 |               73 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 26 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 27 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 28 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 29 |                     20 |                     13 |                    78 |                    85 |               98 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 30 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 31 |                     12 |                     14 |                    46 |                    44 |               58 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 32 |                     20 |                     24 |                    80 |                    76 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "Best 80-20 split found with score: 0.275\n",
      "Train samples: 2433, Validation samples: 552\n",
      "\n",
      "Best split found:\n",
      "Seed: 1208\n",
      "Stratification score: 0.275\n",
      "Author overlap: set()\n",
      "Segments in dev set: 2433\n",
      "Segments in test set: 552\n",
      "Dev segment%: 81.51%\n",
      "Test segment%: 18.49%\n",
      "\n",
      "Segment distribution comparison:\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|    |   Target_Test_Segments |   Actual_Test_Segments |   Target_Dev_Segments |   Actual_Dev_Segments |   Total_Segments |\n",
      "+====+========================+========================+=======================+=======================+==================+\n",
      "|  0 |                     20 |                     13 |                    80 |                    87 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  1 |                     19 |                     10 |                    75 |                    84 |               94 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  2 |                     18 |                     13 |                    72 |                    77 |               90 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  3 |                     11 |                      7 |                    45 |                    49 |               56 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  4 |                     14 |                     18 |                    55 |                    51 |               69 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  5 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  6 |                     20 |                      7 |                    80 |                    93 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  7 |                     20 |                     29 |                    80 |                    71 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  8 |                     20 |                     25 |                    80 |                    75 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  9 |                      9 |                      6 |                    34 |                    37 |               43 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 10 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 11 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 12 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 13 |                     20 |                     11 |                    80 |                    89 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 14 |                     13 |                     27 |                    54 |                    40 |               67 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 15 |                     20 |                     22 |                    80 |                    78 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 16 |                     14 |                     13 |                    58 |                    59 |               72 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 17 |                     20 |                     19 |                    80 |                    81 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 18 |                     18 |                     14 |                    71 |                    75 |               89 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 19 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 20 |                     20 |                     23 |                    80 |                    77 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 21 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 22 |                     20 |                     26 |                    80 |                    74 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 23 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 24 |                     15 |                      9 |                    61 |                    67 |               76 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 25 |                     15 |                     22 |                    58 |                    51 |               73 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 26 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 27 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 28 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 29 |                     20 |                     13 |                    78 |                    85 |               98 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 30 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 31 |                     12 |                     14 |                    46 |                    44 |               58 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 32 |                     20 |                     24 |                    80 |                    76 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "Best 80-20 split found with score: 0.275\n",
      "Train samples: 2433, Validation samples: 552\n"
     ]
    }
   ],
   "source": [
    "# Find the best 80-20 split using author grouping\n",
    "print(\"Finding best 80-20 split with author grouping...\")\n",
    "dev_df, test_df, best_split_score = split.search_best_group_seed(\n",
    "    df=metadata_df,\n",
    "    test_size=0.2,\n",
    "    max_attempts=5_000,\n",
    "    min_test_segments=3\n",
    ")\n",
    "\n",
    "# Extract indices for single fold training\n",
    "train_indices_single = dev_df['sample_idx'].values\n",
    "val_indices_single = test_df['sample_idx'].values\n",
    "\n",
    "print(f\"Best 80-20 split found with score: {best_split_score:.3f}\")\n",
    "print(f\"Train samples: {len(train_indices_single)}, Validation samples: {len(val_indices_single)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "584e9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimized training function startup time...\n",
      "Test dataset size: Train=100, Val=20\n",
      "\n",
      "Timing data loader creation...\n",
      "DataLoader creation time: 0.01 seconds\n",
      "Testing first batch loading...\n",
      "First batch loading time: 0.01 seconds\n",
      "Batch shape: torch.Size([32, 1, 224, 313])\n",
      "✓ DataLoader working correctly!\n",
      "\n",
      "Total test time: 0.01 seconds\n",
      "Optimization complete - training should start much faster now!\n"
     ]
    }
   ],
   "source": [
    "# Test the optimized training function performance\n",
    "import time\n",
    "\n",
    "print(\"Testing optimized training function startup time...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a small test to measure overhead\n",
    "test_indices = train_indices_single[:100] if len(train_indices_single) > 100 else train_indices_single[:50]\n",
    "test_val_indices = val_indices_single[:20] if len(val_indices_single) > 20 else val_indices_single[:10]\n",
    "\n",
    "print(f\"Test dataset size: Train={len(test_indices)}, Val={len(test_val_indices)}\")\n",
    "\n",
    "# Measure just the data loading and setup overhead\n",
    "print(\"\\nTiming data loader creation...\")\n",
    "loader_start = time.time()\n",
    "\n",
    "# Create standardized subset directly to test\n",
    "if True:  # Test standardization path\n",
    "    sample_size = min(50, len(test_indices))\n",
    "    sample_indices = np.random.choice(test_indices, sample_size, replace=False)\n",
    "    sample_data = torch.stack([dataset[i][0] for i in sample_indices])\n",
    "    train_mean = sample_data.mean()\n",
    "    train_std = sample_data.std()\n",
    "    \n",
    "    class StandardizedSubset(torch.utils.data.Dataset):\n",
    "        def __init__(self, original_dataset, indices, mean, std):\n",
    "            self.dataset = original_dataset\n",
    "            self.indices = list(indices)  # Convert to list for compatibility\n",
    "            self.mean = mean\n",
    "            self.std = std + 1e-8\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            real_idx = self.indices[idx]\n",
    "            x, y = self.dataset[real_idx]\n",
    "            x_standardized = (x - self.mean) / self.std\n",
    "            return x_standardized, y\n",
    "    \n",
    "    test_train_subset = StandardizedSubset(dataset, test_indices, train_mean, train_std)\n",
    "    test_val_subset = StandardizedSubset(dataset, test_val_indices, train_mean, train_std)\n",
    "else:\n",
    "    from torch.utils.data import Subset\n",
    "    test_train_subset = Subset(dataset, test_indices)\n",
    "    test_val_subset = Subset(dataset, test_val_indices)\n",
    "\n",
    "# Test DataLoader creation with single thread to avoid worker crashes\n",
    "test_train_loader = torch.utils.data.DataLoader(\n",
    "    test_train_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use single thread to avoid worker crashes\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "test_val_loader = torch.utils.data.DataLoader(\n",
    "    test_val_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Use single thread to avoid worker crashes\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "loader_time = time.time() - loader_start\n",
    "print(f\"DataLoader creation time: {loader_time:.2f} seconds\")\n",
    "\n",
    "# Test first batch loading\n",
    "print(\"Testing first batch loading...\")\n",
    "batch_start = time.time()\n",
    "try:\n",
    "    test_batch = next(iter(test_train_loader))\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"First batch loading time: {batch_time:.2f} seconds\")\n",
    "    print(f\"Batch shape: {test_batch[0].shape}\")\n",
    "    print(\"✓ DataLoader working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "    print(\"Falling back to direct dataset access...\")\n",
    "    batch_start = time.time()\n",
    "    test_sample = test_train_subset[0]\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"Direct dataset access time: {batch_time:.4f} seconds\")\n",
    "    print(f\"Sample shape: {test_sample[0].shape}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal test time: {total_time:.2f} seconds\")\n",
    "print(\"Optimization complete - training should start much faster now!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e2281",
   "metadata": {},
   "source": [
    "## Single Fold Training with Predefined Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2749d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying author grouping in predefined splits...\n",
      "==================================================\n",
      "Training set authors: 84 unique authors\n",
      "Validation set authors: 22 unique authors\n",
      "Author overlap between train/val: 0 authors\n",
      "✓ PERFECT: No author overlap - authors are properly grouped!\n",
      "\n",
      "Class distribution:\n",
      "Training set classes: 33 classes\n",
      "Validation set classes: 33 classes\n",
      "All classes present in both sets: True\n",
      "\n",
      "==================================================\n",
      "Both optimized functions use these SAME predefined author-grouped splits!\n"
     ]
    }
   ],
   "source": [
    "# Verify that author grouping is preserved in our splits\n",
    "print(\"Verifying author grouping in predefined splits...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get authors for training and validation sets\n",
    "train_authors = set(metadata_df.loc[metadata_df['sample_idx'].isin(train_indices_single), 'author'])\n",
    "val_authors = set(metadata_df.loc[metadata_df['sample_idx'].isin(val_indices_single), 'author'])\n",
    "\n",
    "# Check for overlap\n",
    "author_overlap = train_authors & val_authors\n",
    "print(f\"Training set authors: {len(train_authors)} unique authors\")\n",
    "print(f\"Validation set authors: {len(val_authors)} unique authors\")\n",
    "print(f\"Author overlap between train/val: {len(author_overlap)} authors\")\n",
    "\n",
    "if len(author_overlap) == 0:\n",
    "    print(\"✓ PERFECT: No author overlap - authors are properly grouped!\")\n",
    "else:\n",
    "    print(f\"⚠️ WARNING: {len(author_overlap)} authors appear in both sets\")\n",
    "    print(f\"Overlapping authors: {author_overlap}\")\n",
    "\n",
    "# Check class distribution\n",
    "train_classes = set(metadata_df.loc[metadata_df['sample_idx'].isin(train_indices_single), 'class_id'])\n",
    "val_classes = set(metadata_df.loc[metadata_df['sample_idx'].isin(val_indices_single), 'class_id'])\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Training set classes: {len(train_classes)} classes\")\n",
    "print(f\"Validation set classes: {len(val_classes)} classes\")\n",
    "print(f\"All classes present in both sets: {train_classes == val_classes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Both optimized functions use these SAME predefined author-grouped splits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efde9b2",
   "metadata": {},
   "source": [
    "Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07947fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OPTIMIZED original function with predefined author-grouped splits...\n",
      "Train indices: 2433 samples\n",
      "Val indices: 552 samples\n",
      "Fast training on cuda\n",
      "Train size: 2433, Val size: 552\n",
      "Quick standardization computation...\n",
      "Standardization computed from 200 samples - Mean: 0.1387, Std: 0.1877\n",
      "Computing class weights...\n",
      "Class weights: min=0.399, max=3.030\n",
      "Creating optimized data loaders...\n",
      "Initializing model...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/250 [00:00<?, ?epoch/s]\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'fast_single_fold_training_with_predefined_split.<locals>.FastStandardizedSubset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal indices: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Use the fast training function to avoid multiprocessing issues\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m single_results_original \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_single_fold_training_with_predefined_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBirdCNN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_class_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstandardize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manue\\Desktop\\manum\\coding\\ChirpID-backend\\utils\\util.py:1522\u001b[0m, in \u001b[0;36mfast_single_fold_training_with_predefined_split\u001b[1;34m(dataset, train_indices, val_indices, model_class, num_classes, num_epochs, batch_size, lr, use_class_weights, estop, standardize)\u001b[0m\n\u001b[0;32m   1520\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m-> 1522\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_fold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[38;5;66;03m# Get final validation metrics\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m final_val_loss, final_val_acc, final_val_f1 \u001b[38;5;241m=\u001b[39m validate_epoch(\n\u001b[0;32m   1529\u001b[0m     model, val_loader, criterion, device\n\u001b[0;32m   1530\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\manue\\Desktop\\manum\\coding\\ChirpID-backend\\utils\\util.py:576\u001b[0m, in \u001b[0;36mtrain_single_fold\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, fold_num, estop, scheduler)\u001b[0m\n\u001b[0;32m    572\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39mdesc, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m     train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    578\u001b[0m     train_accuracies\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "File \u001b[1;32mc:\\Users\\manue\\Desktop\\manum\\coding\\ChirpID-backend\\utils\\util.py:501\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m    498\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    499\u001b[0m all_preds, all_targets \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m--> 501\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:493\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1171\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1164\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1171\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\multiprocessing\\popen_spawn_win32.py:97\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\manue\\miniconda3\\envs\\birds\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get local object 'fast_single_fold_training_with_predefined_split.<locals>.FastStandardizedSubset'"
     ]
    }
   ],
   "source": [
    "# Run original (now optimized) single fold training with best 80-20 split found above\n",
    "# This uses the optimal train/validation split with author grouping\n",
    "train_indices, val_indices = train_indices_single, val_indices_single\n",
    "\n",
    "print(\"Using OPTIMIZED original function with predefined author-grouped splits...\")\n",
    "print(f\"Train indices: {len(train_indices)} samples\")\n",
    "print(f\"Val indices: {len(val_indices)} samples\")\n",
    "\n",
    "# Use the fast training function to avoid multiprocessing issues\n",
    "single_results_original = util.fast_single_fold_training_with_predefined_split(\n",
    "    dataset=dataset,\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    model_class=models.BirdCNN,\n",
    "    num_classes=len(set(labels_list)),\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    use_class_weights=True,\n",
    "    estop=35,\n",
    "    standardize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual training curves for original optimized function\n",
    "util.plot_single_fold_curve(single_results_original, metric_key='accuracies', title=\"Original Optimized - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results_original, metric_key='losses', title=\"Original Optimized - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results_original, metric_key='f1s', title=\"Original Optimized - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results_original)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results_original['confusion_matrix'], title=\"BirdCNN Original Optimized - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FAST single fold training with best 80-20 split found above\n",
    "# This uses the optimal train/validation split with author grouping\n",
    "train_indices, val_indices = train_indices_single, val_indices_single\n",
    "\n",
    "# Use the optimized fast training function\n",
    "single_results = util.fast_single_fold_training_with_predefined_split(\n",
    "    dataset=dataset,\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    model_class=models.BirdCNN,\n",
    "    num_classes=len(set(labels_list)),\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    use_class_weights=True,\n",
    "    estop=35,\n",
    "    standardize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual training curves for single fold\n",
    "util.plot_single_fold_curve(single_results, metric_key='accuracies', title=\"Single Fold - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='losses', title=\"Single Fold - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='f1s', title=\"Single Fold - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results['confusion_matrix'], title=\"BirdCNN - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single fold training with best 80-20 split found above\n",
    "# This uses the optimal train/validation split with author grouping\n",
    "train_indices, val_indices = train_indices_single, val_indices_single\n",
    "\n",
    "single_results = util.fast_single_fold_training_with_predefined_split(\n",
    "    dataset=dataset,\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    model_class=models.BirdResNet,\n",
    "    num_classes=len(set(labels_list)),\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    use_class_weights=False,\n",
    "    estop=35,\n",
    "    standardize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4fa950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual training curves for single fold\n",
    "util.plot_single_fold_curve(single_results, metric_key='accuracies', title=\"BirdRes - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='losses', title=\"BirdRes - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='f1s', title=\"BirdRes - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results['confusion_matrix'], title=\"BirdResNet - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d924f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single fold training with regular 80-20 stratified split\n",
    "single_results_80_20 = util.single_fold_training(\n",
    "    dataset=dataset,\n",
    "    model_class=models.BirdCNN,\n",
    "    num_classes=31,\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    test_size=0.2,\n",
    "    random_state=435,\n",
    "    use_class_weights=True,\n",
    "    estop=35\n",
    ")\n",
    "\n",
    "# Plot individual training curves for 80-20 split\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='accuracies', title=\"80-20 Split - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='losses', title=\"80-20 Split - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='f1s', title=\"80-20 Split - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results_80_20)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results_80_20['confusion_matrix'], title=\"BirdCNN 80-20 Split - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results_80_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single fold training with regular 80-20 stratified split\n",
    "single_results_80_20 = util.single_fold_training(\n",
    "    dataset=dataset,\n",
    "    model_class=models.BirdResNet,\n",
    "    num_classes=31,\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    test_size=0.2,\n",
    "    random_state=435,\n",
    "    use_class_weights=True,\n",
    "    estop=35\n",
    ")\n",
    "\n",
    "# Plot individual training curves for 80-20 split\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='accuracies', title=\"80-20 Split - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='losses', title=\"80-20 Split - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='f1s', title=\"80-20 Split - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results_80_20)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results_80_20['confusion_matrix'], title=\"BirdCNN 80-20 Split - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results_80_20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
