{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1729b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils import util, models, split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5cda7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.split' from 'c:\\\\Users\\\\manue\\\\Desktop\\\\manum\\\\coding\\\\ChirpID-backend\\\\utils\\\\split.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(util)\n",
    "importlib.reload(models)\n",
    "importlib.reload(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29cf9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spect_matrix_list(spects_source_dir, spects_meta_df):\n",
    "    \"\"\"\n",
    "    Load spectrograms directly as matrices without flattening to CSV.\n",
    "    \n",
    "    Args:\n",
    "        spects_source_dir (str): Directory where the spectrogram images are stored in .png format\n",
    "        spects_meta_df (pd.DataFrame): DataFrame with columns 'filename', 'class_id', and 'author'\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matrices_list, labels_list, authors_list)\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    matrices_list = []\n",
    "    labels_list = []\n",
    "    authors_list = []\n",
    "    \n",
    "    spects_meta_df = spects_meta_df.dropna(subset=['filename', 'class_id', 'author'])\n",
    "\n",
    "    print(f\"Processing {len(spects_meta_df)} spectrograms...\")\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "\n",
    "    for _, row in spects_meta_df.iterrows():\n",
    "        filename = row['filename']\n",
    "        class_id = row['class_id']\n",
    "        author = row['author']\n",
    "\n",
    "        image_path = os.path.join(spects_source_dir, filename)\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"File not found: {image_path}\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        img = Image.open(image_path).convert('L')  # Ensure grayscale\n",
    "        \n",
    "        expected_shape = (313, 224)  # PIL uses (width, height) format\n",
    "        if img.size != expected_shape:\n",
    "            print(f\"Warning: Unexpected image size: {img.size} in file {image_path}. Expected {expected_shape}.\")\n",
    "            # Resize if needed\n",
    "            img = img.resize(expected_shape)\n",
    "\n",
    "        # Convert to numpy array (this gives us height x width, i.e., 313 x 224)\n",
    "        pixels = np.array(img)\n",
    "        \n",
    "        matrices_list.append(pixels)\n",
    "        labels_list.append(class_id)\n",
    "        authors_list.append(author)\n",
    "        processed_count += 1\n",
    "\n",
    "    print(f\"Successfully processed: {processed_count}\")\n",
    "    print(f\"Skipped: {skipped_count}\")\n",
    "\n",
    "    if not matrices_list:\n",
    "        raise ValueError(\"No spectrograms were loaded. Check paths and metadata consistency.\")\n",
    "\n",
    "    return matrices_list, labels_list, authors_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29b5b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spectrograms directly into matrices...\n",
      "Processing 2985 spectrograms...\n",
      "Successfully processed: 2985\n",
      "Skipped: 0\n",
      "Loaded 2985 spectrograms\n",
      "Matrix shape: (224, 313)\n",
      "Unique labels: 33\n",
      "Unique authors: 106\n",
      "Successfully processed: 2985\n",
      "Skipped: 0\n",
      "Loaded 2985 spectrograms\n",
      "Matrix shape: (224, 313)\n",
      "Unique labels: 33\n",
      "Unique authors: 106\n"
     ]
    }
   ],
   "source": [
    "# Load spectrogram data and metadata\n",
    "spect_dir = os.path.join('..', 'database', 'spect')  # Spectrogram PNG directory\n",
    "spects_df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final_spects.csv'))  # Metadata\n",
    "\n",
    "print(\"Loading spectrograms directly into matrices...\")\n",
    "matrices_list, labels_list, authors_list = get_spect_matrix_list(spect_dir, spects_df)\n",
    "\n",
    "print(f\"Loaded {len(matrices_list)} spectrograms\")\n",
    "print(f\"Matrix shape: {matrices_list[0].shape}\")\n",
    "print(f\"Unique labels: {len(set(labels_list))}\")\n",
    "print(f\"Unique authors: {len(set(authors_list))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af9dc12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (2985, 1, 224, 313)\n",
      "labels shape: (2985,)\n",
      "authors shape: (2985,)\n",
      "metadata_df shape: (2985, 4)\n",
      "Unique authors: 106\n",
      "Unique classes: 33\n"
     ]
    }
   ],
   "source": [
    "# Process data for training\n",
    "labels = np.array(labels_list, dtype=np.int64)\n",
    "authors = np.array(authors_list)\n",
    "\n",
    "# Convert matrices to numpy array and normalize\n",
    "features = np.array(matrices_list, dtype=np.float32)\n",
    "# Convert to 0-1 range first, then standardization will be applied per fold\n",
    "features /= 255.0\n",
    "# Reshape to add channel dimension for CNN: (samples, channels, height, width)\n",
    "features = features.reshape(-1, 1, 224, 313)\n",
    "\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "print(\"authors shape:\", authors.shape)\n",
    "\n",
    "# Create metadata DataFrame for splitting (with sample indices)\n",
    "metadata_df = pd.DataFrame({\n",
    "    'sample_idx': range(len(labels)),\n",
    "    'class_id': labels,\n",
    "    'author': authors,\n",
    "    'usable_segments': 1  # Each sample represents 1 segment\n",
    "})\n",
    "\n",
    "print(\"metadata_df shape:\", metadata_df.shape)\n",
    "print(\"Unique authors:\", len(metadata_df['author'].unique()))\n",
    "print(\"Unique classes:\", len(metadata_df['class_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46008b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to pick up any changes\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "importlib.reload(util)\n",
    "importlib.reload(split)\n",
    "\n",
    "# Reload to pick up the StandardizedDataset fix and scheduler changes\n",
    "\n",
    "print(\"Modules reloaded with optimizations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e59a07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 2985 samples\n",
      "Tensor shapes: X=torch.Size([2985, 1, 224, 313]), y=torch.Size([2985])\n"
     ]
    }
   ],
   "source": [
    "# Prepare tensors for PyTorch\n",
    "X_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Tensor shapes: X={X_tensor.shape}, y={y_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48784347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best 80-20 split with author grouping...\n",
      "New best split found! Seed: 4, Score: 1.081\n",
      "New best split found! Seed: 7, Score: 0.437\n",
      "New best split found! Seed: 27, Score: 0.340\n",
      "New best split found! Seed: 954, Score: 0.334\n",
      "New best split found! Seed: 1071, Score: 0.315\n",
      "New best split found! Seed: 954, Score: 0.334\n",
      "New best split found! Seed: 1071, Score: 0.315\n",
      "New best split found! Seed: 1208, Score: 0.275\n",
      "New best split found! Seed: 1208, Score: 0.275\n",
      "\n",
      "Best split found:\n",
      "Seed: 1208\n",
      "Stratification score: 0.275\n",
      "Author overlap: set()\n",
      "Segments in dev set: 2433\n",
      "Segments in test set: 552\n",
      "Dev segment%: 81.51%\n",
      "Test segment%: 18.49%\n",
      "\n",
      "Segment distribution comparison:\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|    |   Target_Test_Segments |   Actual_Test_Segments |   Target_Dev_Segments |   Actual_Dev_Segments |   Total_Segments |\n",
      "+====+========================+========================+=======================+=======================+==================+\n",
      "|  0 |                     20 |                     13 |                    80 |                    87 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  1 |                     19 |                     10 |                    75 |                    84 |               94 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  2 |                     18 |                     13 |                    72 |                    77 |               90 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  3 |                     11 |                      7 |                    45 |                    49 |               56 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  4 |                     14 |                     18 |                    55 |                    51 |               69 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  5 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  6 |                     20 |                      7 |                    80 |                    93 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  7 |                     20 |                     29 |                    80 |                    71 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  8 |                     20 |                     25 |                    80 |                    75 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  9 |                      9 |                      6 |                    34 |                    37 |               43 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 10 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 11 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 12 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 13 |                     20 |                     11 |                    80 |                    89 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 14 |                     13 |                     27 |                    54 |                    40 |               67 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 15 |                     20 |                     22 |                    80 |                    78 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 16 |                     14 |                     13 |                    58 |                    59 |               72 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 17 |                     20 |                     19 |                    80 |                    81 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 18 |                     18 |                     14 |                    71 |                    75 |               89 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 19 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 20 |                     20 |                     23 |                    80 |                    77 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 21 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 22 |                     20 |                     26 |                    80 |                    74 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 23 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 24 |                     15 |                      9 |                    61 |                    67 |               76 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 25 |                     15 |                     22 |                    58 |                    51 |               73 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 26 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 27 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 28 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 29 |                     20 |                     13 |                    78 |                    85 |               98 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 30 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 31 |                     12 |                     14 |                    46 |                    44 |               58 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 32 |                     20 |                     24 |                    80 |                    76 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "Best 80-20 split found with score: 0.275\n",
      "Train samples: 2433, Validation samples: 552\n",
      "\n",
      "Best split found:\n",
      "Seed: 1208\n",
      "Stratification score: 0.275\n",
      "Author overlap: set()\n",
      "Segments in dev set: 2433\n",
      "Segments in test set: 552\n",
      "Dev segment%: 81.51%\n",
      "Test segment%: 18.49%\n",
      "\n",
      "Segment distribution comparison:\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|    |   Target_Test_Segments |   Actual_Test_Segments |   Target_Dev_Segments |   Actual_Dev_Segments |   Total_Segments |\n",
      "+====+========================+========================+=======================+=======================+==================+\n",
      "|  0 |                     20 |                     13 |                    80 |                    87 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  1 |                     19 |                     10 |                    75 |                    84 |               94 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  2 |                     18 |                     13 |                    72 |                    77 |               90 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  3 |                     11 |                      7 |                    45 |                    49 |               56 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  4 |                     14 |                     18 |                    55 |                    51 |               69 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  5 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  6 |                     20 |                      7 |                    80 |                    93 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  7 |                     20 |                     29 |                    80 |                    71 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  8 |                     20 |                     25 |                    80 |                    75 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "|  9 |                      9 |                      6 |                    34 |                    37 |               43 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 10 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 11 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 12 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 13 |                     20 |                     11 |                    80 |                    89 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 14 |                     13 |                     27 |                    54 |                    40 |               67 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 15 |                     20 |                     22 |                    80 |                    78 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 16 |                     14 |                     13 |                    58 |                    59 |               72 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 17 |                     20 |                     19 |                    80 |                    81 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 18 |                     18 |                     14 |                    71 |                    75 |               89 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 19 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 20 |                     20 |                     23 |                    80 |                    77 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 21 |                     20 |                     15 |                    80 |                    85 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 22 |                     20 |                     26 |                    80 |                    74 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 23 |                     20 |                     18 |                    80 |                    82 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 24 |                     15 |                      9 |                    61 |                    67 |               76 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 25 |                     15 |                     22 |                    58 |                    51 |               73 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 26 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 27 |                     20 |                     20 |                    80 |                    80 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 28 |                     20 |                     16 |                    80 |                    84 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 29 |                     20 |                     13 |                    78 |                    85 |               98 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 30 |                     20 |                     17 |                    80 |                    83 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 31 |                     12 |                     14 |                    46 |                    44 |               58 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "| 32 |                     20 |                     24 |                    80 |                    76 |              100 |\n",
      "+----+------------------------+------------------------+-----------------------+-----------------------+------------------+\n",
      "Best 80-20 split found with score: 0.275\n",
      "Train samples: 2433, Validation samples: 552\n"
     ]
    }
   ],
   "source": [
    "# Find the best 80-20 split using author grouping\n",
    "print(\"Finding best 80-20 split with author grouping...\")\n",
    "dev_df, test_df, best_split_score = split.search_best_group_seed(\n",
    "    df=metadata_df,\n",
    "    test_size=0.2,\n",
    "    max_attempts=5_000,\n",
    "    min_test_segments=3\n",
    ")\n",
    "\n",
    "# Extract indices for single fold training\n",
    "train_indices_single = dev_df['sample_idx'].values\n",
    "val_indices_single = test_df['sample_idx'].values\n",
    "\n",
    "print(f\"Best 80-20 split found with score: {best_split_score:.3f}\")\n",
    "print(f\"Train samples: {len(train_indices_single)}, Validation samples: {len(val_indices_single)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "584e9384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimized training function startup time...\n",
      "Test dataset size: Train=100, Val=20\n",
      "\n",
      "Timing data loader creation...\n",
      "DataLoader creation time: 0.00 seconds\n",
      "Testing first batch loading...\n",
      "First batch loading time: 0.01 seconds\n",
      "Batch shape: torch.Size([32, 1, 224, 313])\n",
      "✓ DataLoader working correctly!\n",
      "\n",
      "Total test time: 0.01 seconds\n",
      "Optimization complete - training should start much faster now!\n"
     ]
    }
   ],
   "source": [
    "# Test the optimized training function performance\n",
    "import time\n",
    "\n",
    "print(\"Testing optimized training function startup time...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a small test to measure overhead\n",
    "test_indices = train_indices_single[:100] if len(train_indices_single) > 100 else train_indices_single[:50]\n",
    "test_val_indices = val_indices_single[:20] if len(val_indices_single) > 20 else val_indices_single[:10]\n",
    "\n",
    "print(f\"Test dataset size: Train={len(test_indices)}, Val={len(test_val_indices)}\")\n",
    "\n",
    "# Measure just the data loading and setup overhead\n",
    "print(\"\\nTiming data loader creation...\")\n",
    "loader_start = time.time()\n",
    "\n",
    "# Create standardized subset directly to test\n",
    "if True:  # Test standardization path\n",
    "    sample_size = min(50, len(test_indices))\n",
    "    sample_indices = np.random.choice(test_indices, sample_size, replace=False)\n",
    "    sample_data = torch.stack([dataset[i][0] for i in sample_indices])\n",
    "    train_mean = sample_data.mean()\n",
    "    train_std = sample_data.std()\n",
    "    \n",
    "    class StandardizedSubset(torch.utils.data.Dataset):\n",
    "        def __init__(self, original_dataset, indices, mean, std):\n",
    "            self.dataset = original_dataset\n",
    "            self.indices = list(indices)  # Convert to list for compatibility\n",
    "            self.mean = mean\n",
    "            self.std = std + 1e-8\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            real_idx = self.indices[idx]\n",
    "            x, y = self.dataset[real_idx]\n",
    "            x_standardized = (x - self.mean) / self.std\n",
    "            return x_standardized, y\n",
    "    \n",
    "    test_train_subset = StandardizedSubset(dataset, test_indices, train_mean, train_std)\n",
    "    test_val_subset = StandardizedSubset(dataset, test_val_indices, train_mean, train_std)\n",
    "else:\n",
    "    from torch.utils.data import Subset\n",
    "    test_train_subset = Subset(dataset, test_indices)\n",
    "    test_val_subset = Subset(dataset, test_val_indices)\n",
    "\n",
    "# Test DataLoader creation with single thread to avoid worker crashes\n",
    "test_train_loader = torch.utils.data.DataLoader(\n",
    "    test_train_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use single thread to avoid worker crashes\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "test_val_loader = torch.utils.data.DataLoader(\n",
    "    test_val_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Use single thread to avoid worker crashes\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "loader_time = time.time() - loader_start\n",
    "print(f\"DataLoader creation time: {loader_time:.2f} seconds\")\n",
    "\n",
    "# Test first batch loading\n",
    "print(\"Testing first batch loading...\")\n",
    "batch_start = time.time()\n",
    "try:\n",
    "    test_batch = next(iter(test_train_loader))\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"First batch loading time: {batch_time:.2f} seconds\")\n",
    "    print(f\"Batch shape: {test_batch[0].shape}\")\n",
    "    print(\"✓ DataLoader working correctly!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {e}\")\n",
    "    print(\"Falling back to direct dataset access...\")\n",
    "    batch_start = time.time()\n",
    "    test_sample = test_train_subset[0]\n",
    "    batch_time = time.time() - batch_start\n",
    "    print(f\"Direct dataset access time: {batch_time:.4f} seconds\")\n",
    "    print(f\"Sample shape: {test_sample[0].shape}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTotal test time: {total_time:.2f} seconds\")\n",
    "print(\"Optimization complete - training should start much faster now!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e2281",
   "metadata": {},
   "source": [
    "## Single Fold Training with Predefined Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2749d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying author grouping in predefined splits...\n",
      "==================================================\n",
      "Training set authors: 84 unique authors\n",
      "Validation set authors: 22 unique authors\n",
      "Author overlap between train/val: 0 authors\n",
      "✓ PERFECT: No author overlap - authors are properly grouped!\n",
      "\n",
      "Class distribution:\n",
      "Training set classes: 33 classes\n",
      "Validation set classes: 33 classes\n",
      "All classes present in both sets: True\n",
      "\n",
      "==================================================\n",
      "Both optimized functions use these SAME predefined author-grouped splits!\n"
     ]
    }
   ],
   "source": [
    "# Verify that author grouping is preserved in our splits\n",
    "print(\"Verifying author grouping in predefined splits...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get authors for training and validation sets\n",
    "train_authors = set(metadata_df.loc[metadata_df['sample_idx'].isin(train_indices_single), 'author'])\n",
    "val_authors = set(metadata_df.loc[metadata_df['sample_idx'].isin(val_indices_single), 'author'])\n",
    "\n",
    "# Check for overlap\n",
    "author_overlap = train_authors & val_authors\n",
    "print(f\"Training set authors: {len(train_authors)} unique authors\")\n",
    "print(f\"Validation set authors: {len(val_authors)} unique authors\")\n",
    "print(f\"Author overlap between train/val: {len(author_overlap)} authors\")\n",
    "\n",
    "if len(author_overlap) == 0:\n",
    "    print(\"✓ PERFECT: No author overlap - authors are properly grouped!\")\n",
    "else:\n",
    "    print(f\"⚠️ WARNING: {len(author_overlap)} authors appear in both sets\")\n",
    "    print(f\"Overlapping authors: {author_overlap}\")\n",
    "\n",
    "# Check class distribution\n",
    "train_classes = set(metadata_df.loc[metadata_df['sample_idx'].isin(train_indices_single), 'class_id'])\n",
    "val_classes = set(metadata_df.loc[metadata_df['sample_idx'].isin(val_indices_single), 'class_id'])\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Training set classes: {len(train_classes)} classes\")\n",
    "print(f\"Validation set classes: {len(val_classes)} classes\")\n",
    "print(f\"All classes present in both sets: {train_classes == val_classes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Both optimized functions use these SAME predefined author-grouped splits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efde9b2",
   "metadata": {},
   "source": [
    "Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07947fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run original (now optimized) single fold training with best 80-20 split found above\n",
    "# This uses the optimal train/validation split with author grouping\n",
    "train_indices, val_indices = train_indices_single, val_indices_single\n",
    "\n",
    "print(\"Using OPTIMIZED original function with predefined author-grouped splits...\")\n",
    "print(f\"Train indices: {len(train_indices)} samples\")\n",
    "print(f\"Val indices: {len(val_indices)} samples\")\n",
    "\n",
    "# Use the fast training function to avoid multiprocessing issues\n",
    "single_results_original = util.fast_single_fold_training_with_predefined_split(\n",
    "    dataset=dataset,\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    model_class=models.BirdCNN,\n",
    "    num_classes=len(set(labels_list)),\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    use_class_weights=True,\n",
    "    estop=35,\n",
    "    standardize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92a022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual training curves for original optimized function\n",
    "util.plot_single_fold_curve(single_results_original, metric_key='accuracies', title=\"Original Optimized - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results_original, metric_key='losses', title=\"Original Optimized - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results_original, metric_key='f1s', title=\"Original Optimized - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results_original)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results_original['confusion_matrix'], title=\"BirdCNN Original Optimized - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acd6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FAST single fold training with best 80-20 split found above\n",
    "# This uses the optimal train/validation split with author grouping\n",
    "train_indices, val_indices = train_indices_single, val_indices_single\n",
    "\n",
    "# Use the optimized fast training function\n",
    "single_results = util.fast_single_fold_training_with_predefined_split(\n",
    "    dataset=dataset,\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    model_class=models.BirdCNN,\n",
    "    num_classes=len(set(labels_list)),\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    use_class_weights=True,\n",
    "    estop=35,\n",
    "    standardize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual training curves for single fold\n",
    "util.plot_single_fold_curve(single_results, metric_key='accuracies', title=\"Single Fold - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='losses', title=\"Single Fold - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='f1s', title=\"Single Fold - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results['confusion_matrix'], title=\"BirdCNN - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single fold training with best 80-20 split found above\n",
    "# This uses the optimal train/validation split with author grouping\n",
    "train_indices, val_indices = train_indices_single, val_indices_single\n",
    "\n",
    "single_results = util.fast_single_fold_training_with_predefined_split(\n",
    "    dataset=dataset,\n",
    "    train_indices=train_indices,\n",
    "    val_indices=val_indices,\n",
    "    model_class=models.BirdResNet,\n",
    "    num_classes=len(set(labels_list)),\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    use_class_weights=False,\n",
    "    estop=35,\n",
    "    standardize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4fa950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual training curves for single fold\n",
    "util.plot_single_fold_curve(single_results, metric_key='accuracies', title=\"BirdRes - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='losses', title=\"BirdRes - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results, metric_key='f1s', title=\"BirdRes - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results['confusion_matrix'], title=\"BirdResNet - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d924f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single fold training with regular 80-20 stratified split\n",
    "single_results_80_20 = util.single_fold_training(\n",
    "    dataset=dataset,\n",
    "    model_class=models.BirdCNN,\n",
    "    num_classes=31,\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    test_size=0.2,\n",
    "    random_state=435,\n",
    "    use_class_weights=True,\n",
    "    estop=35\n",
    ")\n",
    "\n",
    "# Plot individual training curves for 80-20 split\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='accuracies', title=\"80-20 Split - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='losses', title=\"80-20 Split - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='f1s', title=\"80-20 Split - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results_80_20)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results_80_20['confusion_matrix'], title=\"BirdCNN 80-20 Split - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results_80_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single fold training with regular 80-20 stratified split\n",
    "single_results_80_20 = util.single_fold_training(\n",
    "    dataset=dataset,\n",
    "    model_class=models.BirdResNet,\n",
    "    num_classes=31,\n",
    "    num_epochs=250,\n",
    "    batch_size=48,\n",
    "    lr=0.001,\n",
    "    test_size=0.2,\n",
    "    random_state=435,\n",
    "    use_class_weights=True,\n",
    "    estop=35\n",
    ")\n",
    "\n",
    "# Plot individual training curves for 80-20 split\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='accuracies', title=\"80-20 Split - Accuracy Curves\", ylabel=\"Accuracy\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='losses', title=\"80-20 Split - Loss Curves\", ylabel=\"Cross Entropy Loss\")\n",
    "util.plot_single_fold_curve(single_results_80_20, metric_key='f1s', title=\"80-20 Split - F1 Score Curves\", ylabel=\"Macro F1 Score\")\n",
    "\n",
    "# Print results summary\n",
    "util.print_single_fold_results(single_results_80_20)\n",
    "\n",
    "# Display confusion matrix\n",
    "util.plot_confusion_matrix(single_results_80_20['confusion_matrix'], title=\"BirdCNN 80-20 Split - Validation Confusion Matrix\")\n",
    "util.print_confusion_matrix_stats(single_results_80_20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
