{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "947b5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils import NnClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this one if on laptop\n",
    "# df = pd.read_csv(os.path.join('..', 'database', 'meta', 'mini_train_data.csv'))\n",
    "\n",
    "# use this one if on pc\n",
    "df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final', 'train_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c239a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (653, 70113)\n",
      "labels shape: (653, 10)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels and pixel values\n",
    "labels = df['label'].values.astype(np.int64)\n",
    "features = df.drop(columns=['label']).values.astype(np.float32)\n",
    "features /= 255.0  # Normalize only once\n",
    "\n",
    "labels = pd.get_dummies(labels, columns=['Label'], dtype=int).values   #onehot encoding\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cdb76",
   "metadata": {},
   "source": [
    "## Original Train-Validation Split (Commented Out)\n",
    "\n",
    "The simple random split method - commented out in favor of advanced author grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original simple train-validation split (commented out)\n",
    "# trset,vset=NnClass.tv_split(df,0.8,semilla=99)\n",
    "\n",
    "# trFeatures = trset.drop(columns=['label']).values.astype(np.float32)\n",
    "# trFeatures /= 255.0  # This was redundant - features already normalized above\n",
    "# trLabels=trset[\"label\"].values.astype(np.int64)\n",
    "# trLabels = pd.get_dummies(trLabels, columns=['Label'], dtype=int).values   #onehot encoding\n",
    "\n",
    "# vFeatures = vset.drop(columns=['label']).values.astype(np.float32)\n",
    "# vFeatures /= 255.0  # This was redundant - features already normalized above\n",
    "# vLabels=vset[\"label\"].values.astype(np.int64)\n",
    "# vLabels = pd.get_dummies(vLabels, columns=['Label'], dtype=int).values   #onehot encoding\n",
    "\n",
    "# fcnn=NnClass.Nn(trainX=trFeatures, trainY=trLabels,m=[512, 128, 32],seed=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a15e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Train-Validation Split with Author Grouping\n",
    "import importlib\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils import split\n",
    "\n",
    "# Load the spectrogram metadata to get author information\n",
    "spects_df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final_spects.csv'))\n",
    "\n",
    "print(f\"Loaded spectrogram metadata: {len(spects_df)} entries\")\n",
    "print(f\"CSV training data: {len(df)} entries\")\n",
    "\n",
    "# Create a mapping from CSV to spectrogram metadata for author information\n",
    "# The CSV rows correspond to spectrograms in order, so we can map by index\n",
    "if len(df) <= len(spects_df):\n",
    "    # Map the first len(df) spectrogram entries to CSV entries\n",
    "    metadata_subset = spects_df.iloc[:len(df)].copy()\n",
    "    metadata_subset['sample_idx'] = range(len(df))\n",
    "    metadata_subset['usable_segments'] = 1  # Each row represents 1 segment\n",
    "    \n",
    "    print(f\"Created metadata mapping for {len(metadata_subset)} sWamples\")\n",
    "    print(f\"Unique authors: {metadata_subset['author'].nunique()}\")\n",
    "    print(f\"Unique classes: {metadata_subset['class_id'].nunique()}\")\n",
    "    \n",
    "    # Find the best 80-20 split using author grouping\n",
    "    print(\"Finding best 80-20 split with author grouping...\")\n",
    "    dev_df, test_df, best_split_score = split.search_best_group_seed(\n",
    "        df=metadata_subset,\n",
    "        test_size=0.2,\n",
    "        max_attempts=2000,\n",
    "        min_test_segments=2\n",
    "    )\n",
    "    \n",
    "    # Extract indices for the split\n",
    "    train_indices = dev_df['sample_idx'].values\n",
    "    val_indices = test_df['sample_idx'].values\n",
    "    \n",
    "    print(f\"Best 80-20 split found with score: {best_split_score:.3f}\")\n",
    "    print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "    \n",
    "    # Create training and validation sets using the grouped split\n",
    "    trFeatures = features[train_indices]  # Already normalized\n",
    "    trLabels = labels[train_indices]      # Already one-hot encoded\n",
    "    \n",
    "    vFeatures = features[val_indices]     # Already normalized\n",
    "    vLabels = labels[val_indices]         # Already one-hot encoded\n",
    "    \n",
    "    print(f\"Training features shape: {trFeatures.shape}\")\n",
    "    print(f\"Training labels shape: {trLabels.shape}\")\n",
    "    print(f\"Validation features shape: {vFeatures.shape}\")\n",
    "    print(f\"Validation labels shape: {vLabels.shape}\")\n",
    "    \n",
    "    # Initialize FCNN with grouped split data\n",
    "    fcnn = NnClass.Nn(trainX=trFeatures, trainY=trLabels, m=[512, 128, 32], seed=99)\n",
    "    \n",
    "else:\n",
    "    print(f\"Warning: CSV has more entries ({len(df)}) than spectrogram metadata ({len(spects_df)})\")\n",
    "    print(\"Falling back to simple random split...\")\n",
    "    \n",
    "    # Fallback to simple split if metadata mapping fails\n",
    "    trset, vset = NnClass.tv_split(df, 0.8, semilla=99)\n",
    "    \n",
    "    trFeatures = trset.drop(columns=['label']).values.astype(np.float32)\n",
    "    trFeatures /= 255.0  # Normalize since we're working with raw CSV\n",
    "    trLabels = trset[\"label\"].values.astype(np.int64)\n",
    "    trLabels = pd.get_dummies(trLabels, columns=['Label'], dtype=int).values\n",
    "    \n",
    "    vFeatures = vset.drop(columns=['label']).values.astype(np.float32)\n",
    "    vFeatures /= 255.0  # Normalize since we're working with raw CSV\n",
    "    vLabels = vset[\"label\"].values.astype(np.int64)\n",
    "    vLabels = pd.get_dummies(vLabels, columns=['Label'], dtype=int).values\n",
    "    \n",
    "    fcnn = NnClass.Nn(trainX=trFeatures, trainY=trLabels, m=[512, 128, 32], seed=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599e557",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn.trainUltimate(\n",
    "    epochs=100,\n",
    "    lr=0.001,\n",
    "    batch_size=32,\n",
    "    optimizer='adam',\n",
    "    l2_lambda=1e-4,\n",
    "    valX=vFeatures,\n",
    "    valy=vLabels,\n",
    "    early_stopping=True,\n",
    "    patience=10,\n",
    "    eval_interval=5,\n",
    "    lr_schedule={'type': 'exponential', 'decay': 0.95}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b66c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results with accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the loss history from training\n",
    "train_losses = fcnn.loss_history if hasattr(fcnn, 'loss_history') else []\n",
    "val_losses = fcnn.val_loss_history if hasattr(fcnn, 'val_loss_history') else []\n",
    "\n",
    "if train_losses and val_losses:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "    plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Calculate and plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_predictions = fcnn.ff(fcnn.X)[1][-1]  # Get final layer output\n",
    "    train_pred_classes = np.argmax(train_predictions, axis=1)\n",
    "    train_true_classes = np.argmax(fcnn.y, axis=1)\n",
    "    train_accuracy = np.mean(train_pred_classes == train_true_classes)\n",
    "    \n",
    "    # Calculate validation accuracy\n",
    "    val_predictions = fcnn.ff(vFeatures)[1][-1]  # Get final layer output\n",
    "    val_pred_classes = np.argmax(val_predictions, axis=1)\n",
    "    val_true_classes = np.argmax(vLabels, axis=1)\n",
    "    val_accuracy = np.mean(val_pred_classes == val_true_classes)\n",
    "    \n",
    "    print(f\"Final Training Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # For plotting accuracy over time, we'd need to store it during training\n",
    "    # For now, plot final accuracies as bars\n",
    "    accuracies = [train_accuracy, val_accuracy]\n",
    "    labels_acc = ['Training', 'Validation']\n",
    "    \n",
    "    plt.bar(labels_acc, accuracies, alpha=0.7, color=['blue', 'red'])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Final Training and Validation Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No loss history available. Make sure training completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "NnClass.plot_confusion_matrix(vLabels, val_predictions, num_classes=vLabels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed results summary\n",
    "print(\"=\"*50)\n",
    "print(\"FCNN TRAINING RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Training samples: {len(trFeatures)}\")\n",
    "print(f\"Validation samples: {len(vFeatures)}\")\n",
    "print(f\"Features per sample: {trFeatures.shape[1]}\")\n",
    "print(f\"Number of classes: {trLabels.shape[1]}\")\n",
    "\n",
    "print(f\"\\nModel Architecture:\")\n",
    "print(f\"Hidden layers: [512, 128, 32]\")\n",
    "print(f\"Total parameters: {sum([np.prod(w.shape) for w in fcnn.W[1:] if w is not None])}\")\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")\n",
    "\n",
    "if hasattr(fcnn, 'loss_history') and fcnn.loss_history:\n",
    "    final_train_loss = fcnn.loss_history[-1]\n",
    "    final_val_loss = fcnn.val_loss_history[-1] if fcnn.val_loss_history else \"N/A\"\n",
    "    print(f\"Final Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_val_loss}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
