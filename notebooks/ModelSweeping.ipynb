{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec5e5b4",
   "metadata": {},
   "source": [
    "Grid Search and Sweeps Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09165fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from utils.models import BirdCNN, BirdResNet\n",
    "from utils.dataset_utils import StandardizedDataset\n",
    "from utils.training_utils import train_single_fold\n",
    "from utils.cross_validation import k_fold_cross_validation_with_predefined_folds\n",
    "from utils.evaluation_utils import plot_kfold_results, save_model, load_model\n",
    "import utils.split as split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ecc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for grid searching\n",
    "df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final', 'train_data.csv'))\n",
    "\n",
    "# Extract labels, authors, and pixel values\n",
    "labels = df['label'].values.astype(np.int64)\n",
    "authors = df['author'].values\n",
    "features = df.drop(columns=['label', 'author']).values.astype(np.float32)\n",
    "# Convert to 0-1 range first, then standardization will be applied per fold\n",
    "features /= 255.0\n",
    "features = features.reshape(-1, 1, 313, 224)\n",
    "\n",
    "print(\"features shape:\", features.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "print(\"authors shape:\", authors.shape)\n",
    "\n",
    "# Create metadata DataFrame for splitting (with sample indices)\n",
    "metadata_df = pd.DataFrame({\n",
    "    'sample_idx': range(len(df)),\n",
    "    'class_id': labels,\n",
    "    'author': authors,\n",
    "    'usable_segments': 1  # Each sample represents 1 segment\n",
    "})\n",
    "\n",
    "print(\"metadata_df shape:\", metadata_df.shape)\n",
    "print(\"Unique authors:\", len(metadata_df['author'].unique()))\n",
    "print(\"Unique classes:\", len(metadata_df['class_id'].unique()))\n",
    "\n",
    "# Prepare tensors\n",
    "X_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "dataset = TensorDataset(X_tensor, y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search parameters\n",
    "grid_params = {\n",
    "    'batch_size': [16, 24, 32, 48],\n",
    "    'lr': [0.0005, 0.001, 0.002],\n",
    "    'num_epochs': [150, 200, 250],\n",
    "    'use_class_weights': [True, False],\n",
    "    'dropout_p': [0.3, 0.5, 0.7]\n",
    "}\n",
    "\n",
    "print(\"Grid search parameter space:\")\n",
    "for param, values in grid_params.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "    \n",
    "total_combinations = np.prod([len(values) for values in grid_params.values()])\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "\n",
    "# For demonstration, let's run a smaller subset\n",
    "limited_params = {\n",
    "    'batch_size': [24, 48],\n",
    "    'lr': [0.001, 0.002], \n",
    "    'num_epochs': [150, 200],\n",
    "    'use_class_weights': [True, False]\n",
    "}\n",
    "\n",
    "print(f\"\\nLimited parameter space for demo:\")\n",
    "for param, values in limited_params.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "    \n",
    "limited_combinations = np.prod([len(values) for values in limited_params.values()])\n",
    "print(f\"Limited combinations: {limited_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8009ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal train/test split for grid search\n",
    "print(\"Finding best 80-20 split with author grouping for grid search...\")\n",
    "dev_df, test_df, best_split_score = split.search_best_group_seed(\n",
    "    df=metadata_df,\n",
    "    test_size=0.2,\n",
    "    max_attempts=5_000,\n",
    "    min_test_segments=5\n",
    ")\n",
    "\n",
    "# Extract indices for grid search\n",
    "train_indices = dev_df['sample_idx'].values\n",
    "val_indices = test_df['sample_idx'].values\n",
    "\n",
    "print(f\"Best split found with score: {best_split_score:.3f}\")\n",
    "print(f\"Train samples: {len(train_indices)}, Validation samples: {len(val_indices)}\")\n",
    "\n",
    "# Initialize results tracking\n",
    "grid_results = []\n",
    "best_score = 0.0\n",
    "best_params = None\n",
    "\n",
    "print(f\"\\nStarting grid search with {limited_combinations} combinations...\")\n",
    "print(\"This may take a while...\")\n",
    "\n",
    "# Execute grid search\n",
    "combination_count = 0\n",
    "for params in product(*limited_params.values()):\n",
    "    combination_count += 1\n",
    "    param_dict = dict(zip(limited_params.keys(), params))\n",
    "    \n",
    "    print(f\"\\nCombination {combination_count}/{limited_combinations}: {param_dict}\")\n",
    "    \n",
    "    try:\n",
    "        # Run training with current parameters\n",
    "        result = train_single_fold(\n",
    "            dataset=dataset,\n",
    "            train_indices=train_indices,\n",
    "            val_indices=val_indices,\n",
    "            model_class=BirdCNN,\n",
    "            num_classes=len(set(labels)),\n",
    "            num_epochs=param_dict['num_epochs'],\n",
    "            batch_size=param_dict['batch_size'],\n",
    "            lr=param_dict['lr'],\n",
    "            use_class_weights=param_dict['use_class_weights'],\n",
    "            estop=35,\n",
    "            standardize=True\n",
    "        )\n",
    "        \n",
    "        # Track results\n",
    "        result_entry = {\n",
    "            **param_dict,\n",
    "            'final_val_acc': result['final_val_acc'],\n",
    "            'final_val_f1': result['final_val_f1'],\n",
    "            'best_val_acc': result['best_val_acc'],\n",
    "            'best_val_f1': result['best_val_f1'],\n",
    "            'training_time': result.get('training_time', 0)\n",
    "        }\n",
    "        \n",
    "        grid_results.append(result_entry)\n",
    "        \n",
    "        # Check if this is the best result so far\n",
    "        if result['final_val_f1'] > best_score:\n",
    "            best_score = result['final_val_f1']\n",
    "            best_params = param_dict.copy()\n",
    "            print(f\"  ✓ New best F1 score: {best_score:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Final F1 score: {result['final_val_f1']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error with parameters {param_dict}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nGrid search completed!\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best F1 score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67563b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "results_df = pd.DataFrame(grid_results)\n",
    "print(\"Grid Search Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(results_df.describe())\n",
    "\n",
    "# Sort by F1 score\n",
    "results_df_sorted = results_df.sort_values('final_val_f1', ascending=False)\n",
    "print(f\"\\nTop 5 Results (by Final Val F1):\")\n",
    "print(results_df_sorted.head().to_string(index=False))\n",
    "\n",
    "# Parameter impact analysis\n",
    "print(f\"\\nParameter Impact Analysis:\")\n",
    "for param in limited_params.keys():\n",
    "    if param in results_df.columns:\n",
    "        param_impact = results_df.groupby(param)['final_val_f1'].agg(['mean', 'std', 'count'])\n",
    "        print(f\"\\n{param}:\")\n",
    "        print(param_impact)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Grid Search Results Analysis', fontsize=16)\n",
    "\n",
    "# Plot 1: F1 scores distribution\n",
    "axes[0, 0].hist(results_df['final_val_f1'], bins=10, alpha=0.7)\n",
    "axes[0, 0].set_title('Distribution of Final Val F1 Scores')\n",
    "axes[0, 0].set_xlabel('F1 Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot 2: Learning rate vs F1\n",
    "if 'lr' in results_df.columns:\n",
    "    lr_grouped = results_df.groupby('lr')['final_val_f1'].mean()\n",
    "    axes[0, 1].bar(range(len(lr_grouped)), lr_grouped.values)\n",
    "    axes[0, 1].set_title('Learning Rate vs Average F1 Score')\n",
    "    axes[0, 1].set_xlabel('Learning Rate')\n",
    "    axes[0, 1].set_ylabel('Average F1 Score')\n",
    "    axes[0, 1].set_xticks(range(len(lr_grouped)))\n",
    "    axes[0, 1].set_xticklabels([f'{lr:.4f}' for lr in lr_grouped.index])\n",
    "\n",
    "# Plot 3: Batch size vs F1\n",
    "if 'batch_size' in results_df.columns:\n",
    "    batch_grouped = results_df.groupby('batch_size')['final_val_f1'].mean()\n",
    "    axes[1, 0].bar(batch_grouped.index, batch_grouped.values)\n",
    "    axes[1, 0].set_title('Batch Size vs Average F1 Score')\n",
    "    axes[1, 0].set_xlabel('Batch Size')\n",
    "    axes[1, 0].set_ylabel('Average F1 Score')\n",
    "\n",
    "# Plot 4: Training time vs F1\n",
    "if 'training_time' in results_df.columns:\n",
    "    axes[1, 1].scatter(results_df['training_time'], results_df['final_val_f1'], alpha=0.7)\n",
    "    axes[1, 1].set_title('Training Time vs F1 Score')\n",
    "    axes[1, 1].set_xlabel('Training Time (seconds)')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../database/meta/grid_search_results.csv', index=False)\n",
    "print(f\"\\nResults saved to ../database/meta/grid_search_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
