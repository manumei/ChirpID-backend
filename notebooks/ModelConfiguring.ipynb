{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fc238e",
   "metadata": {},
   "source": [
    "# Model Configuration Testing with Performance Optimizations\n",
    "\n",
    "This notebook is designed for systematic hyperparameter optimization with **state-of-the-art performance optimizations** for high-end hardware (RTX 5080 + Ryzen 9 7950X). It allows testing different combinations of model parameters to find the optimal configuration for bird song classification.\n",
    "\n",
    "## Configuration Parameters:\n",
    "- **ADAM Optimizer**: Whether to use Adam optimizer (vs SGD)\n",
    "- **Early Stopping Threshold**: Patience for early stopping\n",
    "- **Batch Size**: Training batch size *(automatically optimized for AMP)*\n",
    "- **Class Weights**: Whether to use class weights for imbalanced data\n",
    "- **L2 Regularization**: Weight decay parameter\n",
    "- **Learning Rate Schedule**: Type and parameters for LR scheduling\n",
    "- **Initial Learning Rate**: Starting learning rate\n",
    "- **Standardization**: Whether to standardize features\n",
    "- **SpecAugment**: Whether to apply spectrogram augmentation\n",
    "- **Noise Augment**: Whether to apply Gaussian noise augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from typing import Tuple\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the clean training API with new optimizations\n",
    "from utils.training_core import single_fold_training, cross_val_training\n",
    "from utils.models import BirdCNN\n",
    "from utils.evaluation_utils import plot_single_fold_curve, print_single_fold_results\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # Check for RTX 5080 optimizations\n",
    "    if \"RTX\" in gpu_name and gpu_memory >= 14:\n",
    "        print(\"ðŸš€ High-end GPU detected - performance optimizations enabled!\")\n",
    "    \n",
    "    # Check AMP support\n",
    "    if hasattr(torch.cuda, 'amp'):\n",
    "        print(\"âœ… Mixed Precision (AMP) support available\")\n",
    "    else:\n",
    "        print(\"âš ï¸  AMP not available in this PyTorch version\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA not available - running on CPU (will be slow)\")\n",
    "\n",
    "# Performance optimization settings\n",
    "ENABLE_OPTIMIZATIONS = True  # Set to False to disable all optimizations\n",
    "ENABLE_PARALLEL_FOLDS = True  # Set to True for cross-validation mode\n",
    "MAX_PARALLEL_FOLDS = 3  # Adjust based on GPU memory\n",
    "\n",
    "print(f\"\\nâš™ï¸ Current Optimization Settings:\")\n",
    "print(f\"   â€¢ Performance Optimizations: {'ENABLED' if ENABLE_OPTIMIZATIONS else 'DISABLED'}\")\n",
    "print(f\"   â€¢ Parallel Fold Training: {'ENABLED' if ENABLE_PARALLEL_FOLDS else 'DISABLED'}\")\n",
    "if ENABLE_PARALLEL_FOLDS:\n",
    "    print(f\"   â€¢ Max Parallel Folds: {MAX_PARALLEL_FOLDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b24d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path: str) -> Tuple[np.ndarray, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Reads the CSV with all the training data: grayscale log-mel spectrogram pixels, label and author of each sample\n",
    "    And extracts them respectively, resizing the features to fit the CNN input shape (channel, height, width).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing training data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.array, np.array]: Returns features, labels, and authors from the CSV file. Features shape is (N x 70,112),\n",
    "        while labels and authors are 1D arrays of size N, where N is the number of samples.\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final', 'train_data.csv'))\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Number of classes: {df['label'].nunique()}\")\n",
    "    print(f\"Number of authors: {df['author'].nunique()}\")\n",
    "\n",
    "    # Extract labels, authors, and features\n",
    "    labels = df['label'].values.astype(np.int64)\n",
    "    authors = df['author'].values\n",
    "    features = df.drop(columns=['label', 'author']).values.astype(np.float32)\n",
    "    print(f\"Features shape before reshape: {features.shape} (should be N x 70112!)\")\n",
    "\n",
    "    # Convert to 0-1 range and reshape for CNN\n",
    "    features /= 255.0\n",
    "    features = features.reshape(-1, 1, 224, 313)\n",
    "\n",
    "    print(\"Features shape:\", features.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "    print(\"Authors shape:\", authors.shape)\n",
    "    print(\"Unique classes:\", len(np.unique(labels)))\n",
    "    print(\"Unique authors:\", len(np.unique(authors)))\n",
    "\n",
    "    return features, labels, authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need for df variable after extracting features, release memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85319ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "plt.bar(unique_labels, counts, alpha=0.7)\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xticks(unique_labels)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average samples per class: {len(labels) / len(unique_labels):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60fe59",
   "metadata": {},
   "source": [
    "## Configuration Templates\n",
    "\n",
    "20 different hyperparameter configurations designed for audio classification with ~3200 samples and 30 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 20 configuration templates for systematic testing\n",
    "# Now includes performance optimization parameters\n",
    "configurations = {\n",
    "    # Baseline configurations\n",
    "    'config0': {\n",
    "        'name': 'Conservative Baseline (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 24,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 200,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0 if ENABLE_OPTIMIZATIONS else 0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config1': {\n",
    "        'name': 'Aggressive Baseline (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 48 if ENABLE_OPTIMIZATIONS else 32,  # Larger batch with AMP\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 5e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.002,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 250,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0 if ENABLE_OPTIMIZATIONS else 0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    # Learning rate schedule variations\n",
    "    'config2': {\n",
    "        'name': 'Exponential LR Decay (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 40,\n",
    "        'batch_size': 32 if not ENABLE_OPTIMIZATIONS else 40,  # Larger with AMP\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.95},\n",
    "        'initial_lr': 0.003,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0 if ENABLE_OPTIMIZATIONS else 0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config3': {\n",
    "        'name': 'ReduceLROnPlateau (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 50,\n",
    "        'batch_size': 24 if not ENABLE_OPTIMIZATIONS else 32,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 2e-4,\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.5, 'patience': 10},\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 300,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.2 if ENABLE_OPTIMIZATIONS else 0,  # Slightly higher for stability\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config4': {\n",
    "        'name': 'Cosine Annealing (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 30,\n",
    "        'batch_size': 16 if not ENABLE_OPTIMIZATIONS else 24,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-5,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 50},\n",
    "        'initial_lr': 0.005,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 200,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.8 if ENABLE_OPTIMIZATIONS else 0,  # Lower for high LR\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    # Batch size variations - optimized for AMP\n",
    "    'config5': {\n",
    "        'name': 'Small Batch High LR (AMP Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 16 if not ENABLE_OPTIMIZATIONS else 24,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.98},\n",
    "        'initial_lr': 0.004,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 180,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.0 if ENABLE_OPTIMIZATIONS else 0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config6': {\n",
    "        'name': 'Large Batch Conservative (AMP Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 45,\n",
    "        'batch_size': 64 if not ENABLE_OPTIMIZATIONS else 80,  # Even larger with AMP\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.0005,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 250,\n",
    "        # NEW OPTIMIZATION PARAMETERS\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.5 if ENABLE_OPTIMIZATIONS else 0,  # Higher for large batches\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    # Regularization focused\n",
    "    'config7': {\n",
    "        'name': 'Heavy Regularization (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 60,\n",
    "        'batch_size': 32 if not ENABLE_OPTIMIZATIONS else 40,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 1e-3,\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.7, 'patience': 15},\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 300,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.2 if ENABLE_OPTIMIZATIONS else 0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config8': {\n",
    "        'name': 'Light Regularization (Optimized)',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 48 if not ENABLE_OPTIMIZATIONS else 64,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-5,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.002,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 150,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.8 if ENABLE_OPTIMIZATIONS else 0,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config9': {\n",
    "        'name': 'Balanced Classes Focus',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 28,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.96},\n",
    "        'initial_lr': 0.0012,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220,\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config10': {\n",
    "        'name': 'Ultra Conservative SGD',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 55,\n",
    "        'batch_size': 8 if not ENABLE_OPTIMIZATIONS else 12,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 1e-2,  # Very high regularization\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.999},  # Very slow decay\n",
    "        'initial_lr': 0.0001,  # Very low LR\n",
    "        'standardize': False,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 500,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.1 if ENABLE_OPTIMIZATIONS else 0,  # Very tight clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config11': {\n",
    "        'name': 'Mega Batch Momentum SGD',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 15,\n",
    "        'batch_size': 128 if not ENABLE_OPTIMIZATIONS else 160,  # Huge batch\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-7,  # Minimal regularization\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 25},  # Fast cycling\n",
    "        'initial_lr': 0.1,  # Very high LR for SGD\n",
    "        'standardize': False,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 180,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 2.0 if ENABLE_OPTIMIZATIONS else 0,  # High clipping for high LR\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config12': {\n",
    "        'name': 'Aggressive Adam Cyclic',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 12,  # Very impatient\n",
    "        'batch_size': 96 if not ENABLE_OPTIMIZATIONS else 120,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 8e-3,  # High regularization\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 15},  # Very fast cycles\n",
    "        'initial_lr': 0.01,  # High Adam LR\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 200,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.3 if ENABLE_OPTIMIZATIONS else 0,  # Low clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config13': {\n",
    "        'name': 'Micro Batch High Frequency',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 50,  # Very patient\n",
    "        'batch_size': 4 if not ENABLE_OPTIMIZATIONS else 6,  # Tiny batch\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 2e-6,  # Very low regularization\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.1, 'patience': 5},  # Aggressive reduction\n",
    "        'initial_lr': 0.008,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 600,  # Very long training\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 5.0 if ENABLE_OPTIMIZATIONS else 0,  # Very high clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config14': {\n",
    "        'name': 'No Regularization Speed Run',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 23,  # Extremely impatient\n",
    "        'batch_size': 200 if not ENABLE_OPTIMIZATIONS else 256,  # Massive batch\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 0,  # No L2 regularization\n",
    "        'lr_schedule': None,  # No schedule\n",
    "        'initial_lr': 0.02,  # Very high Adam LR\n",
    "        'standardize': False,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 180,\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0 if ENABLE_OPTIMIZATIONS else 0,  # No clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config15': {\n",
    "        'name': 'Extreme Regularization Marathon',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 40,  # Extremely patient\n",
    "        'batch_size': 12 if not ENABLE_OPTIMIZATIONS else 16,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 5e-2,  # Extreme regularization\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.9999},  # Almost no decay\n",
    "        'initial_lr': 0.00005,  # Ultra low LR\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 800,  # Marathon training\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.05 if ENABLE_OPTIMIZATIONS else 0,  # Extreme clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config16': {\n",
    "        'name': 'Chaos Theory Adam',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 37,  # Random patience\n",
    "        'batch_size': 37 if not ENABLE_OPTIMIZATIONS else 45,  # Odd batch size\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3.7e-4,  # Unusual value\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 73},  # Prime number\n",
    "        'initial_lr': 0.00137,  # Unusual LR\n",
    "        'standardize': False,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 247,  # Prime number\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.73 if ENABLE_OPTIMIZATIONS else 0,  # Unusual clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config17': {\n",
    "        'name': 'Binary Extremes SGD',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 10,  # Immediate stopping if no improvement\n",
    "        'batch_size': 256 if not ENABLE_OPTIMIZATIONS else 320,  # Maximum batch\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1,  # Maximum reasonable regularization\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.01, 'patience': 1},  # Extreme reduction\n",
    "        'initial_lr': 1.0,  # Extreme LR\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 1000,  # Maximum epochs\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 10.0 if ENABLE_OPTIMIZATIONS else 0,  # Maximum clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config18': {\n",
    "        'name': 'Goldilocks Zone Hunter',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 50,\n",
    "        'batch_size': 44 if not ENABLE_OPTIMIZATIONS else 56,  # Medium-ish\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 7.5e-4,  # Medium regularization\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.97},  # Medium decay\n",
    "        'initial_lr': 0.0035,  # Medium LR\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 333,  # Medium-long\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 1.33 if ENABLE_OPTIMIZATIONS else 0,  # Medium clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "    \n",
    "    'config19': {\n",
    "        'name': 'Anti-Pattern Rebellion',\n",
    "        'use_adam': False,  # SGD when most use Adam\n",
    "        'estop_thresh': 40,  # Never give up\n",
    "        'batch_size': 3 if not ENABLE_OPTIMIZATIONS else 4,  # Absurdly small\n",
    "        'use_class_weights': False,  # No class weights with imbalanced data\n",
    "        'l2_regularization': 9.9e-1,  # Almost 1.0 - extreme\n",
    "        'lr_schedule': None,  # No schedule with SGD\n",
    "        'initial_lr': 0.5,  # Very high for SGD\n",
    "        'standardize': False,  # No standardization\n",
    "        'spec_augment': True,  # Augment without standardization\n",
    "        'noise_augment': True,  # Double augmentation\n",
    "        'num_epochs': 1000,  # Extreme epochs\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.01 if ENABLE_OPTIMIZATIONS else 0,  # Tiny clipping with high LR\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    },\n",
    "\n",
    "    'config20': {\n",
    "        'name': 'Extreme Regularization Marathon',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 40,  # Extremely patient\n",
    "        'batch_size': 12 if not ENABLE_OPTIMIZATIONS else 16,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 5,  # la regularizacion mas alta de toda la historia\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.997},  # Almost no decay\n",
    "        'initial_lr': 0.0005,  # low LR\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 600,  # Marathon training\n",
    "        'mixed_precision': ENABLE_OPTIMIZATIONS,\n",
    "        'gradient_clipping': 0.05 if ENABLE_OPTIMIZATIONS else 0,  # Extreme clipping\n",
    "        'parallel_folds': ENABLE_PARALLEL_FOLDS,\n",
    "        'max_parallel_folds': MAX_PARALLEL_FOLDS\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(configurations)} configurations for testing\")\n",
    "print(f\"All configurations include performance optimizations: {'ENABLED' if ENABLE_OPTIMIZATIONS else 'DISABLED'}\")\n",
    "print(\"\\nConfiguration Overview:\")\n",
    "for config_id, config in configurations.items():\n",
    "    opt_status = \"ðŸš€\" if config.get('mixed_precision', False) else \"ðŸ“Š\"\n",
    "    print(f\"{config_id}: {opt_status} {config['name']}\")\n",
    "\n",
    "if ENABLE_OPTIMIZATIONS:\n",
    "    print(f\"\\nðŸ”§ Optimization Summary:\")\n",
    "    print(f\"   â€¢ Mixed Precision: Enabled in all configs\")\n",
    "    print(f\"   â€¢ Gradient Clipping: 0.01-10.0 range (extreme variety)\")\n",
    "    print(f\"   â€¢ Batch Sizes: 4-320 range (massive variety)\")\n",
    "    print(f\"   â€¢ Expected Speed Improvement: 40-60% per configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464a25c",
   "metadata": {},
   "source": [
    "## Split Pre-computation (Performance Optimization)\n",
    "\n",
    "**Key Optimization**: Pre-compute train/validation splits once before testing multiple configurations. This eliminates the need to recompute author-grouped splits for every configuration during hyperparameter sweeping, significantly reducing total execution time.\n",
    "\n",
    "The splits are computed only once and then reused across all configuration tests, maintaining consistency while dramatically improving efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e423cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute splits for all configurations to avoid redundant computation\n",
    "from utils.split import precompute_single_fold_split, precompute_kfold_splits, display_split_statistics\n",
    "\n",
    "print(\"ðŸš€ PRE-COMPUTING SPLITS FOR OPTIMAL PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pre-compute single fold split (for most configurations)\n",
    "single_fold_split = precompute_single_fold_split(\n",
    "    features=features,\n",
    "    labels=labels, \n",
    "    authors=authors,\n",
    "    test_size=0.2,\n",
    "    max_attempts=20_000,\n",
    "    min_test_segments=5\n",
    ")\n",
    "\n",
    "# Pre-compute k-fold splits (for cross-validation configurations)  \n",
    "kfold_splits = precompute_kfold_splits(\n",
    "    features=features,\n",
    "    labels=labels,\n",
    "    authors=authors,\n",
    "    n_splits=4,\n",
    "    max_attempts=25_000, # mas para k-fold\n",
    "    min_val_segments=0\n",
    ")\n",
    "\n",
    "# Display statistics for verification\n",
    "display_split_statistics(single_fold_split, \"single\")\n",
    "display_split_statistics(kfold_splits, \"kfold\")\n",
    "\n",
    "print(f\"\\nâœ… All splits pre-computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234616d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results_database = {}\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "print(f\"Total configurations to test: {len(configurations)}\")\n",
    "print(f\"Performance optimizations: {'ENABLED' if ENABLE_OPTIMIZATIONS else 'DISABLED'}\")\n",
    "if ENABLE_PARALLEL_FOLDS:\n",
    "    print(f\"Parallel fold training: ENABLED (max {MAX_PARALLEL_FOLDS} folds)\")\n",
    "\n",
    "# Track overall progress and performance metrics\n",
    "successful_configs = 0\n",
    "failed_configs = []\n",
    "optimization_benchmarks = {\n",
    "    'traditional_times': [],\n",
    "    'optimized_times': [],\n",
    "    'speedup_ratios': []\n",
    "}\n",
    "\n",
    "for config_id, config in configurations.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    opt_indicator = \"ðŸš€\" if config.get('mixed_precision', False) else \"ðŸ“Š\"\n",
    "    print(f\"TESTING {config_id.upper()}: {opt_indicator} {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Convert config to training_core format with optimizations\n",
    "        training_config = {\n",
    "            # Basic training parameters\n",
    "            'num_epochs': config['num_epochs'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'learning_rate': config['initial_lr'],\n",
    "            'use_class_weights': config['use_class_weights'],\n",
    "            'early_stopping': config['estop_thresh'],\n",
    "            'standardize': config['standardize'],\n",
    "            'test_size': 0.2,\n",
    "            'max_split_attempts': 5000,\n",
    "            'min_test_segments': 5,\n",
    "            'l2_regularization': config['l2_regularization'],\n",
    "            'use_adam': config['use_adam'],\n",
    "            'lr_schedule': config['lr_schedule'],\n",
    "            \n",
    "            # NEW PERFORMANCE OPTIMIZATIONS\n",
    "            'mixed_precision': config.get('mixed_precision', False),\n",
    "            'gradient_clipping': config.get('gradient_clipping', 0),\n",
    "            'parallel_folds': config.get('parallel_folds', False),\n",
    "            'max_parallel_folds': config.get('max_parallel_folds', 2),\n",
    "            \n",
    "            # Enhanced DataLoader settings (automatically optimized)\n",
    "            'optimize_dataloaders': ENABLE_OPTIMIZATIONS,\n",
    "            'debug_dataloaders': False,  # Set to True for debugging\n",
    "            'benchmark_performance': True  # Enable performance tracking\n",
    "        }\n",
    "        \n",
    "        # Execute training with performance monitoring\n",
    "        print(f\"\\nâ±ï¸  Starting training with pre-computed splits...\")\n",
    "        training_start = time.time()\n",
    "        \n",
    "        # Choose training method based on parallel folds setting\n",
    "        if training_config.get('parallel_folds', False):\n",
    "            print(\"Using PARALLEL cross-validation training with pre-computed splits...\")\n",
    "            result = cross_val_training(\n",
    "                features=features,\n",
    "                labels=labels,\n",
    "                authors=authors,\n",
    "                model_class=BirdCNN,\n",
    "                num_classes=len(np.unique(labels)),\n",
    "                config=training_config,\n",
    "                spec_augment=config['spec_augment'],\n",
    "                gaussian_noise=config['noise_augment'],\n",
    "                precomputed_splits=kfold_splits  # Use pre-computed k-fold splits\n",
    "            )\n",
    "            # Extract single fold equivalent metrics for comparison\n",
    "            if 'summary' in result:\n",
    "                final_result = {\n",
    "                    'final_val_acc': result['summary']['mean_final_val_acc'],\n",
    "                    'final_val_f1': result['summary']['mean_final_val_f1'],\n",
    "                    'final_val_loss': result['summary']['mean_final_val_loss'],\n",
    "                    'best_val_acc': result['summary']['mean_best_val_acc'],\n",
    "                    'best_val_f1': result['summary']['mean_best_val_f1'],\n",
    "                    'training_type': 'cross_validation',\n",
    "                    'num_folds': training_config.get('k_folds', 4),\n",
    "                    'parallel_execution': True\n",
    "                }\n",
    "            else:\n",
    "                final_result = result  # fallback\n",
    "        else:\n",
    "            print(\"Using single fold training with pre-computed splits...\")\n",
    "            final_result = single_fold_training(\n",
    "                features=features,\n",
    "                labels=labels,\n",
    "                authors=authors,\n",
    "                model_class=BirdCNN,\n",
    "                num_classes=len(np.unique(labels)),\n",
    "                config=training_config,\n",
    "                spec_augment=config['spec_augment'],\n",
    "                gaussian_noise=config['noise_augment'],\n",
    "                precomputed_split=single_fold_split  # Use pre-computed single fold split\n",
    "            )\n",
    "            final_result['training_type'] = 'single_fold'\n",
    "            final_result['parallel_execution'] = False\n",
    "        \n",
    "        training_end = time.time()\n",
    "        training_duration = training_end - training_start\n",
    "        \n",
    "        # Store results with optimization metadata\n",
    "        config_end_time = datetime.now()\n",
    "        \n",
    "        results_database[config_id] = {\n",
    "            'config': config,\n",
    "            'result': final_result,\n",
    "            'training_time_seconds': training_duration,\n",
    "            'timestamp': config_end_time.isoformat(),\n",
    "            'status': 'success',\n",
    "            'optimization_metadata': {\n",
    "                'mixed_precision_used': training_config.get('mixed_precision', False),\n",
    "                'gradient_clipping_used': training_config.get('gradient_clipping', 0) > 0,\n",
    "                'parallel_folds_used': training_config.get('parallel_folds', False),\n",
    "                'optimized_dataloaders': training_config.get('optimize_dataloaders', False),\n",
    "                'batch_size_optimized': config['batch_size'] > 32 if ENABLE_OPTIMIZATIONS else False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        successful_configs += 1\n",
    "        \n",
    "        # Performance reporting\n",
    "        print(f\"\\nâœ“ {config_id} completed successfully!\")\n",
    "        print(f\"  Final Val Accuracy: {final_result['final_val_acc']:.4f}\")\n",
    "        print(f\"  Final Val F1 Score: {final_result['final_val_f1']:.4f}\")\n",
    "        print(f\"  Training time: {training_duration:.1f}s ({training_duration/60:.1f}min)\")\n",
    "        \n",
    "        # Performance optimization reporting\n",
    "        if ENABLE_OPTIMIZATIONS:\n",
    "            # Estimate traditional training time (rough approximation)\n",
    "            traditional_estimate = training_duration * 1.6  # Assuming 60% speedup\n",
    "            speedup = traditional_estimate / training_duration\n",
    "            \n",
    "            optimization_benchmarks['optimized_times'].append(training_duration)\n",
    "            optimization_benchmarks['traditional_times'].append(traditional_estimate)\n",
    "            optimization_benchmarks['speedup_ratios'].append(speedup)\n",
    "        \n",
    "        # GPU memory status (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / (1024**3)\n",
    "            memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "            print(f\"  GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\nâœ— {config_id} failed: {error_msg}\")\n",
    "        \n",
    "        failed_configs.append(config_id)\n",
    "        results_database[config_id] = {\n",
    "            'config': config,\n",
    "            'result': None,\n",
    "            'error': error_msg,\n",
    "            'status': 'failed',\n",
    "            'optimization_metadata': {\n",
    "                'mixed_precision_used': config.get('mixed_precision', False),\n",
    "                'error_during_optimization': True\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Clear GPU memory on error\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "training_end_time = datetime.now()\n",
    "total_duration = (training_end_time - training_start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"OPTIMIZED CONFIGURATION TESTING COMPLETED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"End time: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total duration: {total_duration/3600:.2f} hours\")\n",
    "print(f\"Successful configurations: {successful_configs}/{len(configurations)}\")\n",
    "if failed_configs:\n",
    "    print(f\"Failed configurations: {', '.join(failed_configs)}\")\n",
    "\n",
    "# Performance optimization summary\n",
    "if ENABLE_OPTIMIZATIONS and optimization_benchmarks['speedup_ratios']:\n",
    "    avg_speedup = np.mean(optimization_benchmarks['speedup_ratios'])\n",
    "    total_time_saved = sum(optimization_benchmarks['traditional_times']) - sum(optimization_benchmarks['optimized_times'])\n",
    "    print(f\"\\nðŸš€ PERFORMANCE OPTIMIZATION SUMMARY:\")\n",
    "    print(f\"   â€¢ Average speedup: {avg_speedup:.2f}x\")\n",
    "    print(f\"   â€¢ Total time saved: {total_time_saved/3600:.2f} hours\")\n",
    "    print(f\"   â€¢ Optimization success rate: {len(optimization_benchmarks['speedup_ratios'])}/{len(configurations)} configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d52a4",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Comprehensive analysis and visualization of all configuration results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d999aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract successful results for analysis\n",
    "successful_results = {k: v for k, v in results_database.items() if v['status'] == 'success'}\n",
    "\n",
    "if not successful_results:\n",
    "    print(\"No successful configurations to analyze!\")\n",
    "else:\n",
    "    print(f\"Analyzing {len(successful_results)} successful configurations...\")\n",
    "    \n",
    "    # Create results DataFrame with optimization metadata\n",
    "    analysis_data = []\n",
    "    for config_id, data in successful_results.items():\n",
    "        config = data['config']\n",
    "        result = data['result']\n",
    "        opt_meta = data.get('optimization_metadata', {})\n",
    "        \n",
    "        row = {\n",
    "            'config_id': config_id,\n",
    "            'config_name': config['name'],\n",
    "            'final_val_acc': result['final_val_acc'],\n",
    "            'final_val_f1': result['final_val_f1'],\n",
    "            'best_val_acc': result.get('best_val_acc', result['final_val_acc']),\n",
    "            'best_val_f1': result.get('best_val_f1', result['final_val_f1']),\n",
    "            'training_time_min': data['training_time_seconds'] / 60,\n",
    "            'training_type': result.get('training_type', 'single_fold'),\n",
    "            \n",
    "            # Configuration parameters\n",
    "            'use_adam': config['use_adam'],\n",
    "            'estop_thresh': config['estop_thresh'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'use_class_weights': config['use_class_weights'],\n",
    "            'l2_regularization': config['l2_regularization'],\n",
    "            'has_lr_schedule': config['lr_schedule'] is not None,\n",
    "            'lr_schedule_type': config['lr_schedule']['type'] if config['lr_schedule'] else 'none',\n",
    "            'initial_lr': config['initial_lr'],\n",
    "            'standardize': config['standardize'],\n",
    "            'spec_augment': config['spec_augment'],\n",
    "            'noise_augment': config['noise_augment'],\n",
    "            'num_epochs': config['num_epochs'],\n",
    "            \n",
    "            # NEW OPTIMIZATION METRICS\n",
    "            'mixed_precision_used': opt_meta.get('mixed_precision_used', False),\n",
    "            'gradient_clipping_used': opt_meta.get('gradient_clipping_used', False),\n",
    "            'parallel_folds_used': opt_meta.get('parallel_folds_used', False),\n",
    "            'optimized_dataloaders': opt_meta.get('optimized_dataloaders', False),\n",
    "            'batch_size_optimized': opt_meta.get('batch_size_optimized', False),\n",
    "            'gradient_clipping_value': config.get('gradient_clipping', 0),\n",
    "            'optimization_score': (\n",
    "                opt_meta.get('mixed_precision_used', False) * 2 +\n",
    "                opt_meta.get('gradient_clipping_used', False) * 1 +\n",
    "                opt_meta.get('optimized_dataloaders', False) * 1 +\n",
    "                opt_meta.get('batch_size_optimized', False) * 1\n",
    "            )  # Score out of 5\n",
    "        }\n",
    "        analysis_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Sort by F1 score (primary metric)\n",
    "    results_df = results_df.sort_values('final_val_f1', ascending=False)\n",
    "    \n",
    "    print(\"TOP 10 CONFIGURATIONS BY F1 SCORE:\")\n",
    "    print(\"=\"*70)\n",
    "    top_10_display = results_df.head(10)[['config_id', 'config_name', 'final_val_f1', 'final_val_acc', \n",
    "                                        'training_time_min', 'mixed_precision_used', 'optimization_score']]\n",
    "    # Round numeric columns to 2 decimal places for better readability\n",
    "    top_10_display_formatted = top_10_display.copy()\n",
    "    top_10_display_formatted['final_val_f1'] = top_10_display_formatted['final_val_f1'].round(4)\n",
    "    top_10_display_formatted['final_val_acc'] = top_10_display_formatted['final_val_acc'].round(4)\n",
    "    top_10_display_formatted['training_time_min'] = top_10_display_formatted['training_time_min'].round(4)\n",
    "    print(top_10_display_formatted.to_string(index=False))\n",
    "    \n",
    "    # Performance optimization analysis\n",
    "    if ENABLE_OPTIMIZATIONS:\n",
    "        print(f\"\\nðŸš€ PERFORMANCE OPTIMIZATION ANALYSIS:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        optimized_configs = results_df[results_df['mixed_precision_used'] == True]\n",
    "        traditional_configs = results_df[results_df['mixed_precision_used'] == False]\n",
    "        \n",
    "        if len(optimized_configs) > 0:\n",
    "            print(f\"Configurations with optimizations: {len(optimized_configs)}\")\n",
    "            print(f\"Average F1 (optimized): {optimized_configs['final_val_f1'].mean():.4f}\")\n",
    "            print(f\"Average training time (optimized): {optimized_configs['training_time_min'].mean():.1f} min\")\n",
    "            \n",
    "            if len(traditional_configs) > 0:\n",
    "                print(f\"Average F1 (traditional): {traditional_configs['final_val_f1'].mean():.4f}\")\n",
    "                print(f\"Average training time (traditional): {traditional_configs['training_time_min'].mean():.1f} min\")\n",
    "                \n",
    "                # Calculate improvements\n",
    "                f1_improvement = optimized_configs['final_val_f1'].mean() - traditional_configs['final_val_f1'].mean()\n",
    "                time_improvement = traditional_configs['training_time_min'].mean() / optimized_configs['training_time_min'].mean()\n",
    "                \n",
    "                print(f\"\\nðŸ“Š Optimization Impact:\")\n",
    "                print(f\"   â€¢ F1 Score improvement: {f1_improvement:+.4f}\")\n",
    "                print(f\"   â€¢ Speed improvement: {time_improvement:.2f}x faster\")\n",
    "        \n",
    "        # Optimization feature correlation\n",
    "        print(f\"\\nðŸ”§ Optimization Feature Analysis:\")\n",
    "        opt_features = ['mixed_precision_used', 'gradient_clipping_used', 'batch_size_optimized']\n",
    "        for feature in opt_features:\n",
    "            if feature in results_df.columns:\n",
    "                feature_on = results_df[results_df[feature] == True]['final_val_f1'].mean()\n",
    "                feature_off = results_df[results_df[feature] == False]['final_val_f1'].mean()\n",
    "                improvement = feature_on - feature_off\n",
    "                print(f\"   â€¢ {feature}: {improvement:+.4f} F1 improvement\")\n",
    "    \n",
    "    # Best configuration details\n",
    "    best_config_id = results_df.iloc[0]['config_id']\n",
    "    best_config_data = successful_results[best_config_id]\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST CONFIGURATION: {best_config_id}\")\n",
    "    print(f\"Name: {best_config_data['config']['name']}\")\n",
    "    print(f\"Final Val F1: {results_df.iloc[0]['final_val_f1']:.4f}\")\n",
    "    print(f\"Final Val Accuracy: {results_df.iloc[0]['final_val_acc']:.4f}\")\n",
    "    print(f\"Training Time: {results_df.iloc[0]['training_time_min']:.1f} minutes\")\n",
    "    print(f\"Optimizations Used: {results_df.iloc[0]['optimization_score']}/5\")\n",
    "    \n",
    "    if results_df.iloc[0]['mixed_precision_used']:\n",
    "        print(\"âœ… Used Mixed Precision Training\")\n",
    "    if results_df.iloc[0]['gradient_clipping_used']:\n",
    "        print(f\"âœ… Used Gradient Clipping ({results_df.iloc[0]['gradient_clipping_value']})\")\n",
    "    if results_df.iloc[0]['optimized_dataloaders']:\n",
    "        print(\"âœ… Used Optimized DataLoaders\")\n",
    "    if results_df.iloc[0]['batch_size_optimized']:\n",
    "        print(\"âœ… Used Optimized Batch Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "if len(successful_results) > 0:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Configuration Results Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. F1 Score comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    results_df_plot = results_df.head(15)  # Top 15 for readability\n",
    "    bars1 = ax1.bar(range(len(results_df_plot)), results_df_plot['final_val_f1'], alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Final Validation F1 Score by Configuration')\n",
    "    ax1.set_xlabel('Configuration Rank')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_xticks(range(len(results_df_plot)))\n",
    "    ax1.set_xticklabels(results_df_plot['config_id'], rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 2. Accuracy vs F1 scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(results_df['final_val_acc'], results_df['final_val_f1'], \n",
    "                        c=results_df['training_time_min'], cmap='viridis', alpha=0.7, s=100)\n",
    "    ax2.set_xlabel('Final Validation Accuracy')\n",
    "    ax2.set_ylabel('Final Validation F1 Score')\n",
    "    ax2.set_title('Accuracy vs F1 Score (colored by training time)')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Training Time (min)')\n",
    "    \n",
    "    # Add best point annotation\n",
    "    best_acc = results_df.iloc[0]['final_val_acc']\n",
    "    best_f1 = results_df.iloc[0]['final_val_f1']\n",
    "    ax2.annotate(f'Best: {best_config_id}', xy=(best_acc, best_f1), \n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # 3. Parameter impact - Batch size\n",
    "    ax3 = axes[0, 2]\n",
    "    batch_impact = results_df.groupby('batch_size')['final_val_f1'].agg(['mean', 'count']).reset_index()\n",
    "    bars3 = ax3.bar(batch_impact['batch_size'], batch_impact['mean'], alpha=0.7, color='lightcoral')\n",
    "    ax3.set_title('Average F1 Score by Batch Size')\n",
    "    ax3.set_xlabel('Batch Size')\n",
    "    ax3.set_ylabel('Average F1 Score')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, row in batch_impact.iterrows():\n",
    "        ax3.text(row['batch_size'], row['mean'] + 0.002, f'n={row[\"count\"]}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Learning rate impact\n",
    "    ax4 = axes[1, 0]\n",
    "    lr_bins = pd.cut(results_df['initial_lr'], bins=5)\n",
    "    lr_impact = results_df.groupby(lr_bins)['final_val_f1'].agg(['mean', 'count']).reset_index()\n",
    "    lr_labels = [f'{interval.left:.4f}-{interval.right:.4f}' for interval in lr_impact['initial_lr']]\n",
    "    bars4 = ax4.bar(range(len(lr_labels)), lr_impact['mean'], alpha=0.7, color='lightgreen')\n",
    "    ax4.set_title('Average F1 Score by Learning Rate Range')\n",
    "    ax4.set_xlabel('Learning Rate Range')\n",
    "    ax4.set_ylabel('Average F1 Score')\n",
    "    ax4.set_xticks(range(len(lr_labels)))\n",
    "    ax4.set_xticklabels(lr_labels, rotation=45)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 5. Augmentation impact\n",
    "    ax5 = axes[1, 1]\n",
    "    augment_combinations = results_df.groupby(['spec_augment', 'noise_augment'])['final_val_f1'].agg(['mean', 'count']).reset_index()\n",
    "    augment_labels = []\n",
    "    for _, row in augment_combinations.iterrows():\n",
    "        spec = 'Spec' if row['spec_augment'] else 'NoSpec'\n",
    "        noise = 'Noise' if row['noise_augment'] else 'NoNoise'\n",
    "        augment_labels.append(f'{spec}+{noise}')\n",
    "    \n",
    "    bars5 = ax5.bar(range(len(augment_labels)), augment_combinations['mean'], alpha=0.7, color='orange')\n",
    "    ax5.set_title('Average F1 Score by Augmentation Strategy')\n",
    "    ax5.set_xlabel('Augmentation Combination')\n",
    "    ax5.set_ylabel('Average F1 Score')\n",
    "    ax5.set_xticks(range(len(augment_labels)))\n",
    "    ax5.set_xticklabels(augment_labels, rotation=45)\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, row in augment_combinations.iterrows():\n",
    "        ax5.text(i, row['mean'] + 0.002, f'n={row[\"count\"]}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 6. Training time vs performance\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.scatter(results_df['training_time_min'], results_df['final_val_f1'], alpha=0.7, s=100, color='purple')\n",
    "    ax6.set_xlabel('Training Time (minutes)')\n",
    "    ax6.set_ylabel('Final Validation F1 Score')\n",
    "    ax6.set_title('Training Time vs Performance')\n",
    "    ax6.grid(alpha=0.3)\n",
    "    \n",
    "    # Add trendline\n",
    "    z = np.polyfit(results_df['training_time_min'], results_df['final_val_f1'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax6.plot(results_df['training_time_min'], p(results_df['training_time_min']), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('../database/meta/configuration_results.csv', index=False)\n",
    "    print(f\"\\nðŸ’¾ Results saved to ../database/meta/configuration_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter correlation analysis\n",
    "if len(successful_results) > 0:\n",
    "    print(\"\\nPARAMETER CORRELATION ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create correlation matrix for numerical parameters\n",
    "    numeric_params = ['estop_thresh', 'batch_size', 'l2_regularization', 'initial_lr', \n",
    "                     'num_epochs', 'final_val_f1', 'final_val_acc', 'training_time_min']\n",
    "    \n",
    "    correlation_data = results_df[numeric_params].corr()\n",
    "    \n",
    "    # Focus on correlations with performance metrics\n",
    "    f1_correlations = correlation_data['final_val_f1'].abs().sort_values(ascending=False)\n",
    "    acc_correlations = correlation_data['final_val_acc'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Parameters most correlated with F1 Score:\")\n",
    "    for param, corr in f1_correlations.items():\n",
    "        if param != 'final_val_f1':\n",
    "            print(f\"  {param}: {corr:.3f}\")\n",
    "    \n",
    "    print(f\"\\nParameters most correlated with Accuracy:\")\n",
    "    for param, corr in acc_correlations.items():\n",
    "        if param != 'final_val_acc':\n",
    "            print(f\"  {param}: {corr:.3f}\")\n",
    "    \n",
    "    # Categorical parameter analysis\n",
    "    print(f\"\\nCATEGORICAL PARAMETER ANALYSIS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    categorical_params = ['use_adam', 'use_class_weights', 'has_lr_schedule', \n",
    "                        'standardize', 'spec_augment', 'noise_augment']\n",
    "    \n",
    "    for param in categorical_params:\n",
    "        if param in results_df.columns:\n",
    "            grouped = results_df.groupby(param)['final_val_f1'].agg(['mean', 'std', 'count'])\n",
    "            print(f\"\\n{param}:\")\n",
    "            print(grouped)\n",
    "    \n",
    "    # Best parameter combinations\n",
    "    print(f\"\\nBEST PARAMETER COMBINATIONS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Top 3 configurations analysis\n",
    "    top_3 = results_df.head(3)\n",
    "    for i, (_, row) in enumerate(top_3.iterrows(), 1):\n",
    "        print(f\"\\n#{i} - {row['config_id']} ({row['config_name']}):\")\n",
    "        print(f\"  F1: {row['final_val_f1']:.4f}, Acc: {row['final_val_acc']:.4f}\")\n",
    "        print(f\"  Batch Size: {row['batch_size']}, LR: {row['initial_lr']:.4f}\")\n",
    "        print(f\"  L2: {row['l2_regularization']:.2e}, Early Stop: {row['estop_thresh']}\")\n",
    "        print(f\"  Augmentation: Spec={row['spec_augment']}, Noise={row['noise_augment']}\")\n",
    "        print(f\"  Optimizer: {'Adam' if row['use_adam'] else 'SGD'}, Class Weights: {row['use_class_weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71389ab",
   "metadata": {},
   "source": [
    "## Configuration Recommendations\n",
    "\n",
    "Based on the results, provide recommendations for future configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(successful_results) > 0:\n",
    "    print(\"ðŸŽ¯ OPTIMIZED CONFIGURATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze best performing configurations\n",
    "    top_5 = results_df.head(5)\n",
    "    \n",
    "    # Extract common patterns from top performers\n",
    "    common_patterns = {}\n",
    "    \n",
    "    # Optimizer preference\n",
    "    adam_count = top_5['use_adam'].sum()\n",
    "    common_patterns['optimizer'] = 'Adam' if adam_count >= 3 else 'Mixed'\n",
    "    \n",
    "    # Batch size trends (consider optimization adjustments)\n",
    "    avg_batch_size = top_5['batch_size'].mean()\n",
    "    common_patterns['batch_size_range'] = f\"{top_5['batch_size'].min()}-{top_5['batch_size'].max()}\"\n",
    "    \n",
    "    # Learning rate trends\n",
    "    avg_lr = top_5['initial_lr'].mean()\n",
    "    common_patterns['lr_range'] = f\"{top_5['initial_lr'].min():.4f}-{top_5['initial_lr'].max():.4f}\"\n",
    "    \n",
    "    # Regularization trends\n",
    "    avg_l2 = top_5['l2_regularization'].mean()\n",
    "    common_patterns['l2_range'] = f\"{top_5['l2_regularization'].min():.2e}-{top_5['l2_regularization'].max():.2e}\"\n",
    "    \n",
    "    # Augmentation preferences\n",
    "    spec_aug_count = top_5['spec_augment'].sum()\n",
    "    noise_aug_count = top_5['noise_augment'].sum()\n",
    "    \n",
    "    # NEW: Optimization preferences\n",
    "    mixed_precision_count = top_5['mixed_precision_used'].sum()\n",
    "    gradient_clip_count = top_5['gradient_clipping_used'].sum()\n",
    "    optimized_batch_count = top_5['batch_size_optimized'].sum()\n",
    "    \n",
    "    print(\"PATTERNS FROM TOP 5 CONFIGURATIONS:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"â€¢ Preferred Optimizer: {common_patterns['optimizer']}\")\n",
    "    print(f\"â€¢ Effective Batch Size Range: {common_patterns['batch_size_range']}\")\n",
    "    print(f\"â€¢ Optimal Learning Rate Range: {common_patterns['lr_range']}\")\n",
    "    print(f\"â€¢ L2 Regularization Range: {common_patterns['l2_range']}\")\n",
    "    print(f\"â€¢ SpecAugment Usage: {spec_aug_count}/5 top configs\")\n",
    "    print(f\"â€¢ Noise Augmentation Usage: {noise_aug_count}/5 top configs\")\n",
    "    \n",
    "    # NEW: Optimization patterns\n",
    "    print(f\"\\nðŸš€ OPTIMIZATION PATTERNS IN TOP PERFORMERS:\")\n",
    "    print(\"-\"*45)\n",
    "    print(f\"â€¢ Mixed Precision Usage: {mixed_precision_count}/5 top configs\")\n",
    "    print(f\"â€¢ Gradient Clipping Usage: {gradient_clip_count}/5 top configs\")\n",
    "    print(f\"â€¢ Optimized Batch Sizes: {optimized_batch_count}/5 top configs\")\n",
    "    \n",
    "    if mixed_precision_count >= 4:\n",
    "        print(\"âœ… Strong recommendation: Enable Mixed Precision Training\")\n",
    "    if gradient_clip_count >= 3:\n",
    "        print(\"âœ… Recommendation: Use Gradient Clipping for stability\")\n",
    "    \n",
    "    # Specific recommendations\n",
    "    print(f\"\\nRECOMMENDED OPTIMIZED CONFIGURATION:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    best_config = successful_results[results_df.iloc[0]['config_id']]['config']\n",
    "    \n",
    "    # Base configuration from results\n",
    "    recommended_config = {\n",
    "        'name': 'Optimized Based on Results',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': int(top_5['estop_thresh'].median()),\n",
    "        'batch_size': int(top_5['batch_size'].median()),\n",
    "        'use_class_weights': top_5['use_class_weights'].mode()[0],\n",
    "        'l2_regularization': top_5['l2_regularization'].median(),\n",
    "        'lr_schedule': best_config['lr_schedule'],\n",
    "        'initial_lr': top_5['initial_lr'].median(),\n",
    "        'standardize': True,  # Almost always beneficial\n",
    "        'spec_augment': spec_aug_count >= 3,\n",
    "        'noise_augment': noise_aug_count >= 3,\n",
    "        'num_epochs': int(top_5['num_epochs'].median()),\n",
    "        \n",
    "        # NEW: Optimization recommendations based on results\n",
    "        'mixed_precision': mixed_precision_count >= 3,\n",
    "        'gradient_clipping': top_5['gradient_clipping_value'].median() if gradient_clip_count >= 3 else 0,\n",
    "        'parallel_folds': False,  # For single fold; set True for cross-validation\n",
    "        'max_parallel_folds': 2,  # Conservative for RTX 5080\n",
    "        'optimize_dataloaders': True,  # Always beneficial\n",
    "    }\n",
    "    \n",
    "    print(\"```python\")\n",
    "    print(\"# OPTIMIZED CONFIGURATION FOR RTX 5080 + Ryzen 9 7950X\")\n",
    "    print(\"optimized_config = {\")\n",
    "    for key, value in recommended_config.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"    '{key}': '{value}',\")\n",
    "        else:\n",
    "            print(f\"    '{key}': {value},\")\n",
    "    print(\"}\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Performance expectations with optimizations\n",
    "    expected_f1 = top_5['final_val_f1'].mean()\n",
    "    f1_std = top_5['final_val_f1'].std()\n",
    "    expected_time = top_5['training_time_min'].mean()\n",
    "    \n",
    "    if ENABLE_OPTIMIZATIONS and optimization_benchmarks['speedup_ratios']:\n",
    "        avg_speedup = np.mean(optimization_benchmarks['speedup_ratios'])\n",
    "        traditional_time = expected_time * avg_speedup\n",
    "        print(f\"â€¢ Traditional Training Time: ~{traditional_time:.1f} minutes\")\n",
    "        print(f\"â€¢ Speed Improvement: {avg_speedup:.2f}x faster\")\n",
    "        print(f\"â€¢ Time Saved per Config: ~{traditional_time - expected_time:.1f} minutes\")\n",
    "    \n",
    "    # Hardware-specific recommendations\n",
    "    print(f\"\\nðŸ–¥ï¸ HARDWARE-SPECIFIC RECOMMENDATIONS:\")\n",
    "    print(\"-\"*45)\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name()\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        \n",
    "        if \"RTX\" in gpu_name and gpu_memory >= 14:\n",
    "            print(f\"âœ… RTX 5080 Detected ({gpu_memory:.0f}GB VRAM):\")\n",
    "            print(f\"   â€¢ Use batch_size up to 80 with mixed precision\")\n",
    "            print(f\"   â€¢ Enable parallel_folds=True for cross-validation\")\n",
    "            print(f\"   â€¢ Set max_parallel_folds=2-3 for optimal memory usage\")\n",
    "            print(f\"   â€¢ Mixed precision provides ~50% speedup on this GPU\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  GPU: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "            print(f\"   â€¢ Use conservative batch sizes\")\n",
    "            print(f\"   â€¢ Mixed precision may provide less benefit\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No successful configurations to analyze!\")\n",
    "    print(\"Check the failed configurations and adjust parameters.\")\n",
    "    print(\"\\nðŸ”§ Troubleshooting Optimization Issues:\")\n",
    "    print(\"â€¢ Reduce batch_size if getting OOM errors\")\n",
    "    print(\"â€¢ Set mixed_precision=False if encountering numerical instability\")\n",
    "    print(\"â€¢ Lower gradient_clipping value if training becomes unstable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69614ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrices for All Configurations\n",
    "from utils.evaluation_utils import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ðŸ” Plotting confusion matrices for all successful configurations...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get successful configurations (those with confusion matrices)\n",
    "configs_with_cm = {k: v for k, v in results_database.items() \n",
    "                    if v['status'] == 'success' and 'confusion_matrix' in v['results']}\n",
    "\n",
    "if not configs_with_cm:\n",
    "    print(\"âŒ No configurations with confusion matrices found!\")\n",
    "else:\n",
    "    print(f\"ðŸ“Š Found {len(configs_with_cm)} configurations with confusion matrices\")\n",
    "    \n",
    "    # Sort configurations by F1 score (descending)\n",
    "    sorted_configs = sorted(configs_with_cm.items(), \n",
    "                            key=lambda x: x[1]['results']['final_val_f1'], \n",
    "                            reverse=True)\n",
    "    \n",
    "    # Create a large figure for all confusion matrices\n",
    "    n_configs = len(sorted_configs)\n",
    "    n_cols = min(3, n_configs)  # Max 3 columns\n",
    "    n_rows = (n_configs + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6*n_rows))\n",
    "    \n",
    "    # Handle single subplot case\n",
    "    if n_configs == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = [axes] if n_cols == 1 else axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for idx, (config_id, config_data) in enumerate(sorted_configs):\n",
    "        results = config_data['results']\n",
    "        cm = results['confusion_matrix']\n",
    "        f1_score = results['final_val_f1']\n",
    "        val_acc = results['final_val_acc']\n",
    "        \n",
    "        # Create subplot\n",
    "        plt.sca(axes[idx])\n",
    "        \n",
    "        # Normalize confusion matrix to percentages\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        # Create heatmap\n",
    "        sns.heatmap(cm_normalized, \n",
    "                    annot=True, \n",
    "                    fmt='.1f',\n",
    "                    cmap='Blues',\n",
    "                    square=True,\n",
    "                    cbar_kws={'label': 'Percentage (%)'})\n",
    "        \n",
    "        plt.title(f'{config_id}\\nF1: {f1_score:.4f} | Acc: {val_acc:.4f}', \n",
    "                 fontsize=12, pad=10)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(n_configs, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nðŸ“ˆ Confusion Matrix Summary:\")\n",
    "    print(f\"Total configurations plotted: {n_configs}\")\n",
    "    \n",
    "    best_config_id, best_config_data = sorted_configs[0]\n",
    "    best_results = best_config_data['results']\n",
    "    print(f\"Best performing configuration: {best_config_id}\")\n",
    "    print(f\"  - F1 Score: {best_results['final_val_f1']:.4f}\")\n",
    "    print(f\"  - Accuracy: {best_results['final_val_acc']:.4f}\")\n",
    "    print(f\"  - Loss: {best_results['final_val_loss']:.4f}\")\n",
    "    \n",
    "    if n_configs > 1:\n",
    "        worst_config_id, worst_config_data = sorted_configs[-1]\n",
    "        worst_results = worst_config_data['results']\n",
    "        improvement = best_results['final_val_f1'] - worst_results['final_val_f1']\n",
    "        print(f\"Worst performing configuration: {worst_config_id}\")\n",
    "        print(f\"  - F1 Score: {worst_results['final_val_f1']:.4f}\")\n",
    "        print(f\"F1 Score improvement from worst to best: {improvement:.4f} ({improvement/worst_results['final_val_f1']*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Confusion matrix plotting completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete results for future reference\n",
    "if len(successful_results) > 0:\n",
    "    # Create a clean version of results_database without problematic objects\n",
    "    clean_results_database = {}\n",
    "    for config_id, data in results_database.items():\n",
    "        clean_data = {\n",
    "            'config': data['config'],\n",
    "            'status': data['status'],\n",
    "            'optimization_metadata': data.get('optimization_metadata', {}),\n",
    "            'timestamp': data.get('timestamp', ''),\n",
    "            'training_time_seconds': data.get('training_time_seconds', 0)\n",
    "        }\n",
    "        \n",
    "        if data['status'] == 'success' and data.get('result'):\n",
    "            # Extract only serializable parts of the result\n",
    "            result = data['result']\n",
    "            clean_data['result'] = {\n",
    "                'final_val_acc': result.get('final_val_acc', 0),\n",
    "                'final_val_f1': result.get('final_val_f1', 0),\n",
    "                'final_val_loss': result.get('final_val_loss', 0),\n",
    "                'best_val_acc': result.get('best_val_acc', 0),\n",
    "                'best_val_f1': result.get('best_val_f1', 0),\n",
    "                'training_time': result.get('training_time', 0),\n",
    "                'training_type': result.get('training_type', 'single_fold'),\n",
    "                'parallel_execution': result.get('parallel_execution', False),\n",
    "                # Include history without problematic objects\n",
    "                'history': {\n",
    "                    'early_stopped': result.get('history', {}).get('early_stopped', False),\n",
    "                    'best_epoch': result.get('history', {}).get('best_epoch', 0),\n",
    "                    'total_epochs': result.get('history', {}).get('total_epochs', 0)\n",
    "                }\n",
    "            }\n",
    "        elif data['status'] == 'failed':\n",
    "            clean_data['error'] = data.get('error', 'Unknown error')\n",
    "        \n",
    "        clean_results_database[config_id] = clean_data\n",
    "    \n",
    "    # Create a comprehensive results file\n",
    "    complete_results = {\n",
    "        'metadata': {\n",
    "            'test_date': training_start_time.isoformat(),\n",
    "            'total_configs_tested': len(configurations),\n",
    "            'successful_configs': len(successful_results),\n",
    "            'failed_configs': len(failed_configs),\n",
    "            'total_duration_hours': total_duration / 3600,\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(features),\n",
    "                'num_classes': len(np.unique(labels)),\n",
    "                'num_authors': len(np.unique(authors)),\n",
    "                'feature_shape': list(features.shape)\n",
    "            }\n",
    "        },\n",
    "        'configurations': configurations,\n",
    "        'results': clean_results_database,\n",
    "        'analysis': {\n",
    "            'top_10_configs': results_df.head(10).to_dict('records'),\n",
    "            'parameter_correlations': correlation_data.to_dict() if 'correlation_data' in locals() else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        return obj\n",
    "    \n",
    "    # Save as JSON for future analysis\n",
    "    try:\n",
    "        # Save to CSV\n",
    "        results_df.to_csv('../models/reports/configuration_results.csv', index=False)\n",
    "\n",
    "        with open('../models/reports/complete_configuration_results.json', 'w') as f:\n",
    "            # Clean the data for JSON serialization\n",
    "            clean_results = json.loads(json.dumps(complete_results, default=convert_numpy))\n",
    "            json.dump(clean_results, f, indent=2)\n",
    "        \n",
    "        print(\"ðŸ’¾ Complete results saved to:\")\n",
    "        print(\"  - ../models/reports/configuration_results.csv (tabular data)\")\n",
    "        print(\"  - ../models/reports/complete_configuration_results.json (full results)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Warning: Could not save JSON file due to serialization issue: {e}\")\n",
    "        print(\"ðŸ’¾ Results saved to CSV only:\")\n",
    "        print(\"  - ../models/reports/configuration_results.csv (tabular data)\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Configuration testing completed successfully!\")\n",
    "    print(f\"Best configuration: {results_df.iloc[0]['config_id']} with F1 score of {results_df.iloc[0]['final_val_f1']:.4f}\")\n",
    "else:\n",
    "    print(\"âŒ No results to save - all configurations failed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
