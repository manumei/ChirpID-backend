{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fc238e",
   "metadata": {},
   "source": [
    "# Model Configuration Testing\n",
    "\n",
    "This notebook is designed for systematic hyperparameter optimization. It allows testing different combinations of model parameters to find the optimal configuration for bird song classification.\n",
    "\n",
    "## Configuration Parameters:\n",
    "- **ADAM Optimizer**: Whether to use Adam optimizer (vs SGD)\n",
    "- **Early Stopping Threshold**: Patience for early stopping\n",
    "- **Batch Size**: Training batch size\n",
    "- **Class Weights**: Whether to use class weights for imbalanced data\n",
    "- **L2 Regularization**: Weight decay parameter\n",
    "- **Learning Rate Schedule**: Type and parameters for LR scheduling\n",
    "- **Initial Learning Rate**: Starting learning rate\n",
    "- **Standardization**: Whether to standardize features\n",
    "- **SpecAugment**: Whether to apply spectrogram augmentation\n",
    "- **Noise Augment**: Whether to apply Gaussian noise augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e13558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Any\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Import the clean training API\n",
    "from utils.training_core import single_fold_training\n",
    "from utils.models import BirdCNN\n",
    "from utils.evaluation_utils import plot_single_fold_curve, print_single_fold_results\n",
    "\n",
    "print(f\"Using device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85319ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = pd.read_csv(os.path.join('..', 'database', 'meta', 'final', 'train_data.csv'))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of classes: {df['label'].nunique()}\")\n",
    "print(f\"Number of authors: {df['author'].nunique()}\")\n",
    "\n",
    "# Extract labels, authors, and features\n",
    "labels = df['label'].values.astype(np.int64)\n",
    "authors = df['author'].values\n",
    "features = df.drop(columns=['label', 'author']).values.astype(np.float32)\n",
    "\n",
    "# Convert to 0-1 range and reshape for CNN\n",
    "features /= 255.0\n",
    "features = features.reshape(-1, 1, 313, 224)\n",
    "\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Authors shape:\", authors.shape)\n",
    "print(\"Unique classes:\", len(np.unique(labels)))\n",
    "print(\"Unique authors:\", len(np.unique(authors)))\n",
    "\n",
    "# Display class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "plt.bar(unique_labels, counts, alpha=0.7)\n",
    "plt.xlabel('Class ID')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xticks(unique_labels)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average samples per class: {len(labels) / len(unique_labels):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60fe59",
   "metadata": {},
   "source": [
    "## Configuration Templates\n",
    "\n",
    "20 different hyperparameter configurations designed for audio classification with ~3200 samples and 30 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 20 configuration templates for systematic testing\n",
    "configurations = {\n",
    "    # Baseline configurations\n",
    "    'config0': {\n",
    "        'name': 'Conservative Baseline',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 24,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 200\n",
    "    },\n",
    "    \n",
    "    'config1': {\n",
    "        'name': 'Aggressive Baseline',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 48,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 5e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.002,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 250\n",
    "    },\n",
    "    \n",
    "    # Learning rate schedule variations\n",
    "    'config2': {\n",
    "        'name': 'Exponential LR Decay',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 40,\n",
    "        'batch_size': 32,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.95},\n",
    "        'initial_lr': 0.003,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220\n",
    "    },\n",
    "    \n",
    "    'config3': {\n",
    "        'name': 'ReduceLROnPlateau',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 50,\n",
    "        'batch_size': 24,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 2e-4,\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.5, 'patience': 10},\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 300\n",
    "    },\n",
    "    \n",
    "    'config4': {\n",
    "        'name': 'Cosine Annealing',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 30,\n",
    "        'batch_size': 16,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-5,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 50},\n",
    "        'initial_lr': 0.005,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 200\n",
    "    },\n",
    "    \n",
    "    # Batch size variations\n",
    "    'config5': {\n",
    "        'name': 'Small Batch High LR',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 16,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.98},\n",
    "        'initial_lr': 0.004,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 180\n",
    "    },\n",
    "    \n",
    "    'config6': {\n",
    "        'name': 'Large Batch Conservative',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 45,\n",
    "        'batch_size': 64,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.0005,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 250\n",
    "    },\n",
    "    \n",
    "    # Regularization focused\n",
    "    'config7': {\n",
    "        'name': 'Heavy Regularization',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 60,\n",
    "        'batch_size': 32,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 1e-3,\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.7, 'patience': 15},\n",
    "        'initial_lr': 0.001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 300\n",
    "    },\n",
    "    \n",
    "    'config8': {\n",
    "        'name': 'Light Regularization',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 48,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-5,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.002,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 150\n",
    "    },\n",
    "    \n",
    "    # SGD variants\n",
    "    'config9': {\n",
    "        'name': 'SGD with Momentum',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 40,\n",
    "        'batch_size': 32,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.9},\n",
    "        'initial_lr': 0.01,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 200\n",
    "    },\n",
    "    \n",
    "    'config10': {\n",
    "        'name': 'SGD High Learning Rate',\n",
    "        'use_adam': False,\n",
    "        'estop_thresh': 30,\n",
    "        'batch_size': 24,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 5e-4,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 40},\n",
    "        'initial_lr': 0.05,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 180\n",
    "    },\n",
    "    \n",
    "    # Augmentation focused\n",
    "    'config11': {\n",
    "        'name': 'Full Augmentation Suite',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 45,\n",
    "        'batch_size': 20,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 2e-4,\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.6, 'patience': 12},\n",
    "        'initial_lr': 0.0015,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 280\n",
    "    },\n",
    "    \n",
    "    'config12': {\n",
    "        'name': 'No Augmentation Fast',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 20,\n",
    "        'batch_size': 48,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.003,\n",
    "        'standardize': False,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 120\n",
    "    },\n",
    "    \n",
    "    # Class weights focus\n",
    "    'config13': {\n",
    "        'name': 'Balanced Classes Focus',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 28,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 3e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.96},\n",
    "        'initial_lr': 0.0012,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 220\n",
    "    },\n",
    "    \n",
    "    # Fine-tuning oriented\n",
    "    'config14': {\n",
    "        'name': 'Fine-tuning Style',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 15,\n",
    "        'batch_size': 16,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-5,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.0001,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 100\n",
    "    },\n",
    "    \n",
    "    # Extreme configurations for boundary testing\n",
    "    'config15': {\n",
    "        'name': 'High Capacity',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 25,\n",
    "        'batch_size': 64,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 1e-6,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 60},\n",
    "        'initial_lr': 0.006,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 240\n",
    "    },\n",
    "    \n",
    "    'config16': {\n",
    "        'name': 'Conservative Long Train',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 80,\n",
    "        'batch_size': 20,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 1e-3,\n",
    "        'lr_schedule': {'type': 'plateau', 'factor': 0.8, 'patience': 20},\n",
    "        'initial_lr': 0.0008,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 400\n",
    "    },\n",
    "    \n",
    "    # Mixed strategies\n",
    "    'config17': {\n",
    "        'name': 'Adaptive Mixed',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 35,\n",
    "        'batch_size': 36,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 4e-4,\n",
    "        'lr_schedule': {'type': 'exponential', 'gamma': 0.94},\n",
    "        'initial_lr': 0.0018,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 200\n",
    "    },\n",
    "    \n",
    "    'config18': {\n",
    "        'name': 'Quick Convergence',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 15,\n",
    "        'batch_size': 52,\n",
    "        'use_class_weights': False,\n",
    "        'l2_regularization': 2e-4,\n",
    "        'lr_schedule': None,\n",
    "        'initial_lr': 0.0025,\n",
    "        'standardize': True,\n",
    "        'spec_augment': False,\n",
    "        'noise_augment': False,\n",
    "        'num_epochs': 100\n",
    "    },\n",
    "    \n",
    "    'config19': {\n",
    "        'name': 'Robust Generalization',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': 50,\n",
    "        'batch_size': 24,\n",
    "        'use_class_weights': True,\n",
    "        'l2_regularization': 6e-4,\n",
    "        'lr_schedule': {'type': 'cosine', 'T_max': 80},\n",
    "        'initial_lr': 0.0014,\n",
    "        'standardize': True,\n",
    "        'spec_augment': True,\n",
    "        'noise_augment': True,\n",
    "        'num_epochs': 320\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(configurations)} configurations for testing\")\n",
    "print(\"\\nConfiguration Overview:\")\n",
    "for config_id, config in configurations.items():\n",
    "    print(f\"{config_id}: {config['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa5aba",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Execute training for each configuration and collect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234616d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results_database = {}\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING CONFIGURATION TESTING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Start time: {training_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total configurations to test: {len(configurations)}\")\n",
    "\n",
    "# Track overall progress\n",
    "successful_configs = 0\n",
    "failed_configs = []\n",
    "\n",
    "for config_id, config in configurations.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TESTING {config_id.upper()}: {config['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # Convert config to training_core format\n",
    "        training_config = {\n",
    "            'num_epochs': config['num_epochs'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'learning_rate': config['initial_lr'],\n",
    "            'use_class_weights': config['use_class_weights'],\n",
    "            'early_stopping': config['estop_thresh'],\n",
    "            'standardize': config['standardize'],\n",
    "            'test_size': 0.2,\n",
    "            'max_split_attempts': 5000,\n",
    "            'min_test_segments': 5,\n",
    "            'l2_regularization': config['l2_regularization'],\n",
    "            'use_adam': config['use_adam'],\n",
    "            'lr_schedule': config['lr_schedule']\n",
    "        }\n",
    "        \n",
    "        # Execute training\n",
    "        result = single_fold_training(\n",
    "            features=features,\n",
    "            labels=labels,\n",
    "            authors=authors,\n",
    "            model_class=BirdCNN,\n",
    "            num_classes=len(np.unique(labels)),\n",
    "            config=training_config,\n",
    "            spec_augment=config['spec_augment'],\n",
    "            gaussian_noise=config['noise_augment']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        config_end_time = datetime.now()\n",
    "        training_duration = (config_end_time - config_start_time).total_seconds()\n",
    "        \n",
    "        results_database[config_id] = {\n",
    "            'config': config,\n",
    "            'result': result,\n",
    "            'training_time_seconds': training_duration,\n",
    "            'timestamp': config_end_time.isoformat(),\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        successful_configs += 1\n",
    "        \n",
    "        print(f\"\\n‚úì {config_id} completed successfully!\")\n",
    "        print(f\"  Final Val Accuracy: {result['final_val_acc']:.4f}\")\n",
    "        print(f\"  Final Val F1 Score: {result['final_val_f1']:.4f}\")\n",
    "        print(f\"  Training time: {training_duration:.1f}s ({training_duration/60:.1f}min)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"\\n‚úó {config_id} failed: {error_msg}\")\n",
    "        \n",
    "        failed_configs.append(config_id)\n",
    "        results_database[config_id] = {\n",
    "            'config': config,\n",
    "            'result': None,\n",
    "            'error': error_msg,\n",
    "            'status': 'failed'\n",
    "        }\n",
    "\n",
    "training_end_time = datetime.now()\n",
    "total_duration = (training_end_time - training_start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONFIGURATION TESTING COMPLETED\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"End time: {training_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total duration: {total_duration/3600:.2f} hours\")\n",
    "print(f\"Successful configurations: {successful_configs}/{len(configurations)}\")\n",
    "if failed_configs:\n",
    "    print(f\"Failed configurations: {', '.join(failed_configs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d52a4",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Comprehensive analysis and visualization of all configuration results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d999aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract successful results for analysis\n",
    "successful_results = {k: v for k, v in results_database.items() if v['status'] == 'success'}\n",
    "\n",
    "if not successful_results:\n",
    "    print(\"No successful configurations to analyze!\")\n",
    "else:\n",
    "    print(f\"Analyzing {len(successful_results)} successful configurations...\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    analysis_data = []\n",
    "    for config_id, data in successful_results.items():\n",
    "        config = data['config']\n",
    "        result = data['result']\n",
    "        \n",
    "        row = {\n",
    "            'config_id': config_id,\n",
    "            'config_name': config['name'],\n",
    "            'final_val_acc': result['final_val_acc'],\n",
    "            'final_val_f1': result['final_val_f1'],\n",
    "            'best_val_acc': result['best_val_acc'],\n",
    "            'best_val_f1': result['best_val_f1'],\n",
    "            'training_time_min': data['training_time_seconds'] / 60,\n",
    "            # Configuration parameters\n",
    "            'use_adam': config['use_adam'],\n",
    "            'estop_thresh': config['estop_thresh'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'use_class_weights': config['use_class_weights'],\n",
    "            'l2_regularization': config['l2_regularization'],\n",
    "            'has_lr_schedule': config['lr_schedule'] is not None,\n",
    "            'lr_schedule_type': config['lr_schedule']['type'] if config['lr_schedule'] else 'none',\n",
    "            'initial_lr': config['initial_lr'],\n",
    "            'standardize': config['standardize'],\n",
    "            'spec_augment': config['spec_augment'],\n",
    "            'noise_augment': config['noise_augment'],\n",
    "            'num_epochs': config['num_epochs']\n",
    "        }\n",
    "        analysis_data.append(row)\n",
    "    \n",
    "    results_df = pd.DataFrame(analysis_data)\n",
    "    \n",
    "    # Sort by F1 score (primary metric)\n",
    "    results_df = results_df.sort_values('final_val_f1', ascending=False)\n",
    "    \n",
    "    print(\"TOP 10 CONFIGURATIONS BY F1 SCORE:\")\n",
    "    print(\"=\"*50)\n",
    "    top_10 = results_df.head(10)[['config_id', 'config_name', 'final_val_f1', 'final_val_acc', 'training_time_min']]\n",
    "    print(top_10.to_string(index=False))\n",
    "    \n",
    "    # Best configuration details\n",
    "    best_config_id = results_df.iloc[0]['config_id']\n",
    "    best_config_data = successful_results[best_config_id]\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST CONFIGURATION: {best_config_id}\")\n",
    "    print(f\"Name: {best_config_data['config']['name']}\")\n",
    "    print(f\"Final Val F1: {results_df.iloc[0]['final_val_f1']:.4f}\")\n",
    "    print(f\"Final Val Accuracy: {results_df.iloc[0]['final_val_acc']:.4f}\")\n",
    "    print(f\"Training Time: {results_df.iloc[0]['training_time_min']:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "if len(successful_results) > 0:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle('Configuration Results Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. F1 Score comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    results_df_plot = results_df.head(15)  # Top 15 for readability\n",
    "    bars1 = ax1.bar(range(len(results_df_plot)), results_df_plot['final_val_f1'], alpha=0.7, color='skyblue')\n",
    "    ax1.set_title('Final Validation F1 Score by Configuration')\n",
    "    ax1.set_xlabel('Configuration Rank')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_xticks(range(len(results_df_plot)))\n",
    "    ax1.set_xticklabels(results_df_plot['config_id'], rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, bar in enumerate(bars1):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 2. Accuracy vs F1 scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(results_df['final_val_acc'], results_df['final_val_f1'], \n",
    "                         c=results_df['training_time_min'], cmap='viridis', alpha=0.7, s=100)\n",
    "    ax2.set_xlabel('Final Validation Accuracy')\n",
    "    ax2.set_ylabel('Final Validation F1 Score')\n",
    "    ax2.set_title('Accuracy vs F1 Score (colored by training time)')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Training Time (min)')\n",
    "    \n",
    "    # Add best point annotation\n",
    "    best_acc = results_df.iloc[0]['final_val_acc']\n",
    "    best_f1 = results_df.iloc[0]['final_val_f1']\n",
    "    ax2.annotate(f'Best: {best_config_id}', xy=(best_acc, best_f1), \n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # 3. Parameter impact - Batch size\n",
    "    ax3 = axes[0, 2]\n",
    "    batch_impact = results_df.groupby('batch_size')['final_val_f1'].agg(['mean', 'count']).reset_index()\n",
    "    bars3 = ax3.bar(batch_impact['batch_size'], batch_impact['mean'], alpha=0.7, color='lightcoral')\n",
    "    ax3.set_title('Average F1 Score by Batch Size')\n",
    "    ax3.set_xlabel('Batch Size')\n",
    "    ax3.set_ylabel('Average F1 Score')\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (batch_size, mean_f1, count) in batch_impact.iterrows():\n",
    "        ax3.text(batch_size, mean_f1 + 0.002, f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 4. Learning rate impact\n",
    "    ax4 = axes[1, 0]\n",
    "    lr_bins = pd.cut(results_df['initial_lr'], bins=5)\n",
    "    lr_impact = results_df.groupby(lr_bins)['final_val_f1'].agg(['mean', 'count']).reset_index()\n",
    "    lr_labels = [f'{interval.left:.4f}-{interval.right:.4f}' for interval in lr_impact['initial_lr']]\n",
    "    bars4 = ax4.bar(range(len(lr_labels)), lr_impact['mean'], alpha=0.7, color='lightgreen')\n",
    "    ax4.set_title('Average F1 Score by Learning Rate Range')\n",
    "    ax4.set_xlabel('Learning Rate Range')\n",
    "    ax4.set_ylabel('Average F1 Score')\n",
    "    ax4.set_xticks(range(len(lr_labels)))\n",
    "    ax4.set_xticklabels(lr_labels, rotation=45)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 5. Augmentation impact\n",
    "    ax5 = axes[1, 1]\n",
    "    augment_combinations = results_df.groupby(['spec_augment', 'noise_augment'])['final_val_f1'].agg(['mean', 'count']).reset_index()\n",
    "    augment_labels = []\n",
    "    for _, row in augment_combinations.iterrows():\n",
    "        spec = 'Spec' if row['spec_augment'] else 'NoSpec'\n",
    "        noise = 'Noise' if row['noise_augment'] else 'NoNoise'\n",
    "        augment_labels.append(f'{Spec}+{noise}')\n",
    "    \n",
    "    bars5 = ax5.bar(range(len(augment_labels)), augment_combinations['mean'], alpha=0.7, color='orange')\n",
    "    ax5.set_title('Average F1 Score by Augmentation Strategy')\n",
    "    ax5.set_xlabel('Augmentation Combination')\n",
    "    ax5.set_ylabel('Average F1 Score')\n",
    "    ax5.set_xticks(range(len(augment_labels)))\n",
    "    ax5.set_xticklabels(augment_labels, rotation=45)\n",
    "    ax5.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (_, mean_f1, count) in augment_combinations.iterrows():\n",
    "        ax5.text(i, mean_f1 + 0.002, f'n={count}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 6. Training time vs performance\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.scatter(results_df['training_time_min'], results_df['final_val_f1'], alpha=0.7, s=100, color='purple')\n",
    "    ax6.set_xlabel('Training Time (minutes)')\n",
    "    ax6.set_ylabel('Final Validation F1 Score')\n",
    "    ax6.set_title('Training Time vs Performance')\n",
    "    ax6.grid(alpha=0.3)\n",
    "    \n",
    "    # Add trendline\n",
    "    z = np.polyfit(results_df['training_time_min'], results_df['final_val_f1'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax6.plot(results_df['training_time_min'], p(results_df['training_time_min']), \"r--\", alpha=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv('../database/meta/configuration_results.csv', index=False)\n",
    "    print(f\"\\nüíæ Results saved to ../database/meta/configuration_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter correlation analysis\n",
    "if len(successful_results) > 0:\n",
    "    print(\"\\nPARAMETER CORRELATION ANALYSIS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create correlation matrix for numerical parameters\n",
    "    numeric_params = ['estop_thresh', 'batch_size', 'l2_regularization', 'initial_lr', \n",
    "                     'num_epochs', 'final_val_f1', 'final_val_acc', 'training_time_min']\n",
    "    \n",
    "    correlation_data = results_df[numeric_params].corr()\n",
    "    \n",
    "    # Focus on correlations with performance metrics\n",
    "    f1_correlations = correlation_data['final_val_f1'].abs().sort_values(ascending=False)\n",
    "    acc_correlations = correlation_data['final_val_acc'].abs().sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Parameters most correlated with F1 Score:\")\n",
    "    for param, corr in f1_correlations.items():\n",
    "        if param != 'final_val_f1':\n",
    "            print(f\"  {param}: {corr:.3f}\")\n",
    "    \n",
    "    print(f\"\\nParameters most correlated with Accuracy:\")\n",
    "    for param, corr in acc_correlations.items():\n",
    "        if param != 'final_val_acc':\n",
    "            print(f\"  {param}: {corr:.3f}\")\n",
    "    \n",
    "    # Categorical parameter analysis\n",
    "    print(f\"\\nCATEGORICAL PARAMETER ANALYSIS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    categorical_params = ['use_adam', 'use_class_weights', 'has_lr_schedule', \n",
    "                         'standardize', 'spec_augment', 'noise_augment']\n",
    "    \n",
    "    for param in categorical_params:\n",
    "        if param in results_df.columns:\n",
    "            grouped = results_df.groupby(param)['final_val_f1'].agg(['mean', 'std', 'count'])\n",
    "            print(f\"\\n{param}:\")\n",
    "            print(grouped)\n",
    "    \n",
    "    # Best parameter combinations\n",
    "    print(f\"\\nBEST PARAMETER COMBINATIONS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Top 3 configurations analysis\n",
    "    top_3 = results_df.head(3)\n",
    "    for i, (_, row) in enumerate(top_3.iterrows(), 1):\n",
    "        print(f\"\\n#{i} - {row['config_id']} ({row['config_name']}):\")\n",
    "        print(f\"  F1: {row['final_val_f1']:.4f}, Acc: {row['final_val_acc']:.4f}\")\n",
    "        print(f\"  Batch Size: {row['batch_size']}, LR: {row['initial_lr']:.4f}\")\n",
    "        print(f\"  L2: {row['l2_regularization']:.2e}, Early Stop: {row['estop_thresh']}\")\n",
    "        print(f\"  Augmentation: Spec={row['spec_augment']}, Noise={row['noise_augment']}\")\n",
    "        print(f\"  Optimizer: {'Adam' if row['use_adam'] else 'SGD'}, Class Weights: {row['use_class_weights']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71389ab",
   "metadata": {},
   "source": [
    "## Configuration Recommendations\n",
    "\n",
    "Based on the results, provide recommendations for future configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(successful_results) > 0:\n",
    "    print(\"üéØ CONFIGURATION RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze best performing configurations\n",
    "    top_5 = results_df.head(5)\n",
    "    \n",
    "    # Extract common patterns from top performers\n",
    "    common_patterns = {}\n",
    "    \n",
    "    # Optimizer preference\n",
    "    adam_count = top_5['use_adam'].sum()\n",
    "    common_patterns['optimizer'] = 'Adam' if adam_count >= 3 else 'Mixed'\n",
    "    \n",
    "    # Batch size trends\n",
    "    avg_batch_size = top_5['batch_size'].mean()\n",
    "    common_patterns['batch_size_range'] = f\"{top_5['batch_size'].min()}-{top_5['batch_size'].max()}\"\n",
    "    \n",
    "    # Learning rate trends\n",
    "    avg_lr = top_5['initial_lr'].mean()\n",
    "    common_patterns['lr_range'] = f\"{top_5['initial_lr'].min():.4f}-{top_5['initial_lr'].max():.4f}\"\n",
    "    \n",
    "    # Regularization trends\n",
    "    avg_l2 = top_5['l2_regularization'].mean()\n",
    "    common_patterns['l2_range'] = f\"{top_5['l2_regularization'].min():.2e}-{top_5['l2_regularization'].max():.2e}\"\n",
    "    \n",
    "    # Augmentation preferences\n",
    "    spec_aug_count = top_5['spec_augment'].sum()\n",
    "    noise_aug_count = top_5['noise_augment'].sum()\n",
    "    \n",
    "    print(\"PATTERNS FROM TOP 5 CONFIGURATIONS:\")\n",
    "    print(\"-\"*40)\n",
    "    print(f\"‚Ä¢ Preferred Optimizer: {common_patterns['optimizer']}\")\n",
    "    print(f\"‚Ä¢ Effective Batch Size Range: {common_patterns['batch_size_range']}\")\n",
    "    print(f\"‚Ä¢ Optimal Learning Rate Range: {common_patterns['lr_range']}\")\n",
    "    print(f\"‚Ä¢ L2 Regularization Range: {common_patterns['l2_range']}\")\n",
    "    print(f\"‚Ä¢ SpecAugment Usage: {spec_aug_count}/5 top configs\")\n",
    "    print(f\"‚Ä¢ Noise Augmentation Usage: {noise_aug_count}/5 top configs\")\n",
    "    \n",
    "    # Specific recommendations\n",
    "    print(f\"\\nRECOMMENDED CONFIGURATION FOR FINE-TUNING:\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    best_config = successful_results[results_df.iloc[0]['config_id']]['config']\n",
    "    \n",
    "    recommended_config = {\n",
    "        'name': 'Optimized Based on Results',\n",
    "        'use_adam': True,\n",
    "        'estop_thresh': int(top_5['estop_thresh'].median()),\n",
    "        'batch_size': int(top_5['batch_size'].median()),\n",
    "        'use_class_weights': top_5['use_class_weights'].mode()[0],\n",
    "        'l2_regularization': top_5['l2_regularization'].median(),\n",
    "        'lr_schedule': best_config['lr_schedule'],\n",
    "        'initial_lr': top_5['initial_lr'].median(),\n",
    "        'standardize': True,  # Almost always beneficial\n",
    "        'spec_augment': spec_aug_count >= 3,\n",
    "        'noise_augment': noise_aug_count >= 3,\n",
    "        'num_epochs': int(top_5['num_epochs'].median())\n",
    "    }\n",
    "    \n",
    "    print(\"```python\")\n",
    "    print(\"recommended_config = {\")\n",
    "    for key, value in recommended_config.items():\n",
    "        if isinstance(value, str):\n",
    "            print(f\"    '{key}': '{value}',\")\n",
    "        else:\n",
    "            print(f\"    '{key}': {value},\")\n",
    "    print(\"}\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Performance expectations\n",
    "    expected_f1 = top_5['final_val_f1'].mean()\n",
    "    f1_std = top_5['final_val_f1'].std()\n",
    "    \n",
    "    print(f\"\\nEXPECTED PERFORMANCE:\")\n",
    "    print(f\"‚Ä¢ F1 Score: {expected_f1:.4f} ¬± {f1_std:.4f}\")\n",
    "    print(f\"‚Ä¢ Accuracy: {top_5['final_val_acc'].mean():.4f} ¬± {top_5['final_val_acc'].std():.4f}\")\n",
    "    print(f\"‚Ä¢ Training Time: ~{top_5['training_time_min'].mean():.1f} minutes\")\n",
    "    \n",
    "    # Areas for further exploration\n",
    "    print(f\"\\nAREAS FOR FURTHER EXPLORATION:\")\n",
    "    print(\"-\"*40)\n",
    "    print(\"‚Ä¢ Learning rate scheduling showed mixed results - try more variants\")\n",
    "    print(\"‚Ä¢ Batch size optimization could be further refined\")\n",
    "    print(\"‚Ä¢ L2 regularization sweet spot appears to be around 2e-4 to 4e-4\")\n",
    "    print(\"‚Ä¢ Early stopping threshold could be dataset-dependent\")\n",
    "    print(\"‚Ä¢ Consider ensemble methods combining top configurations\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No successful configurations to analyze!\")\n",
    "    print(\"Check the failed configurations and adjust parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete results for future reference\n",
    "if len(successful_results) > 0:\n",
    "    # Create a comprehensive results file\n",
    "    complete_results = {\n",
    "        'metadata': {\n",
    "            'test_date': training_start_time.isoformat(),\n",
    "            'total_configs_tested': len(configurations),\n",
    "            'successful_configs': len(successful_results),\n",
    "            'failed_configs': len(failed_configs),\n",
    "            'total_duration_hours': total_duration / 3600,\n",
    "            'dataset_info': {\n",
    "                'total_samples': len(features),\n",
    "                'num_classes': len(np.unique(labels)),\n",
    "                'num_authors': len(np.unique(authors)),\n",
    "                'feature_shape': list(features.shape)\n",
    "            }\n",
    "        },\n",
    "        'configurations': configurations,\n",
    "        'results': results_database,\n",
    "        'analysis': {\n",
    "            'top_10_configs': results_df.head(10).to_dict('records'),\n",
    "            'parameter_correlations': correlation_data.to_dict() if 'correlation_data' in locals() else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save as JSON for future analysis\n",
    "    with open('../database/meta/complete_configuration_results.json', 'w') as f:\n",
    "        # Convert numpy types to native Python types for JSON serialization\n",
    "        def convert_numpy(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            return obj\n",
    "        \n",
    "        # Clean the data for JSON serialization\n",
    "        clean_results = json.loads(json.dumps(complete_results, default=convert_numpy))\n",
    "        json.dump(clean_results, f, indent=2)\n",
    "    \n",
    "    print(\"üíæ Complete results saved to:\")\n",
    "    print(\"  - ../database/meta/configuration_results.csv (tabular data)\")\n",
    "    print(\"  - ../database/meta/complete_configuration_results.json (full results)\")\n",
    "    \n",
    "    print(f\"\\nüéâ Configuration testing completed successfully!\")\n",
    "    print(f\"Best configuration: {results_df.iloc[0]['config_id']} with F1 score of {results_df.iloc[0]['final_val_f1']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to save - all configurations failed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
